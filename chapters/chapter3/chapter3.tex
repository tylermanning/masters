\chapter{Kernel Change Point Detection}
\label{chapter3}
In this chapter, we focus on the class of change point models that leverage kernel techniques, known as \textit{kernel change point detection}. A unique feature of kernel methods is the use of the maximum mean discrepancy (MMD) metric as the change point test statistic. Therefore, a short review of the MMD statistic is presented first.

%All methods used in the experiments section make use of the maximum mean discrepancy (MMD) for their change point test statistic, therefore, a short review of the MMD statistic is presented first.

% For an extensive review of kernel embeddings and their applications see \cite{muandet2017kernel}.

\section{Kernel-Based Test Statistic}
\label{mmd}
Recall we are looking for a suitable statistic to solve the change point hypothesis test presented in \ref{formula_cpd}. To solve this, we need a statistic that can measure the difference between two distributions, $P$ and $Q$. In our setting, this is particularly hard because we do not know $P$ or $Q$ and only have access to data generated by them. Ideally, the metric measuring the difference should return a small value if $P \approx  Q$ and large value if $P$ and $Q$ are very different from each other. This means we need a metric that can compute the distance between two probability distributions using only data drawn from them. It turns out the maximum mean discrepancy (MMD) introduced in \cite{gretton2012kernel} meets all these criteria.

The idea behind the MMD is the following: rather than comparing the expected value of $P$ and $Q$ in their original feature space, why not compare their expected value in a different feature space that makes discerning between $P$ and $Q$ more effective. Doing so would allow the detection of differences between $P$ and $Q$ that were not possible before. A classic technique for comparing samples in a different feature space is through the use of kernel functions. Therefore, we will start with a brief background of kernel functions.

\subsection{Kernel Background}

Suppose we have a non-empty set  $\mathcal{X}$ for which $x,y \in \mathcal{X}$. A function $k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ is called a \textit{kernel} if there exists a \textit{feature space} $ \mathcal{H}$ with a corresponding \textit{feature map} $\phi: \mathcal{X} \rightarrow \mathcal{H}$. A kernel can be expressed as,
\begin{equation}
\label{kernel}
k(x,y) = \left\langle \phi(x), \phi(y) \right\rangle_\mathcal{H}
\end{equation}
where $\left\langle \cdot, \cdot \right\rangle_\mathcal{H}$ is the \textit{inner product} in $ \mathcal{H}$  \cite{scholkopf2002learning}. Using the kernel function, we can feed it two points and implicitly compute the dot product of these points in the transformed space. This is the so-called \textit{kernel trick}. It's a trick because the dot product can be computed without having to explicitly know the feature map or what the coordinates of $\phi(x)$ or $\phi(y)$ evaluate to. Lastly, because dot products are symmetric, this implies that the kernel function as defined above is also symmetric, i.e. $k(x, y) = k(y, x)$.

A related concept is the \textit{Gram matrix} which can be computed from a set of vectors $\{\mathbf{x}_1,..,\mathbf{x}_{\ell}\}$ as an $\ell \times \ell$ matrix $\mathbf{G}$, whose entries are $\mathbf{G}_{ij} = \left\langle \mathbf{x}_i, \mathbf{x}_j \right\rangle$. Using \ref{kernel}, we can define a \textit{kernel gram matrix} or just \textit{kernel matrix}, denoted by $\mathbf{K}$, as:
\begin{equation}
\mathbf{K}_{ij} := k(\mathbf{x}_i, \mathbf{x}_j ).
\end{equation}
By symmetry of the kernel, the kernel matrix is also symmetric $\mathbf{K}_{ij}=\mathbf{K}_{ji}$ or $\mathbf{K}^\text{T} = \mathbf{K}$. More explicitly, it is expressed as:

\begin{equation}
\begin{array}{lllll}
\hline \hline \mathbf{K} & 1 & 2 & \cdots & \ell \\
\hline 1 & k \left(\mathbf{x}_{1}, \mathbf{x}_{1}\right) & k\left(\mathbf{x}_{1}, \mathbf{x}_{2}\right) & \cdots & k
\left(\mathbf{x}_{1}, \mathbf{x}_{\ell}\right) \\
2 & k\left(\mathbf{x}_{2}, \mathbf{x}_{1}\right) & k\left(\mathbf{x}_{2}, \mathbf{x}_{2}\right) & \cdots & k\left(\mathbf{x}_{2}, \mathbf{x}_{\ell}\right) \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\ell & k\left(\mathbf{x}_{\ell}, \mathbf{x}_{1}\right) & k\left(\mathbf{x}_{\ell}, \mathbf{x}_{2}\right) & \cdots & k \left(\mathbf{x}_{\ell}, \mathbf{x}_{\ell}\right) \\
\hline \hline
\end{array}
\end{equation}

In most applications, $k$ is chosen to be positive definite. A function $k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ is a positive definite kernel if it is symmetric and
\begin{equation}
\sum_{i, j=1}^{n} c_{i} c_{j} k\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right) \geq 0,
\end{equation}
for any $n \in \mathbb{N}$, any choice of $\mathbf{x}_1,...,\mathbf{x}_n \in \mathcal{X}$ and any $c_1,...,c_n \in \mathbb{R}$. This is equivalent to requiring that the constructed kernel matrix has entirely positive eigenvalues. Furthermore, there always exists $\phi: \mathcal{X} \rightarrow \mathcal{H}$ for which equation \ref{kernel} holds  if $k$ is positive definite.

By selecting a positive definite kernel, the feature space $\mathcal{H}$ is actually a space of functions known as a \textit{reproducing kernel Hilbert space} (RKHS). While this may seem odd, it actually simplifies a lot of algebra because elements in a RKHS can be treated like finite vectors.  For the purposes of this  thesis, we will only deal with positive definite kernels to ensure the corresponding feature space is a RKHS.

%Given a kernel, neither the feature map nor the feature are uniquely determined from the above definition. Consider the following example. Let $\mathcal{X} \leftarrow \mathbb{R}$ and $k(x,y)=xy$ for all $x,y \in \mathbb{R}$. This is a trivial case in which $k$ is a kernel because the feature map can be set to the identity function such that $\phi(x)=x$ that maps into  $\mathcal{H} \leftarrow \mathbb{R}$. However, we can equally define $\phi: \mathcal{X} \rightarrow \mathbb{R}^2$ where $\phi(x) = (\frac{x}{\sqrt{2}}, \frac{x}{\sqrt{2}})$ that is also a feature map of $k$. We can see this by expanding $ \left\langle \phi(x), \phi(y) \right\rangle = \frac{x}{\sqrt{2}} \cdot \frac{y}{\sqrt{2}} +  \frac{x}{\sqrt{2}} \cdot \frac{y}{\sqrt{2}}=xy=k(x,y)$.

 %Conversely every symmetric, positive definite kernel K on X Ã— X defines a unique RKHS on X with K as its reproducing kernel.

%RKHS have the reproducing property for which any $f \in H$ we have f(x) = \left\langle f, k(x, \cdot) \right\rangle_{\mathcal(H)}

%Detailed properties of Hilbert spaces and inner product spaces can be found in \cite{young1988introduction}.  Proofs for these statements are out of scope of this thesis but can be found in\cite{scholkopf2002learning}. 
\subsection{Kernel Mean Embedding}
\label{kernel_mean_emb}
While kernel functions are useful for comparing vectors in a new feature space, this is not quite what we need because we are looking for a way to compare two probability distributions. In the above setting, $\phi$ accepts points or vectors as inputs, but this can be generalized to random variables in the following way: Let $X$ be a random variable with domain $\Omega$ and distribution $P(X)$. We refer to instantiations of $X$ as a lower case $x$. In \cite{smola2007hilbert}, it is shown that $\phi$ can be used to represent a probability distribution as an element in a RKHS by
\begin{equation}
\mu_X =\underset{X \sim P} {\mathbb{E}}[\phi(X)],
\end{equation}
which is known as the \textit{kernel mean embedding} of $P$. This can be interpreted as a  distribution being mapped to its expected feature map in $\mathcal{H}$. 

Because we are in a non-parametric setting, we cannot assume what $P(X)$ is nor compute $\mu_X$ exactly. However, we can estimate the embedding using a finite sample average  \cite{smola2007hilbert}. Suppose we have access to a sample $\{x_1, x_2, ..., x_m\}$ where $x_i$ is drawn i.i.d from $P(X)$, the empirical kernel mean embedding is 
\begin{equation}
\hat{\mu}_X = \frac{1}{m} \sum_{i=1}^m \phi(x_i).
\end{equation}
See Figure \ref{fig:kme} for a better understanding of the relationship between the distribution, the instances of the random variable and their embeddings into the new feature space.
\begin{minipage}{\textwidth}
\begin{center} 
\captionof{figure}[Kernel Mean Embedding]{Stylized depiction of a kernel mean embedding from a probability distribution to a feature space. On the left, the random variable $X$ has observations denoted by $x_i$, forming a probability distribution $P(X)$.  Using the feature map, $\phi$, each observation can be mapped to its coordinate in the new feature space.  If we repeat this many times empirically, then the approximate mean kernel embedding, $\hat{\mu}_X$, will be close to the theoretical counterpart, $\mu_X$.} 
\ctikzfig{kme}
\label{fig:kme} 
%\medskip
%\tiny
\end{center}
\end{minipage}

Why bother mapping $P$ to a RKHS? Because we can now compare two probability distributions in $\mathcal{H}$ as if they were vectors in an infinite-dimensional space. This means we can use vector operations such as dot products or norms with kernel mean embeddings.

While this new feature space representation is helpful, it is not necessarily unique. This a problem because $P$ and $Q$ may be mapped into a space where they are measurably similar when in fact they are not. Consequently, we need to ensure the kernel mapping is \textit{injective}, i.e. that the mapping from $\mathcal{X}$ to  $\mathcal{H}$ is one to one. %See 2.1.1 of \cite{atkinson2005theoretical} for more details on injective functions.

To ensure the kernel mapping is injective, we can make use of a special class of kernel functions known as \textit{characteristic kernels} that are introduced in \cite{fukumizu2008kernel} and \cite{sriperumbudur2011universality}. These papers show that if $k$ is characteristic then the mapping of $\phi$ to $\mathcal{H}$ is unique. This means if a characteristic kernel is used, then $X$ and $Y$ can be uniquely mapped to another feature space where the expected value in $\mathcal{H}$ is also unique. 

With this last piece of information we can now map two different distributions into a new feature space and compute a distance-like metric between the two, which is the basis of the maximum mean discrepancy.   %By combining the previous properties of kernel functions, we can now understand where the idea of the maximum mean discrepancy comes from. Rather than comparing the expected values of a moment in the original feature space like we did in 2.1, we will compare the expected values in this new feature space.

\subsection{Maximum Mean Discrepancy}
Suppose $m$ observations from a sample $\{x_1, x_2, ..., x_m\}$ and $n$ observations from a different sample $\{y_1, y_2, ..., y_n\}$ are derived from $\mathcal{X}$. Assume the observations from each sample are instantiations of random variables $X \sim  P$ and $Y \sim Q$ respectively. Using $\phi$ as it is described in \ref{kernel}, the maximum mean discrepancy (MMD) is defined in \cite{smola2007hilbert} as:
\begin{equation}
\label{mmd_theory}
\text{MMD}(P,Q)=||\mu_X - \mu_Y ||=|| \underset{X \sim P}{\mathbb{E}}[\phi(X)] -  \underset{Y \sim Q}{\mathbb{E}}[\phi(Y)]||_\mathcal{H},
\end{equation}

where $||\cdot||_\mathcal{H}$ is the \textit{norm} on $\mathcal{H}$. The MMD can be interpreted as the distance in $\mathcal{H}$ between the kernel mean embeddings of the features. As long as a kernel function can be defined on the given data structure, the MMD can be used. This is why it can be applied to non-numeric data such as strings, graphs, and other structured domains \cite{hofmann2008kernel}. From theorem 5 of \cite{gretton2012kernel}, the theoretical MMD$(P,Q) = 0$ if and only if $P=Q$. This fact arises from the injective property defined earlier. While this is a nice property, in practice we use empirical estimates of the kernel mean embeddings, $\hat{\mu}_X $ and $\hat{\mu}_Y$, meaning an estimate of MMD will never be exactly zero even if our samples are from the same distribution. Nonetheless, the power of MMD lies in its ease of estimation and the fact that it can detect any type of parameter difference between $P$ and $Q$. 
%This is because MMD is essentially a moment generaing function of P and Q.......

Using $X$ and $Y$, the unbiased estimate of the squared MMD is shown in \cite{gretton2012kernel} to be:
\begin{equation}
\label{mmd_unbiased}
\begin{split}
\widehat{\text{MMD}}_{u}^{2}(X, Y)=\frac{1}{m(m-1)} \sum_{i=1}^m \sum_{ j=1}^{m} k\left(x_{i}, x_{j}\right)-\frac{2}{m n} \sum_{i=1}^m \sum_{ j=1}^{n} k\left(x_{i}, y_{j}\right)+ \\
\frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j=1}^{n} k\left(y_{i}, y_{j}\right).
\end{split}
\end{equation}
One caveat of equation \ref{mmd_unbiased} is that while it is an unbiased estimate of the square of \ref{mmd_theory}, the calculation of $\sqrt{\widehat{\text{MMD}}_{u}^{2}}$ is not an unbiased estimate of \ref{mmd_theory}.

Similarly, the biased estimate of the squared MMD is
\begin{equation}
\label{mmd_biased}
\widehat{\text{MMD}}_{b}^{2}(X, Y)= \frac{1}{n^{2}} \sum_{i=1}^{n} \sum_{j=1}^{n} k\left(x_{i}, x_{j}\right)+\frac{1}{m^{2}} \sum_{i=1}^{m} \sum_{j=1}^{m} k\left(y_{i}, y_{j}\right)-\frac{2}{n m} \sum_{i=1}^{n} \sum_{j=1}^{m} k\left(x_{i}, y_{j}\right).
\end{equation}

Because both \ref{mmd_unbiased} and \ref{mmd_biased} are computed in $\mathcal{O}((m + n)^2)$ time, they will be referred to as quadratic-time estimates of MMD. For high-dimensional datasets (large $m$ or $n$), using a quadratic-time estimate of MMD is impractical.  

An alternative is proposed in section 6 of \cite{gretton2012kernel} that is linear in computation time and still uses all available data. Letting $X$ and $Y$ have equal size $m$ for notational simplicity, then
\begin{equation}
\label{mmd_linear}
\widehat{\text{MMD}}_{l}^{2}(X, Y) :=\frac{1}{m_{2}} \sum_{i=1}^{m_{2}} h\left(\left(x_{2 i-1}, y_{2 i-1}\right),\left(x_{2 i}, y_{2 i}\right)\right)
\end{equation}

is the linear-time estimate of MMD, where $m_2=\frac{m}{2}$ and
\begin{equation}
h\left((x_i, x_j), (y_i, y_j)\right):=k\left(x_{i}, x_{j}\right)+k\left(y_{i}, y_{j}\right)-k\left(x_{i}, y_{j}\right)-k\left(x_{j}, y_{i}\right).
\end{equation}
This estimation of the squared MMD is computed in $\mathcal{O}(m)$, which is significantly faster than the quadratic-time estimates. However, it pays for this speed with accuracy as the MMD$_l$ estimate has higher variance than MMD$_u$. Besides its speed of computation, an additional benefit of MMD$_l$ is that it is normally distributed under the null distribution unlike the quadratic-time estimate. This facilitates analysis and provides statistical guarantees for worst-case detection delays and the expected time to false alarm. 

Now that we have a suitable metric for non-parametrically computing a test statistic between samples $X$ and $Y$, a hypothesis test can be constructed using the MMD statistic as shown in \cite{gretton2012kernel}. Similar to previous hypothesis tests, a threshold and kernel function is chosen to adequately balance between type-I and type-II error. The null hypothesis is that $X$ and $Y$ are equivalent in their kernel mean elements.  If the MMD is greater than the threshold, then the null hypothesis is rejected and the alternative hypothesis is accepted. In this case, the alternative hypothesis would indicate $X$ and $Y$ differ in one  or more of their statistical moments.

\subsection{Choice of Kernel}
Notice we have not yet explicitly chosen a kernel function for calculating the MMD statistic. Choosing an appropriate kernel is often done on a per case basis based on the type of application. Additionally, optimal kernel selection is a difficult problem that is still actively researched \cite{fukumizu2009kernel} \cite{gretton2012optimal}. We will only discuss one kernel function in this thesis, the Gaussian kernel. The Gaussian kernel is the most widely used kernel and it is the only kernel used in the kernel change point detection methods discussed in section \ref{related_works}.  For other possible choices of kernel functions see table 3.1 in \cite{muandet2017kernel}.

Given two $d$ dimensional real vectors $x, y \in \mathbb{R}^d$, the Gaussian kernel is defined as:
\begin{equation}
k(x, y)= e^{-\frac{1}{2\sigma^2}||x-y||^2}
\end{equation}
where $\sigma > 0$ and is called the \textit{kernel bandwidth}. In \cite{gretton2005kernel}, the authors recommend selecting the bandwidth based on the \textit{median heuristic} using a sample of the data. The median is computed on the set of pairwise distances between all data points in the sample. Therefore, $\sigma$ is computed by, 
\begin{equation}
\sigma=\text{median}\{||x_i-x_j||:i,j = 1,...,n \}.
\end{equation}

It is not clear where this heuristic was first introduced although many papers incorrectly cite the 2002 textbook, \textit{Learning with kernels: support vector machines, regularization, optimization, and beyond} by Sch{\"o}lkopf, Smola, Bach et al \cite{garreau2017large}. Despite its unknown origins, it has been shown to be empirically suitable for many kernel applications. It seems as long as the bandwidth is chosen $\mathcal{O}(\sqrt{d})$, which the median heuristic often achieves, then test power is independent of the bandwidth. However, it has been shown to be sub-optimal in cases with very high-dimensions \cite{muandet2014kernel} or small sample size\cite{ramdas2015decreasing}. Because the median heuristic is critical to overall performance of the kernel two-sample test using the MMD, its online implementation will be explored in greater detail in section \ref{our_approach}.
 
\section{Related Work}
\label{related_works}

\subsection{Offline Kernel Change Point Detection}
While offline change point detection techniques are not the focus of this thesis, we provide a short summary of some kernel approaches in this domain to highlight some important, recent work.  Similar to online kernel methods, offline kernel methods are employed non-parametrically on data to provide more robust measures in situations where assumptions cannot be made about the data's distribution. Offline detection has the advantage that all data is fully available from the start. This means the key problem to solve is where to segment the time series so that the true and estimated change points align as closely as possible.  In other words, predicting too many change points (over-segmentation) must be balanced with predicting too little change points (under-segmentation). 

In \cite{harchaoui2007retrospective}, the authors approached the offline change point problem with a fixed number of change points using kernel change point detection. This was further extended to an unknown number of change points in \cite{arlot2019kernel}. The authors show their kernel change point detection procedure outputs an offline segmentation near optimal with high probability. The authors recommend choosing the kernel based on best possible signal to noise ratio. Therefore, they rely on prior knowledge of a reference or training set is necessary for calibrating the kernel. Theoretical guarantees for this approach are extensively covered in \cite{garreau2018}, where it is demonstrated that their kernel change point procedure can always locate all the true change points with high probability. Finally, a greedy kernel approach is introduced in \cite{truong2019greedy} to improve the trade-off between simpler, non-kernel based change point detection algorithms that are fast but inaccurate and the aforementioned kernel change point detection algorithms that are very accurate but computationally slow. While the authors tested their procedure with a known number of change points, they suggest using the Bayesian Information Criterion (BIC) as a model selection procedure to estimate the number of change points before initiating the greedy kernel algorithm.

More recently in \cite{chang2019kernel}, a  kernel change point detection method is proposed that uses deep generative models to augment the test power of the kernel two sample test statistic from equation \ref{mmd_theory}. They point out MMD's lack of test power when using small samples from the out of control distribution, which may easily lead to over-fitting with kernels. Thus, they use a generative adversarial neural network (GAN), trained on historical samples of $X \sim  P$  with noise injected into $X$. This surrogate distribution is then used in conjunction with possible change points to improve the test power of a modified MMD measure that makes use of compositional kernels. The method is compared to other prominent change point methods for offline change detection. All comparisons are done on synthetic data with piece-wise i.i.d. data. All methods are benchmarked using the AUC metric for classification performance and it is shown the KL-CPD method is competitive or better than the state of the art methods.  Furthermore, the AUC performance is maintained as the dimensionality of the data is increased, making their kernel learning framework very interesting for future offline change point detection. It remains to be seen if this framework can be adopted in an online context where time to detection is a key constraint on practicality.

All sources cited above make use of the kernel mean embedding machinery presented in section \ref{kernel_mean_emb}. It is also interesting to note that all offline kernel change point methods cited above make use of the Gaussian kernel, which speaks to the ubiquitousness of this kernel function. The following literature does not make use of kernel mean embeddings but does use kernels in some way for offline change point detection, so we include them for completeness.

Recent approaches include a supervised kernel change point analysis with partial annotations \cite{truong2019supervised}. The authors combine manually annotated signals with a kernel Mahalanobis-type norm for improved segmentation. While it is non-parametric, it is task specific as it relies on a human expert to correctly identify periods of signal stability. This supervised scheme does pay-off though as the improvement in performance indicates that change points that are the least accurately detected  in a strictly unsupervised scenario are likely to see the most improvement in detection accuracy. % It is then worth considering if a similar cost is worth paying for an online situation

See section 4.2.3  in \cite{truong2020selective} for a complete review of kernel-based methods used for offline detection. 

\subsection{Online Kernel Change Point Detection}
One of the first papers to use the term kernel change point detection was in \cite{desobry2005online}. The authors present an online kernel change point detection model based on single class support vector machines ($\nu$-SVMs). They train two single class support vectors, one on a past set and one on a future set. A ratio is then computed between the two sets that acts as the dissimilarity measure in a Hilbert space. If the sets are sufficiently dissimilar over some predetermined threshold, then a change point is assigned to the time step that splits the two sets of data. The authors argue that a dissimilarity measure between kernel mappings in a Hilbert space should estimate the \textit{density supports} rather than estimate the probability distributions of each set of points. While this approach inspired a lot of interesting research that will be discussed below, it does not use the maximum mean discrepancy and has not been studied since.

The following methods are all inspired by the classic algorithms from section \ref{classic_algo}. They all make use of a variation of the maximum mean discrepancy. They are also the most recent online kernel techniques developed and will be used for experiments in later sections. Thus, they will be described in more detail than previous methods.

In  \cite{li2015m}, the authors use the B-test introduced in \cite{zaremba2013b} and develop an offline and online change point detection algorithm called the Scan-B\footnote{This algorithm was originally called MStats but was subsequently re-named in a later version.} algorithm. The B-test estimates the MMD by applying the MMD$_u$ calculation from equation \ref{mmd_unbiased} on blocks of data of size $B$ and averages them. It is meant to be a compromise between the linear-time estimation MMD$_l$ and the quadratic-time estimation MMD$_u$.

At each time-step $t$, the online Scan-B algorithm samples new data from a window of size $B_0$ and computes a B-test statistic with $N$ past samples, also of size $B_0$ that are kept as reference samples. The reference samples are denoted as $X_{i}^{(B)}$ from $i,..,N$ and the test sample as $Y^{(B)}$. Using equation \ref{mmd_unbiased}, the unbiased MMD$_u^2$ is then computed between each reference sample and the test sample and finally all averaged together:
\begin{equation}
Z_{B_{0}, t}:=\frac{1}{N} \sum_{i=1}^{N} \operatorname{MMD}_{u}^{2}\left(X_{i}^{\left(B_{0}, t\right)}, Y^{\left(B_{0}, t\right)}\right).
\end{equation}
The resulting test statistic is then normalized by $Z_{B_{0}, t}/\sqrt{Var[Z_{B_0}]}$ where the authors provide a theoretical calculation of $\text{Var}[Z_B]$. If the normalized test statistic exceeds some predefined threshold then a change point is declared. Because the statistic is calculated each time and only the value of the last calculation has any weight,  it is essentially  memoryless. This is similar to a Shewart control chart that calculates a z-score at each iteration. Adjusting the size of the window, $B_0$, results in the usual trade-off of performance in online change point detection. A smaller block size will have a smaller computational cost and a smaller detection delay but will result in higher type II error. A larger block size will have better type II error but will take longer to compute. Unfortunately, no matter what block size is chosen, the computation time for the Scan-B algorithm is the longest relative to the other kernel change point methods discussed in this section. 

From here, theoretical bounds are developed for the time to false alarm and expected detection delay. They run several experiments on synthetic data including change in mean, change in variance, change from Gaussian to Gaussian mixture, and change from Gaussian to Laplace distribution. Experiments are also done on real-data sets including a speech dataset and the Human Activity Sensing Consortium (HASC) dataset where the performance was better than the relative density-ratio (RDR) algorithm described in \cite{liu2013change}.

%https://media.nips.cc/nipsbooks/nipspapers/paper_files/nips28/reviews/1852.html

A modified, "no-prior-knowledge" exponentially-weighted moving average called NEWMA is introduced in \cite{keriven2020newma}. Based on the standard exponentially weighted moving average shown in \ref{ewma}, NEWMA computes two EWMA statistics of different weights. If the difference between the two EWMA statistics exceeds a predefined threshold then a changepoint is declared at that time step. The reason for using two EWMA statistics is to set one to have a larger forgetting factor. Any recent changes in a distribution will weigh heavily on one statistic, resulting in a sudden, large difference between the two statistics. 

Since a standard EWMA is a parametric method, the authors apply a kernel mapping function, $\Psi$, to the data prior to applying the exponential weights. This provides a memoryless, non-parametric, online change point detection method that does not need to store all previously streamed data. Once the statistics are updated at each iteration, the raw data may be discarded. %This characteristic makes it especially useful in applications where security is a concern.

While kernel mean embeddings could be used for approximating $\Psi$, as is the case for standard implementations of MMD, this would require the storage of past examples of data. Because the authors aim to reduce run-time cost and storage cost, they use a \textit{random fourier features} (RFF) approach for estimating $\Psi$. Because $\Psi$  is estimated using RFF, this is the only method discussed that does not use the kernel function for an implicit representation of the data in another feature. Instead, the RFF explicitly maps the data to a lower dimension Euclidean product space using a randomized feature map $\mathbf{z}: \mathbb{R}^d \rightarrow \mathbb{R}^m$ such that:
\begin{equation}
k(\mathbf{x}, \mathbf{y})=\langle\phi(\mathbf{x}), \phi(\mathbf{y})\rangle_{\mathcal{H}} \approx \mathbf{z}(\mathbf{x})^{\top} \mathbf{z}(\mathbf{y})
\end{equation}

where $\mathbf{z}(\mathbf{x}):=\mathbf{W}^{\top} \mathbf{x}$. Each element, $w_{ij}$, is sampled from a distribution that is the Fourier transform of a translation invariant kernel \cite{rahimi2008random}.

There are several approaches available for calculating RFF from the literature and the authors use three common ones for comparison. They use the standard RFF implementation from \cite{rahimi2008random}, the FastFood implementation introduced in \cite{le2013fastfood}, and Optical Processing Unit implementation from \cite{saade2016random}. All three are used to create three variants of their NEWMA algorithm.

The NEWMA variants are compared to the Scan-B algorithm by running empirical experiments on synthetic and real datasets. The synthetic datasets use streaming data that is generated from different Gaussian mixture models. They also use an audio dataset for testing on real data. The results of the NEWMA variants are similar, if not better than Scan-B in terms of missed detection percentage. In terms of average detection delay and false alarm trade-off, the NEWMA algorithm and its variants appear to be mildly better as well. The largest advantage of the NEWMA variants over the Scan-B method is in the execution time. Scan-B's execution scales linearly with window size, while NEWMA's execution time does not depend on window size.

Finally, in a recent, preprint paper \cite{flynn2019change}, a kernel CUSUM (KCUSUM) algorithm is proposed, where the classic CUSUM algorithm from \ref{cusum} is adapted using the faster MMD$_l$ statistic for online detection. The algorithm functions as follows, every two observations, the MMD$_l$ is calculated using newly observed data points and data points sampled from some reference distribution that is known at the outset. The calculated MMD$_l$ acts as the update term to the cumulative sum statistic. If this kernel cumulative sum statistic exceeds some predefined threshold, then a change point is flagged. Interestingly, unlike the previous methods that set the Gaussian kernel's bandwidth using the median heuristic, the authors chose to fix the bandwidth to one. Furthermore, because it relies on a reference distribution that is outside the data stream for comparison, KCUSUM is more of a semi-supervised algorithm rather than a completely unsupervised one. %This reference distribution can be thought of as the "in-control" distribution of the data-stream that new observations are compared to. 

While this non-parametric approach can detect any change in the distribution of a sequence, it does struggle with more complicated distributional changes such as variance changes of a single dimension and changes beyond first and second-order moments. It was also not benchmarked against other kernel methods, which we cover in this thesis for completeness.

Given how important the bandwidth hyperparameter is for the Gaussian kernel function, it is noteworthy then to see how the above online change point models tune this hyperparameter. In the context of an infinite data stream, most methods simply take a small initial sample and use that to set the bandwidth once at the beginning and never change it again, like the Scan-B algorithm. But how long is an initial bandwidth valid in a continuous data stream? If the data changes significantly in orders of magnitude then this bandwidth selection will become stale and will have to be re-computed. One option is to set it to a fixed value rather than use the median heuristic like KCUSUM does. However, this is somewhat arbitrary and may lead to poor performance in certain cases. Another possibility would be to simply run an expanding median on all incoming data. But this would require storing all the observations ever observed which is obviously intractable. To our knowledge, these issues surrounding online bandwidth selection have not been addressed anywhere in the kernel change point detection literature. This is why the topic of successive median calculation is a focus in section \ref{our_approach}.

%In 2016, James et al. \cite{james2016leveraging} proposed using energy statistics to test the significance of a change point that is robust to anomalies. They point out that their work is the first to accurately detect change points with fast time to detection while not being affected by extreme outliers that are not change points. They compare their E-divisive with medians algorithm to the parametric PELT technique. 

\section{Our Approach}
\label{our_approach}

As stated in the related work, all kernel change point detection methods that use a Gaussian kernel use the median heuristic to determine the bandwidth. While this method has proved to be sufficient for most applications, it has not been thoroughly tested against any alternative bandwidth selection procedures. This is ironic given how much impact the choice of bandwidth can have on performance. We present a natural alternative for calculating the median heuristic in real-time for online change point detection. This method is based on the \texttt{Binapprox} algorithm shown below \cite{tibshirani2008fast}. 

\subsection{Successive Median Computation}
Suppose we have a sample denoted as $x_i,..,x_n,$ with corresponding sample mean and sample standard deviation, denoted by $\bar{x}$ and $s_x$ respectively. We can form $B$ evenly spaced bins across the interval $[\bar{x} - s_x, \bar{x} + s_x]$, which is guaranteed to contain the median. We can then map $x_i,..,x_n,$ to the created bins. The number of points in each bin is denoted by $N_i$ and any data that is to the left of the leftmost interval is totalled and is denoted by $N_L$. To locate the bin $b$ that contains the median, we add up the counts per bin starting from the left until the total exceeds $\frac{n}{2}.$ The median is then approximated by taking the midpoint of bin $b$.

%https://tex.stackexchange.com/questions/142313/how-to-see-the-steps-number-in-an-algorithm
\begin{center}
\begin{algorithm}[H]
\SetAlgoLined
\KwIn{$x_i,..,x_n, B$}
\KwOut{Approx. median}
Calculate $\bar{x}$ and $s_x$\\
Form $B$ bins across $[\bar{x} - s_x, \bar{x} + s_x]$\\
Map $x_i,..,x_n$ to bins\\
Find the bin $b$ that contains the median\\
Return the midpoint of bin b\\
 \caption{Binapprox algorithm}
\end{algorithm}
\end{center}

Suppose now that a new data sample arrives and we wish to re-compute the median. \texttt{Binapprox} can be leveraged to provide an updated estimate of the median. In this situation, the median has already been estimated using a sample denoted by $x_1,..,x_{n_0}$ and the values $\hat{x}_0, s_0, N_i$ and  $N_L$ are stored. If a new sample $x_{n_0+1},..,x_{n}$ arrives, how do we avoid redoing all the past calculations to find the median for the entire sample $x_1,..,x_n$? We simply need to allocate $x_{n_0+1},..,x_{n}$  to the original $B$ bins and increment the counts per bin. Like before, we then determine where the new median is by adding up each bin's counts until we find the median bin. If it lies outside the original bins, then \texttt{Binapprox} is re-computed on the entire sample. Otherwise, we return the midpoint of the median's bin. Note, the same logic can be applied to the situation where data is removed from the original sample. The only difference is rather than mapping new points to the bins, the bin counts are decremented according to what data is removed. The procedure then continues as previously described.

Several characteristics make the \texttt{Binapprox} algorithm particularly appealing for online, non-parametric data streaming.  Firstly, the runtime of \texttt{Binapprox} does not depend on a data's distribution. Secondly, the algorithm requires $\mathcal{O}(1)$ storage space, and doesn't rearrange the input data. Thirdly, in the worst-case scenario the algorithm has $\mathcal{O}(n)$ computational complexity. In practice, it consistently runs faster than the classic quickselect algorithm and \texttt{Binmedian}, a more exhaustive  algorithm that is presented in the same paper. Lastly, it can handle newly acquired data very quickly to provide an updated approximation of the median. 

The main drawback of the algorithm is hinted in its name. It is an approximation of the median and will be at most $\sigma / B$ away from the true median. Therefore, if the standard deviation of the data is extremely large,
the approximation could be significantly different from the actual median. To combat this, the author recommends setting $B=1000$, which yields sufficient approximations in most empirical cases.

Naturally, this fits in exactly to what we want to accomplish with online change point detection. We have incoming data that we want to roll into the calculation of the median heuristic but we also want a fast way to update the median while being accurate. The \texttt{Binapprox} algorithm checks all of these boxes and is simple to incorporate into any online change point detection work flow. 

\subsection{Online Median Heuristic}

As new data arrives, we recompute the bandwidth using an online median for the kernel function. As a new samples arrive, we store them in a rolling window that pops out the oldest data. Using this updated window, the median heuristic is then recalculated and used for the kernel change point test statistic.%Using the binapprox algorithm, the new data is allocated to the existing bins, and the new median bin is found if it is already in an existing bin. 

For this setup, there are two hyperparameters that must be chosen that will affect the estimated online bandwidth. The size of the window of past observations and how often we update this window of observations. On one extreme we could keep track of all past data and run an expanding median that is updated as soon as we observe a new observation. This would be cumbersome as the amount of data stored would grow linearly over time, which is not sustainable. Therefore, we must be careful not to store more past data than we need for storage efficiency sake. This must be balanced with how often we add new samples and remove the oldest samples simultaneously. Ideally, we want to update the bandwidth as often as possible so it does not become stale over time. However, simply removing the oldest data point and adding the newest data point to our median window may result in unstable median estimations. Therefore, this must also be tuned to the exact dataset to get a stable rolling result over time.

