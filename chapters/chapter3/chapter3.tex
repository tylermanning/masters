\chapter{Kernel Change Point Detection}

\section{Maximum Mean Discrepancy}
\label{mmd}
Suppose $n$ samples from a set $X = \{x_1, x_2, ..., x_n\}$ and $m$ samples from a different set $Y=\{y_1, y_2, ..., y_m\}$ are derived from a metric space $\mathcal{X}$. Assume both are distributed as $X \sim  P$ and $Y \sim Q$ respectively. As mentioned in \ref{hyptesting}, there needs to be some notion of distance, $d(X,Y)$, that can be computed between the two samples in order to conduct a two-sample test. Intuitively, if $P$ and $Q$ are very similar then $d(X,Y)$ should be close to zero. On the other hand, if $P$ and $Q$ are very different then $d(X,Y)$ should be large.

One such family of statistical distances is the group of \textit{integral probability metrics}, first described in \cite{muller1997integral}. The basic setup of these metrics is consider a space of functions, $\mathcal{F}$, where every function $f \in \mathcal{F}$ is a mapping such that $f : \mathcal{X}  \rightarrow \mathbb{R}$. An IPM between $P$ and $Q$ can be defined as:

\begin{equation}
d(P,Q)=\sup _{f \in F}|\underset{X \sim P}{\mathbb{E}}[f(X)]-\underset{Y \sim Q}{\mathbb{E}}[f(Y)]|.
\end{equation}

The choice of the space of functions, $\mathcal{F}$, determines the kind of distance measured between $P$ and $Q$. [Discuss example using KS distance.]

In \cite{gretton2012kernel}, the authors propose setting $\mathcal{F}$ to the unit ball in a Reproducing Kernel Hilbert space (RKHS), denoted by $\mathcal{H}$, i.e. $\mathcal{F} = \{f : ||f||_{\mathcal{H}} \leq 1\}$. The fact that a RKHS is chosen means there exists a \textit{feature map}, $\phi: \mathcal{X} \rightarrow \mathcal{H}$. In \cite{smola2007hilbert}, the formal definition of the maximum mean discrepancy (MMD) is defined as:
\begin{equation}
\label{mmd_theory}
\text{MMD}(P,Q)=|| \underset{X \sim P}{\mathbb{E}}[\phi(X)] -  \mathbb{E}_{Y \sim Q}[\phi(Y)]||_\mathcal{H}.
\end{equation}

Where the MMD can be interpreted as the distance in $\mathcal{H}$ between the mean embeddings of the features. As long as a kernel mean embedding can be defined on the given data structure, the MMD can be used. This is why it can be applied to non-numeric data such as strings, graphs, and other structured domains \cite{hofmann2008kernel}. 

Due to the reproducing property, the feature map, $\phi$, has a corresponding kernel function, $k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ where $k(x, y)=\langle\varphi(x), \varphi(y)\rangle_{\mathcal{H}}$. The proofs for this setting are rigorous and can be explored in \cite{smola2007hilbert} and XXXX and out of scope for discussion in this thesis. For our purposes it is important to understand that the kernel map, $\phi$, does not need to be explicitly computed thanks to the \textit{kernel trick}. Instead, we can only concern ourselves with the computation of the kernel function. Therefore, \ref{mmd_theory} can be rewritten as:

\begin{equation}
\text{MMD}(P,Q)=\sup _{f \in F}|\underset{X \sim P}{\mathbb{E}}[f(X)]-\underset{Y \sim Q}{\mathbb{E}}[f(Y)]|.
\end{equation}

If the kernel mapping into the Hilbert space is injective then $k$ is called a \textit{characteristic} kernel function. In  \cite{fukumizu2008kernel}, it is proven using a characteristic function to compute the MMD implies that $||\mu_P - \mu_Q ||=0$ if and only if $P=Q$.  As \cite{muandet2017kernel} points out, there is no loss if information in this case. 

%One advantage of using the MMD for comparing distributions over other classic techniques like Kullbackâ€“Leibler (K-L) divergence is the density of the distributions do not have to be estimated as an interim step. 


%  The kernel mean embedding can be thought of applying the \textit{kernel trick} to probability measures and mapping them into a higher dimensional feature space. Rather than applying the kernel trick to individual data points and mapping them to an implicit feature space, the kernel mapping, $\phi$, (also called feature map) will be applied to a probability distribution in order to represent it in a reproducing kernel Hilbert space (RKHS), that is $\phi: \mathcal{X} \rightarrow \mathcal{H}$.

%The kernel mean embeddings are denoted by $\mu_P = \mathbb{E}_{X \sim P}[\phi(X)] $ and $\mu_Q =\mathbb{E}_{Y \sim Q}[\phi(Y)]$ for samples $X$ and $Y$ respectively, where $\mu_P, \mu_Q \in \mathcal{H}$.


Typically, the actual distributions $P$ and $Q$ are unknown, making $\mu_P$ and $\mu_Q$ also unknown. Therefore, for applications, empirical estimates using the sampled data $X$ and $Y$ must be used instead. As shown in \cite{muandet2017kernel}, the empirical estimate of the kernel mean embeddings, denoted by $\hat{\mu}_P$ and  $\hat{\mu}_Q$ respectively, is

\noindent
\begin{tabularx}{\linewidth}{@{}XX@{}}
\begin{equation}
  \hat{\mu}_P := \frac{1}{n} \sum_{i=1}^n k(x_i, \cdot)
\end{equation}
&
\begin{equation}
    \hat{\mu}_Q := \frac{1}{n} \sum_{i=1}^n k(x_i, \cdot).
\end{equation}
\end{tabularx}
As shown in \cite{sriperumbudur2012empirical}, $\hat{\mu}_P$ is an unbiased estimate of $\mu_P$  and by law of large numbers, $\hat{\mu}_P$ converges to as $n \to \infty$. 

In \cite{gretton2012kernel}, the unbiased, estimate of the squared MMD is shown to be:

\begin{equation}
\begin{split}
\widehat{\mathbf{M M D}}_{u}^{2}(X, Y)=\frac{1}{m(m-1)} \sum_{i=1}^m \sum_{ j=1}^{m} k\left(x_{i}, x_{j}\right)-\frac{2}{m n} \sum_{i=1}^m \sum_{ j=1}^{n} k\left(x_{i}, y_{j}\right)+ \\
\frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j=1}^{n} k\left(y_{i}, y_{j}\right)
\end{split}
\end{equation}

and the biased estimation of the squared MMD is
\begin{equation}
\widehat{\mathbf{M M D}}_{b}^{2}(X, Y)= \frac{1}{n^{2}} \sum_{i=1}^{n} \sum_{j=1}^{n} k\left(x_{i}, x_{j}\right)+\frac{1}{m^{2}} \sum_{i=1}^{m} \sum_{j=1}^{m} k\left(y_{i}, y_{j}\right)-\frac{2}{n m} \sum_{i=1}^{n} \sum_{j=1}^{m} k\left(x_{i}, y_{j}\right).
\end{equation}

%Which by law of large numbers converges to the theoretical values at a rate of XX. 
%On its own, the MMD is not a metric but it can be one if the chosen kernel,$k$, is a \textit{characteristic kernel}. 

Common characteristic kernel functions used in practice are the radial basis function kernel, $k(x, y)= e^{-\frac{1}{2\sigma^2}||x-y||^2}$ and the laplace kernel, $k(x, y)= e^{-\frac{1}{\sigma^2}||x-y||}$, where $\sigma > 0$. In \cite{gretton2005kernel}, the authors recommend selecting the bandwidth kernel, $\sigma$, based on the \textit{median heuristic}, which computes $\sigma$ according to, 
\begin{equation}
\sigma^2=\text{median}\{||x_i-x_j||:i,j = 1,...,n \}.
\end{equation}

This heuristic is a good starting point, however, it has been shown to be sub-optimal in high-dimensional and small-sample cases as shown in \cite{muandet2014kernel} and \cite{ramdas2015decreasing}. Choosing the proper kernel is often done on a per case basis by maximizing test power. Overall kernel selection is a difficult problem that is still actively researched.

Originally in \cite{gretton2012kernel}, it was thought the MMD does not suffer from the curse of dimensionality when used to compare distributions in higher dimensions. However, it was shown in \cite{ramdas2015decreasing} that indeed the MMD does struggle in higher dimensions.

\iffalse
\hl{REWORD: We call the function that achieves the supremum, the witness function because it is the function that witnesses the difference in the two distributions. This means that we can interpret the witness function as showing where the estimated densities of
p and q are most different.}

The witness function 
\begin{equation}
f(x)=\mathbb{E}_{x^{\prime} \sim p}\left[k\left(x, x^{\prime}\right)\right]-\mathbb{E}_{x^{\prime} \sim q}\left[k\left(x, x^{\prime}\right)\right]
\end{equation}

which can also be estimated from finite samples of data by:
\begin{equation}
\hat{f}(x)=\frac{1}{m} \sum_{i=1}^{m} k\left(x, x_{i}\right)-\frac{1}{n} \sum_{i=1}^{n} k\left(x, y_{i}\right)
\end{equation}
Thus, as [need citation] points at, the witness function tracks where the densities of $X$ and $Y$ are most different. 
Kernel selection is important because it decides the kind of witness functions that can be learned. For most applications it is simply set to the RBF kernel but ideally it should be selected based on maximizing test power.
\fi 
 
Using the properties of the MMD and characteristic kernel functions, a non-parametric two-sample test can now be defined to solve the following non-parametric two-sample hypothesis test. Assuming $P$ and $Q$ are defined as above, 


In \cite{gretton2012kernel}, the authors defined a two-sample kernel test statistic using the MMD as a distance measure for comparing two probability distributions. 
1. Choosing a rejection threshold, $c_\alpha > 0$, and kernel function, $k$, as defined above.
2. Estimate MMD$(P,Q)$ using equation 6 or 7.
3. If $\hat{MMD} > c_\alpha$, reject $H_0$, else accept $H_0$

Notice the choice of kernel function and threshold here is critical for assessing performance. Because noisy samples are used to estimate the distributions of $P$ and $Q$, it will almost certainly never be exactly zero even if a characteristic kernel function is used and even if both samples come from the exact same distribution.

%Under H0, mMMD converges to distribution depending on data, k

\section{Related Work}

One of the first papers to use the term \textit{kernel change point detection} was in \cite{desobry2005online}. The authors present an online kernel change point detection model based on single class support vector machines ($\nu$-SVMs). They train a single class support vector on a past set, $\mathbf{x}_{t,1}={x_{t-m_1},...,x_{t-1}}$ of size $m_1$ and train another single class support vector on a future set $\mathbf{x}_{t,2}={x_t,...,x_{t+m_2-1}}$ of size $m_2$. A ratio is then computed between the two sets that acts as the dissimilarity measure in Hilbert space. If the sets are sufficiently dissimilar over some predetermined threshold,then a change point is assigned to the time step that spits the two sets of data. The authors argue that a dissimilarity measure between kernel projection of points in a Hilbert space should estimate the \textit{density supports} rather than estimate the probability distributions of each set of points. While this approach inspired a lot of interesting research that will be discussed below, it has not been studied since.

In \cite{harchaoui2007retrospective}, the authors approached the off-line change point problem with a fixed number of change points using kernel change point detection. This was further extended to an unknown number of change points in \cite{arlot2012kernel}. Garreau shows their KCP procedure outputs an off-line segmentation near optimal with high probability. Offline kernel change point detection differs in techniques than the online one, but they do have several common benefits. One is that it operates on any kind of data for which a kernel can properly reproduce a Hilbert space. For example, it can be applied to image data, histogram data, as well as $d-$dimensional vectors in $\mathbb{R}^d$. Lastly, the authors recommend choosing the kernel based on best possible signal to noise ratio. Therefore, some prior knowledge of a reference or training set is necessary for calibrating the kernel. 

In  \cite{li2015m}, the authors make use of the B-test introduced in \cite{zaremba2013b} and develop an offline and online change point detection algorithm called the MStats algorithm (the authors also refer to this algorithm as the Scan-B algorithm in a follow-up paper). At each time-step, the online model samples new data from a window of size $B_0$ and does a B-test with $N$ past samples that are kept as reference samples. 

$$Z_{B_{0}, t}:=\frac{1}{N} \sum_{i=1}^{N} \operatorname{MMD}_{u}^{2}\left(X_{i}^{\left(B_{0}, t\right)}, Y^{\left(B_{0}, t\right)}\right)$$

The resulting test statistic is then normalized by $Z_{B_{0}, t}/\sqrt{Var[Z_{B_0}]}$ where the authors provide a theoretical calculation of $\text{Var}[Z_B]$. If the normalized test statistic exceeds some predefined threshold then a change point is declared. The B-test is memoryless in the sense that the statistic is calculated each time and only the value of the last calculation has any weight. This is similar to a control chart that calculates a z-score at each iteration. Adjusting the size of the window, $B_0$, results in the usual trade-off of performance in online change point detection. A smaller block size will have a smaller computational cost and a smaller detection delay but will result in higher type II error and, as a result, worse test power.

From here, theoretical bounds are developed for the average run length and expected detection delay. Experiments are done on real-data sets including a speech dataset  and the Human Activity Sensing Consortium (HASC) dataset where the performance was better than the relative density-ratio (RDR) algorithm described in \cite{liu2013change}.

More recently in \cite{chang2019kernel}, a  kernel change point detection method is proposed that uses deep generative models to augment the test power of the kernel two sample test statistic. They point out MMD's lack of test power when using limited samples from the new distribution, $Q$, which may easily leading to over-fitting with kernels. Thus they use a generative adversarial neural network (GAN), trained on historical samples of $X \sim  P$  with noise injected into $X$. This surrogate distribution is then used in conjunction with possible change points to improve the test power of a modified MMD measure that makes use of compositional kernels.

The method is compared to other prominent change point methods for off-line  change detection such as the aforementioned MStats-KCPD, LSTNet, and Gaussian process change point models. All comparisons are done on synthetic data with piece-wise i.i.d. data. All methods are benchmarked using the AUC metric for classification performance and it is shown the KL-CPD method is competitive or better than the state of the art methods.  Furthermore, the AUC performance is maintained as the dimensionality of the data is increased, making their kernel learning framework very interesting for future off-line change point detection. It remains to be seen if this framework can be adopted in an online context where time to detection is a key constraint on practicality.

In the online setting, several methods use kernel embeddings with a two-sample hypothesis test. This is done in a similar vein to the classic CUSUM and Shewart control charts. They all make use of the maximum mean discrepancy (MMD) test statistic for a two-sample kernel hypothesis test. 

%https://media.nips.cc/nipsbooks/nipspapers/paper_files/nips28/reviews/1852.html

A modified, "no-prior-knowledge" exponentially-weighted moving average (NEWMA) is introduced in \cite{keriven2018newma}. Based on the standard exponentially weighted moving average, NEWMA computes two EWMA statistics of different weights. If the difference between the two EWMA statistics exceeds a predefined threshold then a changepoint is declared at that time step. The point of using two EWMA statistics is for one to have a larger forgetting factor. Any recent changes in a distribution will weigh heavily on one statistic, resulting in a large difference between the two statistics. If this this statistical difference exceeds a predefined threshold, then a change point is declared at this point in time.

Since a standard EWMA is a parametric method, the authors apply a kernel mapping function, $\Psi$, to the data prior to applying the exponential weights. This provides a memoryless, non-parametric, online changepoint detection method that does not need to constantly store all previously streamed data. Once the statistics are updated at each iteration, the raw data may be discarded. This characteristic makes it especially useful in applications where security is a concern.

While kernel mean embeddings could be used for approximating, $\Psi$, as is the case for standard implementations of MMD, this would require the storage of past examples of data. Because the authors aim to reduce run-time cost and storage cost, they use a Random Fourier Features (RFF) approach for estimating $\Psi$ (Note RFF is sometimes referred to as \textit{random kitchen sinks}). There are several approaches available for optimizing the RFF approach that are well studied in the literature, namely the standard RFF implementation, the FastFood implementation introduced in \cite{le2014fastfood}, and Optical Processing Unit implementation from \cite{saade2016random}. 

The three implementations of RFF and are compared to the MStats (Scan-B) algorithm by running empirical experiments on synthetic and real datasets. The synthetic datasets are run using streaming data that is generated from different Gaussian mixture models. They use an audio dataset for testing on real data. The variants of NEWMA are similar, if not better than MStats (Scan-B) in terms of missed detection percentage. In terms of average detection delay and false alarm trade-off, the NEWMA algorithm and its variants appear to be mildly better as well. The largest advantage of the NEWMA variants over the MStats (Scan-B) method is in the execution time. MStats (Scan-B)'s execution scales linearly with window size, while NEWMA's execution time does not depend on window size.

Finally, in a recent, preprint paper \cite{flynn2019change}, a kernel CUSUM (KCUSUM) algorithm is proposed, where the classic CUSUM algorithm is adapted using the MMD statistic for online detection. The authors use a modified, unbiased MMD statistic that can be computed in linear time. This formulation of the MMD statistic was originally defined in section 6 of \cite{gretton2012kernel} as:

$$\operatorname{MMD}_{l}^{2}[\mathcal{F}, X, Y] :=\frac{1}{m_{2}} \sum_{i=1}^{m_{2}} h\left(\left(x_{2 i-1}, y_{2 i-1}\right),\left(x_{2 i}, y_{2 i}\right)\right)$$

Where,

$$h\left((x_i, x_j), (y_i, y_j)\right):=k\left(x_{i}, x_{j}\right)+k\left(y_{i}, y_{j}\right)-k\left(x_{i}, y_{j}\right)-k\left(x_{j}, y_{i}\right)$$

The algorithm functions as follows, every two observations, the MMD$_l$ is calculated using newly observed data points and data points sampled from some \textit{reference} distribution that is known at the outset. This reference distribution can be thought of as the "in-control" distribution of the data-stream that new observations are compared to. The calculated MMD$_l$ acts as the update term to the cumulative sum statistic, hence the name KCUSUM. If this kernel cumulative sum statistic exceeds some predefined threshold, then a change point is identified. 

Besides its speed of computation, an additional benefit of MMD$_l$ is it is normally distributed under the null distribution unlike the quadratically-calculated MMD$_u$. This facilitates analysis of bounds and provides statistical guarantees for worst-case detection delays and time to false alarm rate. While this non-parametric approach can detect any change in the distribution of a sequence, it does struggle with more complicated distributional changes such as variance changes of a single dimension and changes beyond first and second-order moments.

%In 2016, James et al. \cite{james2016leveraging} proposed using energy statistics to test the significance of a change point that is robust to anomalies. They point out that their work is the first to accurately detect change points with fast time to detection while not being affected by extreme outliers that are not change points. They compare their E-divisive with medians algorithm to the parametric PELT technique. 

\section{Our Approach}

This section describes our novel method for change point detection.


\section{Synthetic Datasets}
A common difficulty in change point detection is evaluating the performance of an algorithm with datasets that aren't overly simplistic and difficult enough to ascertain some real world use.

Unlike fields like image recognition where datasets like MNIST provide a common benchmark, there are no standard datasets that are widely used across the change point detection literature for evaluating new methods. Most papers propose experiments that are relevant for the specific problem they are trying to solve  but lack examples or explanations of when their method would not be applicable.  Furthermore, because change point evolved out of the statistics literature, many papers focus on theoretical results and provide minor experimental results if any.

Given the empirical focus of this thesis, we attempt to put together the most comprehensive experiments using synthetic. To the best of our knowledge, no change point detection paper covers as many variations as presented in this thesis. While synthetic datasets are idealistic in their formulation, they provide a good starting point for comparing different methods because many variables can be controlled for. Often in the real world, the exact location of change points is not known. Therefore, it is important for the evaluation of a change point detection algorithm that it performs competitively on synthetic data.

Inspired by recent papers \cite{chang2019kernel} and \cite{flynn2019change} that attempt to bridge the gap between the statistics and machine-learning literature, the following synthetic datasets are created: change in mean, scaling variance, alternating between two Gaussian mixtures, and alternating between random distributions. It is truly hard to properly generalize all the possible situations a non-parametric algorithm may be used in, but the synthetic cases presented in this thesis are common across domains and cover a range of applications.

For a change in mean, a change point is inserted in the time series at some random time where the mean is shifted either positively or negatively. There are two variants to this scenario. In the first, the mean change is in all dimensions simultaneously. In the second variation, the mean change is in only one dimension making it harder to detect. 

For each experiment above, a Monte-Carlo approach is used to estimate time to false alarm, detection delay, and test power. 

\begin{center}
\captionof{table}{Synthetic Datasets Summary}
\begin{tabular}{SSSSSS} \toprule
    {Type of Change} & {No. of Dimensions} & {Length} & {No. of Changepoints}  \\ \midrule
    {Mean (all dimensions)}  & 20 & 5000 & 50  \\
    {Mean (single dimension)}  & 20 & 5000 & 50  \\
    {Variance}  & 20 & 5000 & 50  \\
    {Frequency}  & 1 & 5000  & 50  \\
    {Correlation} & 2 & 5000  & 50  \\
    {Blobs}  & 10 & 10 000  & 50  \\
    {GMMs}  & 50 & 10 000  & 50  \\ \bottomrule
\end{tabular}
\end{center}



