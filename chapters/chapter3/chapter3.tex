\chapter{Experimental Results}
\section{Datasets}
A common difficulty in change-point detection is evaluating the performance of an algorithm with datasets that aren't overly simplistic and difficult enough to ascertain some real world use.

Unlike fields like image recognition or XXX, there are no standard datasets that are widely used across literature for evaluating new methods. Most papers propose experiments that are relevant for the specific problem they are trying to solve.  Furthermore, because change-point evolved out of the statistics literature, the focus for years was mostly on theoretical results.

Given the empirical focus of this thesis, we attempt to put together the most comprehensive, controlled experiments using both synthetic and real-world datasets.
\subsection{Synthetic Datasets}
To the best of our knowledge, no change-point detection paper covers as many variations as presented in this thesis. While synthetic datasets are idealistic in their formulation, they provide a good starting point for comparing different methods because many variables can be controlled for. Often in the real world, the exact location of change-points is not Therefore, it is important for the evaluation of a change-point detection algorithm that it performs competitively on synthetic data.

Inspired by recent papers \cite{chang2019kernel} and \cite{flynn2019change} that attempt to bridge the gap between the statistics and machine-learning literature, the following synthetic datasets are create: jumping mean, scaling variance, alternating between two Gaussian mixtures, and alternating between random distributions.

For each experiment above, a Monte-Carlo approach is used to estimate time to false alarm, detection delay, and test power. 

\subsection{Real World Datasets}
Subsection Test
