\chapter{Kernel Change Point Detection}
In this chapter, we focus on the class of change point models that leverage kernel techniques, known as \textit{kernel change point detection}. All methods used in the experiments section make use of the maximum mean discrepancy (MMD) for their change point test statistic, therefore, a short review of the MMD statistic is presented first. For an extensive review of kernel embeddings and their applications see \cite{muandet2017kernel}.

\section{Maximum Mean Discrepancy}
\label{mmd}
Recall we are looking for a suitable statistic to solve the change point hypothesis test presented in \ref{formula_cpd}. The idea behind MMD is the following: rather than comparing the expected value of $P$ and $Q$ in their original feature space, why not compare their expected value in a different feature space that makes discerning between $P$ and $Q$ more effective. Doing so would allow the detection of differences between $P$ and $Q$ that were not possible before. A classic technique for comparing samples in a different feature space is through the use of kernel functions. Therefore, we'll start with a brief definition of kernel functions.

Suppose we have a non-empty set  $\mathcal{X}$ for which $x,y \in \mathcal{X}$. A positive definite function $k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ is called a \textit{kernel} if there exists a Hilbert space $ \mathcal{H}$ with corresponding map $\phi: \mathcal{X} \rightarrow \mathcal{H}$. From \cite{scholkopf2002learning}, a kernel is defined as,
\begin{equation}
\label{kernel}
k(x,y) = \left\langle \phi(x), \phi(y) \right\rangle_\mathcal{H}
\end{equation}
where $\left\langle \cdot, \cdot \right\rangle_\mathcal{H}$ is the \textit{inner product} in $ \mathcal{H}$. Detailed properties of Hilbert spaces and inner product spaces can be found in \cite{young1988introduction}.  It can be shown that for a positive definite kernel function, there exists a unique \textit{reproducing kernel Hilbert space} (RKHS) for which $k$ is a reproducing kernel. Proofs for these statements are out of scope of this thesis but can be found in\cite{scholkopf2002learning}. The usefulness of these properties lies in the fact that an inner product in $ \mathcal{H}$ can be computed implicitly through a well-defined kernel function.

%Using \ref{kernel}, a $kernel\text{ }matrix$ or $Gram\text{ }matrix$ can defined as:
%\begin{equation}
%K_{ij} := k(x_i, x_j)
%\end{equation}

While this new feature space representation is helpful, it is not necessarily unique. This a problem because $P$ and $Q$ may mapped into a space where they are similar when in fact they are not. Consequently, we need to ensure the kernel mapping is \textit{injective}, i.e. that the mapping from $\mathcal{X}$ to  $\mathcal{H}$ is one to one. See 2.1.1 of \cite{atkinson2005theoretical} for more details on injective functions.

To ensure the kernel mapping is injective, we can make use of a special class of kernel functions known as \textit{characteristic kernels} that are introduced in \cite{fukumizu2008kernel} and \cite{sriperumbudur2011universality}. The papers show that if $k$ is characteristic then the mapping of $\phi$ into the $\mathcal{H}$ is unique. This means if a characteristic kernel is used, then $X$ and $Y$ can be uniquely mapped to another feature space where the expected value in $\mathcal{H}$ is also unique.  By combining the previous properties of kernel functions, we can now understand where the idea of the maximum mean discrepancy comes from. Rather than comparing the expected values of a moment in the original feature space like we did in 2.1, we will compare the expected values in this new feature space.

More formally, suppose $n$ samples from a set $X = \{x_1, x_2, ..., x_n\}$ and $m$ samples from a different set $Y=\{y_1, y_2, ..., y_m\}$ are derived from $\mathcal{X}$. Assume both are distributed as $X \sim  P$ and $Y \sim Q$ respectively. Using $\phi$ as it is described in \ref{kernel}, the maximum mean discrepancy (MMD) is defined in \cite{smola2007hilbert} as:
\begin{equation}
\label{mmd_theory}
\text{MMD}(P,Q)=|| \underset{X \sim P}{\mathbb{E}}[\phi(X)] -  \underset{Y \sim Q}{\mathbb{E}}[\phi(Y)]||_\mathcal{H},
\end{equation}

where $||\cdot||_\mathcal{H}$ is the \textit{norm} on $\mathcal{H}$. The MMD can be interpreted as the distance in $\mathcal{H}$ between the kernel mean embeddings of the features. As long as a kernel function can be defined on the given data structure, the MMD can be used. This is why it can be applied to non-numeric data such as strings, graphs, and other structured domains \cite{hofmann2008kernel}. From theorem 5 of \cite{gretton2012kernel}, the theoretical MMD$(P,Q) = 0$ if and only if $P=Q$. This fact arises from the injective property defined earlier. While this is a nice property, in practice our noisy estimates of $P$ and $Q$ means an estimate of MMD will never be exactly zero even if our samples are from the same distribution. Nonetheless, the power of MMD lies in its ease of computation and the fact that it can detect any type of parameter difference between $P$ and $Q$. 
%This is because MMD is essentially a moment generaing function of P and Q.......

Using $X$ and $Y$, the unbiased estimate of the squared MMD is shown in \cite{gretton2012kernel} to be:
\begin{equation}
\label{mmd_unbiased}
\begin{split}
\widehat{\text{MMD}}_{u}^{2}(X, Y)=\frac{1}{m(m-1)} \sum_{i=1}^m \sum_{ j=1}^{m} k\left(x_{i}, x_{j}\right)-\frac{2}{m n} \sum_{i=1}^m \sum_{ j=1}^{n} k\left(x_{i}, y_{j}\right)+ \\
\frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j=1}^{n} k\left(y_{i}, y_{j}\right)
\end{split}
\end{equation}

and the biased estimate of the squared MMD is
\begin{equation}
\label{mmd_biased}
\widehat{\text{MMD}}_{b}^{2}(X, Y)= \frac{1}{n^{2}} \sum_{i=1}^{n} \sum_{j=1}^{n} k\left(x_{i}, x_{j}\right)+\frac{1}{m^{2}} \sum_{i=1}^{m} \sum_{j=1}^{m} k\left(y_{i}, y_{j}\right)-\frac{2}{n m} \sum_{i=1}^{n} \sum_{j=1}^{m} k\left(x_{i}, y_{j}\right).
\end{equation}

Because both \ref{mmd_unbiased} and \ref{mmd_biased} are computed in $\mathcal{O}((m + n)^2)$ time, they will be referred to as quadratic-time estimates of MMD. For high-dimensional datasets (large $m$ or $n$), using a quadratic-time estimate of MMD impractical. 

An alternative is proposed in section 6 of \cite{gretton2012kernel} that is linear in computation time and still uses all available data. Letting $X$ and $Y$ have equal size $m$ for notational simplicity, then
\begin{equation}
\label{mmd_linear}
\widehat{\text{MMD}}_{l}^{2}(X, Y) :=\frac{1}{m_{2}} \sum_{i=1}^{m_{2}} h\left(\left(x_{2 i-1}, y_{2 i-1}\right),\left(x_{2 i}, y_{2 i}\right)\right)
\end{equation}

is the linear-time estimate of MMD, where $m_2=\frac{m}{2}$ and
\begin{equation}
h\left((x_i, x_j), (y_i, y_j)\right):=k\left(x_{i}, x_{j}\right)+k\left(y_{i}, y_{j}\right)-k\left(x_{i}, y_{j}\right)-k\left(x_{j}, y_{i}\right).
\end{equation}
This estimation of MMD is compute in $\mathcal{O}(m)$,which is significantly faster than the quadratic-time estimates. However, it pays for this speed with accuracy as the MMD$_l$ estimate has higher variance than MMD$_u$.

Now that we have a suitable metric for computing a test statistic between samples $X$ and $Y$, a hypothesis test can be constructed using the MMD statistic as shown in \cite{gretton2012kernel}. Similar to previous hypothesis tests, a threshold and kernel function is chosen to adequately balance between type-I and type-II error. The null hypothesis is that $X$ and $Y$ are equivalent in their kernel mean elements.  If the MMD is greater than the threshold, then the null hypothesis is rejected and the alternative hypothesis is accepted. In this case, the alternative hypothesis would indicate $X$ and $Y$ differ in one  or more of their statistical moments.

\subsection{Choice of Kernel}
Notice we have not yet explicitly chosen a kernel function for calculating the MMD statistic. Choosing the proper kernel is often done on a per case basis. Overall optimal kernel selection is a difficult problem that is still actively researched \cite{gretton2012optimal}. We will only discuss one kernel function in this thesis, the Gaussian kernel. The Gaussian kernel is the most widely used kernel and it is the only kernel used in the kernel change point detection methods discussed in \ref{related_works}.  For other possible choices of kernel functions see table 3.1 in \cite{muandet2017kernel}.

Given two $d$ dimensional real vectors $x, y \in \mathbb{R}^d$, the Gaussian kernel is defined as:
\begin{equation}
k(x, y)= e^{-\frac{1}{\sigma^2}||x-y||^2}
\end{equation}
where $\sigma > 0$ and is called the \textit{kernel bandwidth}. In \cite{gretton2005kernel}, the authors recommend selecting the bandwidth based on the \textit{median heuristic}, which computes $\sigma$ according to, 
\begin{equation}
\sigma^2=\text{median}\{||x_i-x_j||:i,j = 1,...,n \},
\end{equation}

which is the median of distances between all pairs of points. It is not clear where this heuristic was first introduced although many papers incorrectly cite \cite{scholkopf2002learning}. Despite its unknown origins, it has been shown to be empirically suitable for many kernel applications. In \cite{garreau2017large}, a large sample analysis of the median heuristic was done to expand what little theoretical understanding there is of it. However, it has been shown to be sub-optimal in high-dimensional and small-sample cases as shown in \cite{muandet2014kernel} and \cite{ramdas2015decreasing}. 


%One such family of statistical distances is the group of \textit{integral probability metrics}, first described in \cite{muller1997integral}. The basic setup of these metrics is consider a space of functions, $\mathcal{F}$, where every function $f \in \mathcal{F}$ is a mapping such that $f : \mathcal{X}  \rightarrow \mathbb{R}$. An IPM between $P$ and $Q$ 

%One advantage of using the MMD for comparing distributions over other classic techniques like Kullbackâ€“Leibler (K-L) divergence is the density of the distributions do not have to be estimated as an interim step. 


%  The kernel mean embedding can be thought of applying the \textit{kernel trick} to probability measures and mapping them into a higher dimensional feature space. Rather than applying the kernel trick to individual data points and mapping them to an implicit feature space, the kernel mapping, $\phi$, (also called feature map) will be applied to a probability distribution in order to represent it in a reproducing kernel Hilbert space (RKHS), that is $\phi: \mathcal{X} \rightarrow \mathcal{H}$.

%The kernel mean embeddings are denoted by $\mu_P = \mathbb{E}_{X \sim P}[\phi(X)] $ and $\mu_Q =\mathbb{E}_{Y \sim Q}[\phi(Y)]$ for samples $X$ and $Y$ respectively, where $\mu_P, \mu_Q \in \mathcal{H}$.


%Typically, the actual distributions $P$ and $Q$ are unknown, making $\mu_P$ and $\mu_Q$ also unknown. Therefore, for applications, empirical estimates using the sampled data $X$ and $Y$ must be used instead. 

%Originally in \cite{gretton2012kernel}, it was thought the MMD does not suffer from the curse of dimensionality when used to compare distributions in higher dimensions. However, it was shown in \cite{ramdas2015decreasing} that indeed the MMD does struggle in higher dimensions.

\iffalse
\hl{REWORD: We call the function that achieves the supremum, the witness function because it is the function that witnesses the difference in the two distributions. This means that we can interpret the witness function as showing where the estimated densities of
p and q are most different.}

The witness function 
\begin{equation}
f(x)=\mathbb{E}_{x^{\prime} \sim p}\left[k\left(x, x^{\prime}\right)\right]-\mathbb{E}_{x^{\prime} \sim q}\left[k\left(x, x^{\prime}\right)\right]
\end{equation}

which can also be estimated from finite samples of data by:
\begin{equation}
\hat{f}(x)=\frac{1}{m} \sum_{i=1}^{m} k\left(x, x_{i}\right)-\frac{1}{n} \sum_{i=1}^{n} k\left(x, y_{i}\right)
\end{equation}
Thus, as [need citation] points at, the witness function tracks where the densities of $X$ and $Y$ are most different. 
Kernel selection is important because it decides the kind of witness functions that can be learned. For most applications it is simply set to the RBF kernel but ideally it should be selected based on maximizing test power.
\fi 
 
\section{Related Work}
\label{related_works}

\subsection{Offline Kernel Change Point Detection}
Offline kernel change point detection differs in techniques than the online one, but they do have several common features. One is that it operates on any kind of data for which a kernel can properly reproduce a Hilbert space and thereby make use of the MMD statistic. The second is that all offline kernel change point methods cited here make use of the Gaussian kernel, which speaks to the ubiquitousness of this kernel function. 

In \cite{harchaoui2007retrospective}, the authors approached the offline change point problem with a fixed number of change points using kernel change point detection. This was further extended to an unknown number of change points in \cite{arlot2012kernel}. The authors show their kernel change point detection procedure outputs an offline segmentation near optimal with high probability. Lastly, the authors recommend choosing the kernel based on best possible signal to noise ratio. Therefore, some prior knowledge of a reference or training set is necessary for calibrating the kernel. 

More recently in \cite{chang2019kernel}, a  kernel change point detection method is proposed that uses deep generative models to augment the test power of the kernel two sample test statistic. They point out MMD's lack of test power when using limited samples from the new distribution, $Q$, which may easily leading to over-fitting with kernels. Thus they use a generative adversarial neural network (GAN), trained on historical samples of $X \sim  P$  with noise injected into $X$. This surrogate distribution is then used in conjunction with possible change points to improve the test power of a modified MMD measure that makes use of compositional kernels.

The method is compared to other prominent change point methods for offline change detection such as the aforementioned MStats-KCPD, LSTNet, and Gaussian process change point models. All comparisons are done on synthetic data with piece-wise i.i.d. data. All methods are benchmarked using the AUC metric for classification performance and it is shown the KL-CPD method is competitive or better than the state of the art methods.  Furthermore, the AUC performance is maintained as the dimensionality of the data is increased, making their kernel learning framework very interesting for future off-line change point detection. It remains to be seen if this framework can be adopted in an online context where time to detection is a key constraint on practicality.

See section 4.2.3 \cite{truong2018review} for a review of kernel-based methods used for offline detecion. 

\subsection{Online Kernel Change Point Detection}
One of the first papers to use the term kernel change point detection was in \cite{desobry2005online}. The authors present an online kernel change point detection model based on single class support vector machines ($\nu$-SVMs). They train two single class support vectors, one on a past set and one on a future set. A ratio is then computed between the two sets that acts as the dissimilarity measure in Hilbert space. If the sets are sufficiently dissimilar over some predetermined threshold,then a change point is assigned to the time step that spits the two sets of data. The authors argue that a dissimilarity measure between kernel projection of points in a Hilbert space should estimate the \textit{density supports} rather than estimate the probability distributions of each set of points. While this approach inspired a lot of interesting research that will be discussed below, it has not been studied since.

The following methods are all inspired by the classic algorithms from section \ref{classic_algo}. They all make use of a variation of the maximum mean discrepancy (MMD). They are also the most recent online kernel techniques developed and will be used for experiments in later sections. Thus, it is worth detailing how they work.

In  \cite{li2015m}, the authors make use of the B-test introduced in \cite{zaremba2013b} and develop an offline and online change point detection algorithm called the MStats algorithm (the authors also refer to this algorithm as the Scan-B algorithm in a follow-up paper). At each time-step, the online model samples new data from a window of size $B_0$ and computes a B-test statistic with $N$ past samples that are kept as reference samples. 
\begin{equation}
Z_{B_{0}, t}:=\frac{1}{N} \sum_{i=1}^{N} \operatorname{MMD}_{u}^{2}\left(X_{i}^{\left(B_{0}, t\right)}, Y^{\left(B_{0}, t\right)}\right)
\end{equation}

The resulting test statistic is then normalized by $Z_{B_{0}, t}/\sqrt{Var[Z_{B_0}]}$ where the authors provide a theoretical calculation of $\text{Var}[Z_B]$. If the normalized test statistic exceeds some predefined threshold then a change point is declared. The B-test is memoryless in the sense that the statistic is calculated each time and only the value of the last calculation has any weight. This is similar to a control chart that calculates a z-score at each iteration. Adjusting the size of the window, $B_0$, results in the usual trade-off of performance in online change point detection. A smaller block size will have a smaller computational cost and a smaller detection delay but will result in higher type II error and, as a result, worse test power.

From here, theoretical bounds are developed for the time to false alarm rate and expected detection delay. Experiments are done on real-data sets including a speech dataset  and the Human Activity Sensing Consortium (HASC) dataset where the performance was better than the relative density-ratio (RDR) algorithm described in \cite{liu2013change}.

%https://media.nips.cc/nipsbooks/nipspapers/paper_files/nips28/reviews/1852.html

A modified, "no-prior-knowledge" exponentially-weighted moving average (NEWMA) is introduced in \cite{keriven2018newma}. Based on the standard exponentially weighted moving average, NEWMA computes two EWMA statistics of different weights. If the difference between the two EWMA statistics exceeds a predefined threshold then a changepoint is declared at that time step. The point of using two EWMA statistics is for one to have a larger forgetting factor. Any recent changes in a distribution will weigh heavily on one statistic, resulting in a large difference between the two statistics. If this this statistical difference exceeds a predefined threshold, then a change point is declared at this point in time.

Since a standard EWMA is a parametric method, the authors apply a kernel mapping function, $\Psi$, to the data prior to applying the exponential weights. This provides a memoryless, non-parametric, online change point detection method that does not need to constantly store all previously streamed data. Once the statistics are updated at each iteration, the raw data may be discarded. This characteristic makes it especially useful in applications where security is a concern.

While kernel mean embeddings could be used for approximating, $\Psi$, as is the case for standard implementations of MMD, this would require the storage of past examples of data. Because the authors aim to reduce run-time cost and storage cost, they use a Random Fourier Features (RFF) approach for estimating $\Psi$ (Note RFF is sometimes referred to as \textit{random kitchen sinks}). Because of the use of RFF, this is the only method discussed that does not use the kernel function for an implicit representation of the data in another feature. Instead, the RFF explicitly maps the data to a lower dimension Euclidean product space using a randomized feature map $\mathbf{z}: \mathbb{R}^d \rightarrow \mathbb{R}^m$ such that:
\begin{equation}
k(\mathbf{x}, \mathbf{y})=\langle\phi(\mathbf{x}), \phi(\mathbf{y})\rangle_{\mathcal{H}} \approx \mathbf{z}(\mathbf{x})^{\top} \mathbf{z}(\mathbf{y})
\end{equation}

where $\mathbf{z}(\mathbf{x}):=\mathbf{W}^{\top} \mathbf{x}$. Each element, $w_{ij}$, is sampled from a distribution that is the Fourier transform of a translation invariant kernel. See XXX for why this technique is a valid approximation.

There are several approaches available for calculating RFF already studied in the literature and the authors use three common ones for comparison. They use the standard RFF implementation from XXXX, the FastFood implementation introduced in \cite{le2014fastfood}, and Optical Processing Unit implementation from \cite{saade2016random}. 

The three implementations of RFF and are compared to the MStats (Scan-B) algorithm by running empirical experiments on synthetic and real datasets. The synthetic datasets are run using streaming data that is generated from different Gaussian mixture models. They use an audio dataset for testing on real data. The variants of NEWMA are similar, if not better than MStats (Scan-B) in terms of missed detection percentage. In terms of average detection delay and false alarm trade-off, the NEWMA algorithm and its variants appear to be mildly better as well. The largest advantage of the NEWMA variants over the MStats (Scan-B) method is in the execution time. MStats (Scan-B)'s execution scales linearly with window size, while NEWMA's execution time does not depend on window size.

Finally, in a recent, preprint paper \cite{flynn2019change}, a kernel CUSUM (KCUSUM) algorithm is proposed, where the classic CUSUM algorithm is adapted using the faster MMD$_l$ statistic for online detection. The algorithm functions as follows, every two observations, the MMD$_l$ is calculated using newly observed data points and data points sampled from some \textit{reference} distribution that is known at the outset. This reference distribution can be thought of as the "in-control" distribution of the data-stream that new observations are compared to. The calculated MMD$_l$ acts as the update term to the cumulative sum statistic. If this kernel cumulative sum statistic exceeds some predefined threshold, then a change point is identified. Unlike the previous methods that set the Gaussian kernel's bandwidth using the median heuristic, the authors chose to fix the bandwidth to one.

Besides its speed of computation, an additional benefit of MMD$_l$ is it is normally distributed under the null distribution unlike the quadratic-time estimate. This facilitates analysis of bounds and provides statistical guarantees for worst-case detection delays and time to false alarm rate. While this non-parametric approach can detect any change in the distribution of a sequence, it does struggle with more complicated distributional changes such as variance changes of a single dimension and changes beyond first and second-order moments.

%In 2016, James et al. \cite{james2016leveraging} proposed using energy statistics to test the significance of a change point that is robust to anomalies. They point out that their work is the first to accurately detect change points with fast time to detection while not being affected by extreme outliers that are not change points. They compare their E-divisive with medians algorithm to the parametric PELT technique. 

\section{Our Approach}

This section describes our novel method for change point detection.

\subsection{Evaluation}
In cases where a change point is detected, the average detection delay (ADD) is used to estimate the average time it takes for a change point to be detected. It is computed by comparing the time difference between a predicted change point and an actual change point. This difference is then normalized the total by the total number of change points:
\begin{equation}
\text{ADD} = \frac{\sum_i^{\#CP} |\hat{t_i} - t^*|}{\#CP}
\end{equation}
The closer the predicted change points are to the actual change points, the smaller the average detection delay. Because it relies on ground truth change points, it can only be used when experimenting on synthetic data where all the information is available.%Therefore, the ADD can range from $(0, \inf)$. 

If labelled change points are available for a real world dataset or a synthetic dataset, then the ground truth change point vector, 
$t^*$, is known. Evaluating performance in this case is the same as evaluating a binary classifier from a supervised learning problem. For example, the metrics from discussed in hypothesis testing, type-I error (false positives) and type-II error (false negatives), are commonly used for evaluating change point detection performance. The false positive rate (FPR) can be calculated by $FPR = FP / N$. Plotting the false positive rate and the true positive rate, gives the receiver operator characteristic curve. The area under this curve (AUC), is calculated and compared to random baseline performance of 0.5 that would be equivalent to classification by tossing a coin.


\subsection{Synthetic Datasets}
A common difficulty in change point detection is evaluating the performance of an algorithm with datasets that aren't overly simplistic and difficult enough to ascertain some real world use.

Unlike fields like image recognition where datasets like MNIST provide a common benchmark, there are no standard datasets that are widely used across the change point detection literature for evaluating new methods. Most papers propose experiments that are relevant for the specific problem they are trying to solve  but lack examples or explanations of when their method would not be applicable.  Furthermore, because change point evolved out of the statistics literature, many papers focus on theoretical results and provide minor experimental results if any.

Given the empirical focus of this thesis, we attempt to put together the most comprehensive experiments using synthetic. To the best of our knowledge, no change point detection paper covers as many variations as presented in this thesis. While synthetic datasets are idealistic in their formulation, they provide a good starting point for comparing different methods because many variables can be controlled for. Often in the real world, the exact location of change points is not known. Therefore, it is important for the evaluation of a change point detection algorithm that it performs competitively on synthetic data.

Inspired by recent papers \cite{chang2019kernel} and \cite{flynn2019change} that attempt to bridge the gap between the statistics and machine-learning literature, the following synthetic datasets are created: change in mean, scaling variance, alternating between two Gaussian mixtures, and alternating between random distributions. It is truly hard to properly generalize all the possible situations a non-parametric algorithm may be used in, but the synthetic cases presented in this thesis are common across domains and cover a range of applications.

For a change in mean, a change point is inserted in the time series at some random time where the mean is shifted either positively or negatively. There are two variants to this scenario. In the first, the mean change is in all dimensions simultaneously. In the second variation, the mean change is in only one dimension making it harder to detect. 

For each experiment above, a Monte-Carlo approach is used to estimate time to false alarm, detection delay, and test power. 

\begin{center}
\captionof{table}{Synthetic Datasets Summary}
\begin{tabular}{SSSSSS} \toprule
    {Type of Change} & {No. of Dimensions} & {Length} & {No. of Changepoints}  \\ \midrule
    {Mean (all dimensions)}  & 20 & 5000 & 50  \\
    {Mean (single dimension)}  & 20 & 5000 & 50  \\
    {Variance}  & 20 & 5000 & 50  \\
    {Frequency}  & 1 & 5000  & 50  \\
    {Correlation} & 2 & 5000  & 50  \\
    {Blobs}  & 10 & 10 000  & 50  \\
    {GMMs}  & 50 & 10 000  & 50  \\ \bottomrule
\end{tabular}
\end{center}



