\chapter{Kernel Change Point Detection}
In this chapter, we focus on the class of change point models that leverage kernel techniques, known as \textit{kernel change point detection}. A unique feature of kernel methods is the use of the maximum mean discrepancy (MMD) metric as the change point test statistic. Therefore, a short review of the MMD statistic is presented first.

%All methods used in the experiments section make use of the maximum mean discrepancy (MMD) for their change point test statistic, therefore, a short review of the MMD statistic is presented first.

% For an extensive review of kernel embeddings and their applications see \cite{muandet2017kernel}.

\section{Kernel-Based Test Statistic}
\label{mmd}
Recall we are looking for a suitable statistic to solve the change point hypothesis test presented in \ref{formula_cpd}. To solve this, we need a statistic that can measure the difference between two distributions, $P$ and $Q$. In our setting, this is particularly hard because we do not know $P$ or $Q$ and only have access to data generated by them. Ideally, the metric measuring the difference should return a small value if $P \approx  Q$ and large value if $P$ and $Q$ are very different from each other. This means we need a metric that can compute the distance between two probability distributions using only data drawn from them. It turns out the maximum mean discrepancy (MMD) introduced in \cite{gretton2012kernel} meets all these criteria.

The idea behind the MMD is the following: rather than comparing the expected value of $P$ and $Q$ in their original feature space, why not compare their expected value in a different feature space that makes discerning between $P$ and $Q$ more effective. Doing so would allow the detection of differences between $P$ and $Q$ that were not possible before. A classic technique for comparing samples in a different feature space is through the use of kernel functions. Therefore, we'll start with a brief background of kernel functions.

\subsection{Kernel Background}

Suppose we have a non-empty set  $\mathcal{X}$ for which $x,y \in \mathcal{X}$. A function $k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ is called a \textit{kernel} if there exists a \textit{feature space} $ \mathcal{H}$ with a corresponding \textit{feature map} $\phi: \mathcal{X} \rightarrow \mathcal{H}$. A kernel can be expressed as,
\begin{equation}
\label{kernel}
k(x,y) = \left\langle \phi(x), \phi(y) \right\rangle_\mathcal{H}
\end{equation}
where $\left\langle \cdot, \cdot \right\rangle_\mathcal{H}$ is the \textit{inner product} in $ \mathcal{H}$  \cite{scholkopf2002learning}. Using the kernel function, we can feed it two points and implicitly compute the dot product of these points in the transformed space. This is the so-called \textit{kernel trick}. It's a trick because the dot product can be computed without having to explicitly know the feature map or what the coordinates of $\phi(x)$ or $\phi(y)$ evaluate to. Lastly, because dot products are symmetric, this implies that the kernel function as defined above is also symmetric, i.e. $k(x, y) = k(y, x)$.

A related concept is the \textit{Gram matrix} which can be computed from a set of vectors $\{\mathbf{x}_1,..,\mathbf{x}_{\ell}\}$ as an $\ell \times \ell$ matrix $\mathbf{G}$, whose entries are $\mathbf{G}_{ij} = \left\langle \mathbf{x}_i, \mathbf{x}_j \right\rangle$. Using \ref{kernel}, we can define a \textit{kernel gram matrix} or just \textit{kernel matrix}, denoted by $\mathbf{K}$, as:
\begin{equation}
\mathbf{K}_{ij} := k(\mathbf{x}_i, \mathbf{x}_j ).
\end{equation}
By symmetry of the kernel, the kernel matrix is also symmetric $\mathbf{K}_{ij}=\mathbf{K}_{ji}$ or $\mathbf{K}^\text{T} = \mathbf{K}$. More explicitly, it is expressed as:

\begin{equation}
\begin{array}{lllll}
\hline \hline \mathbf{K} & 1 & 2 & \cdots & \ell \\
\hline 1 & k \left(\mathbf{x}_{1}, \mathbf{x}_{1}\right) & k\left(\mathbf{x}_{1}, \mathbf{x}_{2}\right) & \cdots & k
\left(\mathbf{x}_{1}, \mathbf{x}_{\ell}\right) \\
2 & k\left(\mathbf{x}_{2}, \mathbf{x}_{1}\right) & k\left(\mathbf{x}_{2}, \mathbf{x}_{2}\right) & \cdots & k\left(\mathbf{x}_{2}, \mathbf{x}_{\ell}\right) \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\ell & k\left(\mathbf{x}_{\ell}, \mathbf{x}_{1}\right) & k\left(\mathbf{x}_{\ell}, \mathbf{x}_{2}\right) & \cdots & k \left(\mathbf{x}_{\ell}, \mathbf{x}_{\ell}\right) \\
\hline \hline
\end{array}
\end{equation}

In most applications, $k$ is chosen to be positive definite. A function $k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ is a positive definite kernel if it is symmetric and
\begin{equation}
\sum_{i, j=1}^{n} c_{i} c_{j} k\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right) \geq 0,
\end{equation}
for any $n \in \mathbb{N}$, any choice of $\mathbf{x}_1,...,\mathbf{x}_n \in \mathcal{X}$ and any $c_1,...,c_n \in \mathbb{R}$. This is equivalent to requiring that the constructed kernel matrix has entirely positive eigenvalues. Furthermore, there always exists $\phi: \mathcal{X} \rightarrow \mathcal{H}$ for which equation \ref{kernel} holds  if $k$ is positive definite.

By selecting a positive definite kernel, the feature space $\mathcal{H}$ is actually a space of functions known as a \textit{reproducing kernel Hilbert space} (RKHS). While this may seem odd, it actually simplifies a lot of algebra because elements in a RKHS can be treated like finite vectors.  For the purposes of this  thesis, we will only deal with positive definite kernels to ensure the corresponding feature space is a RKHS.

%Given a kernel, neither the feature map nor the feature are uniquely determined from the above definition. Consider the following example. Let $\mathcal{X} \leftarrow \mathbb{R}$ and $k(x,y)=xy$ for all $x,y \in \mathbb{R}$. This is a trivial case in which $k$ is a kernel because the feature map can be set to the identity function such that $\phi(x)=x$ that maps into  $\mathcal{H} \leftarrow \mathbb{R}$. However, we can equally define $\phi: \mathcal{X} \rightarrow \mathbb{R}^2$ where $\phi(x) = (\frac{x}{\sqrt{2}}, \frac{x}{\sqrt{2}})$ that is also a feature map of $k$. We can see this by expanding $ \left\langle \phi(x), \phi(y) \right\rangle = \frac{x}{\sqrt{2}} \cdot \frac{y}{\sqrt{2}} +  \frac{x}{\sqrt{2}} \cdot \frac{y}{\sqrt{2}}=xy=k(x,y)$.

 %Conversely every symmetric, positive definite kernel K on X Ã— X defines a unique RKHS on X with K as its reproducing kernel.

%RKHS have the reproducing property for which any $f \in H$ we have f(x) = \left\langle f, k(x, \cdot) \right\rangle_{\mathcal(H)}

%Detailed properties of Hilbert spaces and inner product spaces can be found in \cite{young1988introduction}.  Proofs for these statements are out of scope of this thesis but can be found in\cite{scholkopf2002learning}. 
\subsection{Kernel Mean Embedding}
While kernel functions are useful for comparing vectors in a new feature space, this is not quite what we need because we are looking for a way to compare two probability distributions. In the above setting, $\phi$ accepts points or vectors as inputs, but this can be generalized to random variables in the following way: Let $X$ be a random variable with domain $\Omega$ and distribution $P(X)$. We refer to instantiations of $X$ as a lower case $x$. In \cite{smola2007hilbert}, it is shown that $\phi$ can be used to represent a probability distribution as an element in a RKHS by
\begin{equation}
\mu_X =\underset{X \sim P} {\mathbb{E}}[\phi(X)],
\end{equation}
which is known as the \textit{kernel mean embedding} of $P$. This can be interpreted as a  distribution being mapped to its expected feature map in $\mathcal{H}$. 

Because we are in a non-parametric setting, we cannot assume what $P(X)$ is. However, we can estimate the embedding using a finite sample average  \cite{smola2007hilbert}. Suppose we have access to a sample $\{x_1, x_2, ..., x_m\}$ where $x_i$ is drawn i.i.d from $P(X)$, the empirical kernel mean embedding is 
\begin{equation}
\hat{\mu}_X = \frac{1}{m} \sum_{i=1}^m \phi(x_i).
\end{equation}
See Figure \ref{fig:kme} for a better understanding of the relationship between the distribution, the instances of the random variable and their embeddings into the new feature space.
\begin{center} 
\captionof{figure}[Kernel Mean Embedding]{Stylized depiction of a kernel mean embedding of $P(X)$ to a RKHS as $\mu_X$. The empirical estimate, $\hat{\mu}_X$, is shown to be close in $\mathcal{H}$ to its theoretical counterpart.} 
\ctikzfig{kme}
\label{fig:kme} 
%\medskip
%\tiny
\end{center}

Why bother mapping $P$ to a RKHS? Because we can now compare two probability distributions in $\mathcal{H}$ as if they were vectors in an infinite-dimensional space. This means we can use vector operations such as dot products or norms with kernel mean embeddings.

While this new feature space representation is helpful, it is not necessarily unique. This a problem because $P$ and $Q$ may be mapped into a space where they are measurably similar when in fact they are not. Consequently, we need to ensure the kernel mapping is \textit{injective}, i.e. that the mapping from $\mathcal{X}$ to  $\mathcal{H}$ is one to one. %See 2.1.1 of \cite{atkinson2005theoretical} for more details on injective functions.

To ensure the kernel mapping is injective, we can make use of a special class of kernel functions known as \textit{characteristic kernels} that are introduced in \cite{fukumizu2008kernel} and \cite{sriperumbudur2011universality}. These papers show that if $k$ is characteristic then the mapping of $\phi$ into the $\mathcal{H}$ is unique. This means if a characteristic kernel is used, then $X$ and $Y$ can be uniquely mapped to another feature space where the expected value in $\mathcal{H}$ is also unique. 

With this last piece of information we can now map two different distributions into a new feature space and compute a distance-like metric between the two, which is the basis of the maximum mean discrepancy.   %By combining the previous properties of kernel functions, we can now understand where the idea of the maximum mean discrepancy comes from. Rather than comparing the expected values of a moment in the original feature space like we did in 2.1, we will compare the expected values in this new feature space.

\subsection{Maximum Mean Discrepancy}

Suppose $m$ observations from a sample $\{x_1, x_2, ..., x_m\}$ and $n$ observations from a different sample $\{y_1, y_2, ..., y_n\}$ are derived from $\mathcal{X}$. Assume the observations from each sample are instantiations of random variables $X \sim  P$ and $Y \sim Q$ respectively. Using $\phi$ as it is described in \ref{kernel}, the maximum mean discrepancy (MMD) is defined in \cite{smola2007hilbert} as:
\begin{equation}
\label{mmd_theory}
\text{MMD}(P,Q)=||\mu_X - \mu_Y ||=|| \underset{X \sim P}{\mathbb{E}}[\phi(X)] -  \underset{Y \sim Q}{\mathbb{E}}[\phi(Y)]||_\mathcal{H},
\end{equation}

where $||\cdot||_\mathcal{H}$ is the \textit{norm} on $\mathcal{H}$. The MMD can be interpreted as the distance in $\mathcal{H}$ between the kernel mean embeddings of the features. As long as a kernel function can be defined on the given data structure, the MMD can be used. This is why it can be applied to non-numeric data such as strings, graphs, and other structured domains \cite{hofmann2008kernel}. From theorem 5 of \cite{gretton2012kernel}, the theoretical MMD$(P,Q) = 0$ if and only if $P=Q$. This fact arises from the injective property defined earlier. While this is a nice property, in practice we use empirical estimates of the kernel mean embeddings, $\hat{\mu}_X $ and $\hat{\mu}_Y$, meaning an estimate of MMD will never be exactly zero even if our samples are from the same distribution. Nonetheless, the power of MMD lies in its ease of estimation and the fact that it can detect any type of parameter difference between $P$ and $Q$. 
%This is because MMD is essentially a moment generaing function of P and Q.......

Using $X$ and $Y$, the unbiased estimate of the squared MMD is shown in \cite{gretton2012kernel} to be:
\begin{equation}
\label{mmd_unbiased}
\begin{split}
\widehat{\text{MMD}}_{u}^{2}(X, Y)=\frac{1}{m(m-1)} \sum_{i=1}^m \sum_{ j=1}^{m} k\left(x_{i}, x_{j}\right)-\frac{2}{m n} \sum_{i=1}^m \sum_{ j=1}^{n} k\left(x_{i}, y_{j}\right)+ \\
\frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j=1}^{n} k\left(y_{i}, y_{j}\right).
\end{split}
\end{equation}
One caveat of equation \ref{mmd_unbiased} is that while it is an unbiased estimate of the square of \ref{mmd_theory}, the calculation of $\sqrt{\widehat{\text{MMD}}_{u}^{2}}$ is not an unbiased estimate of \ref{mmd_theory}.

Similarly, the biased estimate of the squared MMD is
\begin{equation}
\label{mmd_biased}
\widehat{\text{MMD}}_{b}^{2}(X, Y)= \frac{1}{n^{2}} \sum_{i=1}^{n} \sum_{j=1}^{n} k\left(x_{i}, x_{j}\right)+\frac{1}{m^{2}} \sum_{i=1}^{m} \sum_{j=1}^{m} k\left(y_{i}, y_{j}\right)-\frac{2}{n m} \sum_{i=1}^{n} \sum_{j=1}^{m} k\left(x_{i}, y_{j}\right).
\end{equation}

Because both \ref{mmd_unbiased} and \ref{mmd_biased} are computed in $\mathcal{O}((m + n)^2)$ time, they will be referred to as quadratic-time estimates of MMD. For high-dimensional datasets (large $m$ or $n$), using a quadratic-time estimate of MMD is impractical.  

An alternative is proposed in section 6 of \cite{gretton2012kernel} that is linear in computation time and still uses all available data. Letting $X$ and $Y$ have equal size $m$ for notational simplicity, then
\begin{equation}
\label{mmd_linear}
\widehat{\text{MMD}}_{l}^{2}(X, Y) :=\frac{1}{m_{2}} \sum_{i=1}^{m_{2}} h\left(\left(x_{2 i-1}, y_{2 i-1}\right),\left(x_{2 i}, y_{2 i}\right)\right)
\end{equation}

is the linear-time estimate of MMD, where $m_2=\frac{m}{2}$ and
\begin{equation}
h\left((x_i, x_j), (y_i, y_j)\right):=k\left(x_{i}, x_{j}\right)+k\left(y_{i}, y_{j}\right)-k\left(x_{i}, y_{j}\right)-k\left(x_{j}, y_{i}\right).
\end{equation}
This estimation of the squared MMD is computed in $\mathcal{O}(m)$, which is significantly faster than the quadratic-time estimates. However, it pays for this speed with accuracy as the MMD$_l$ estimate has higher variance than MMD$_u$.

Now that we have a suitable metric for computing a test statistic between samples $X$ and $Y$, a hypothesis test can be constructed using the MMD statistic as shown in \cite{gretton2012kernel}. Similar to previous hypothesis tests, a threshold and kernel function is chosen to adequately balance between type-I and type-II error. The null hypothesis is that $X$ and $Y$ are equivalent in their kernel mean elements.  If the MMD is greater than the threshold, then the null hypothesis is rejected and the alternative hypothesis is accepted. In this case, the alternative hypothesis would indicate $X$ and $Y$ differ in one  or more of their statistical moments.

\subsection{Choice of Kernel}
Notice we have not yet explicitly chosen a kernel function for calculating the MMD statistic. Choosing an appropriate kernel is often done on a per case basis based on the type of application. Additionally, optimal kernel selection is a difficult problem that is still actively researched \cite{fukumizu2009kernel} \cite{gretton2012optimal}. We will only discuss one kernel function in this thesis, the Gaussian kernel. The Gaussian kernel is the most widely used kernel and it is the only kernel used in the kernel change point detection methods discussed in section \ref{related_works}.  For other possible choices of kernel functions see table 3.1 in \cite{muandet2017kernel}.

Given two $d$ dimensional real vectors $x, y \in \mathbb{R}^d$, the Gaussian kernel is defined as:
\begin{equation}
k(x, y)= e^{-\frac{1}{2\sigma^2}||x-y||^2}
\end{equation}
where $\sigma > 0$ and is called the \textit{kernel bandwidth}. In \cite{gretton2005kernel}, the authors recommend selecting the bandwidth based on the \textit{median heuristic} using a sample of the data. The median is computed on the set of pairwise distances between all data points in the sample. Therefore, $\sigma$ is computed by, 
\begin{equation}
\sigma=\text{median}\{||x_i-x_j||:i,j = 1,...,n \}.
\end{equation}

It is not clear where this heuristic was first introduced although many papers incorrectly cite the 2002 textbook, \textit{Learning with kernels: support vector machines, regularization, optimization, and beyond} by Sch{\"o}lkopf, Smola, Bach et al \cite{garreau2017large}. Despite its unknown origins, it has been shown to be empirically suitable for many kernel applications. It seems as long as the bandwidth is chosen $\mathcal{O}(\sqrt{d})$, which the median heuristic often achieves, then test power is independent of the bandwidth. However, it has been shown to be sub-optimal in cases with very high-dimensions \cite{muandet2014kernel} or small sample size\cite{ramdas2015decreasing}. Because the median heuristic is critical to overall performance of the kernel two-sample test using the MMD, its online implementation will be explored in greater detail in section \ref{our_approach}.
 
\section{Related Work}
\label{related_works}

\subsection{Offline Kernel Change Point Detection}
While offline change point detection techniques are not the focus of this thesis, we provide a short summary of some kernel approaches in this domain to highlight some commonalities with online kernel approaches. 

\hl{There are several common characteristic that offline kernel change point detection have with online kernel CPD. The first of data for which a kernel can properly reproduce a Hilbert space and thereby make use of the MMD statistic.} The second is that all offline kernel change point methods cited here make use of the Gaussian kernel, which speaks to the ubiquitousness of this kernel function. 

In \cite{harchaoui2007retrospective}, the authors approached the offline change point problem with a fixed number of change points using kernel change point detection. This was further extended to an unknown number of change points in \cite{arlot2012kernel}. The authors show their kernel change point detection procedure outputs an offline segmentation near optimal with high probability. The authors recommend choosing the kernel based on best possible signal to noise ratio. Therefore, they rely on prior knowledge of a reference or training set is necessary for calibrating the kernel. 

More recently in \cite{chang2019kernel}, a  kernel change point detection method is proposed that uses deep generative models to augment the test power of the kernel two sample test statistic. They point out MMD's lack of test power when using small samples from the out of control distribution, which may easily leading to over-fitting with kernels. Thus, they use a generative adversarial neural network (GAN), trained on historical samples of $X \sim  P$  with noise injected into $X$. This surrogate distribution is then used in conjunction with possible change points to improve the test power of a modified MMD measure that makes use of compositional kernels. The method is compared to other prominent change point methods for offline change detection. All comparisons are done on synthetic data with piece-wise i.i.d. data. All methods are benchmarked using the AUC metric for classification performance and it is shown the KL-CPD method is competitive or better than the state of the art methods.  Furthermore, the AUC performance is maintained as the dimensionality of the data is increased, making their kernel learning framework very interesting for future off-line change point detection. It remains to be seen if this framework can be adopted in an online context where time to detection is a key constraint on practicality.

See section 4.2.3  in \cite{truong2018review} for a review of kernel-based methods used for offline detecion. 

\subsection{Online Kernel Change Point Detection}
One of the first papers to use the term kernel change point detection was in \cite{desobry2005online}. The authors present an online kernel change point detection model based on single class support vector machines ($\nu$-SVMs). They train two single class support vectors, one on a past set and one on a future set. A ratio is then computed between the two sets that acts as the dissimilarity measure in a Hilbert space. If the sets are sufficiently dissimilar over some predetermined threshold, then a change point is assigned to the time step that splits the two sets of data. The authors argue that a dissimilarity measure between kernel projection of points in a Hilbert space should estimate the \textit{density supports} rather than estimate the probability distributions of each set of points. While this approach inspired a lot of interesting research that will be discussed below, it does not use the maximum mean discrepancy and has not been studied since.

The following methods are all inspired by the classic algorithms from section \ref{classic_algo}. They all make use of a variation of the maximum mean discrepancy. They are also the most recent online kernel techniques developed and will be used for experiments in later sections. Thus, they will be described in more detail than previous methods.

In  \cite{li2015m}, the authors use the B-test introduced in \cite{zaremba2013b} and develop an offline and online change point detection algorithm called the MStats algorithm (the authors also refer to this algorithm as the Scan-B algorithm in a follow-up paper). At each time-step, the online model samples new data from a window of size $B_0$ and computes a B-test statistic with $N$ past samples that are kept as reference samples. 
\begin{equation}
Z_{B_{0}, t}:=\frac{1}{N} \sum_{i=1}^{N} \operatorname{MMD}_{u}^{2}\left(X_{i}^{\left(B_{0}, t\right)}, Y^{\left(B_{0}, t\right)}\right)
\end{equation}

The resulting test statistic is then normalized by $Z_{B_{0}, t}/\sqrt{Var[Z_{B_0}]}$ where the authors provide a theoretical calculation of $\text{Var}[Z_B]$. If the normalized test statistic exceeds some predefined threshold then a change point is declared. The B-test is memoryless in the sense that the statistic is calculated each time and only the value of the last calculation has any weight. This is similar to a Shewart control chart that calculates a z-score at each iteration. Adjusting the size of the window, $B_0$, results in the usual trade-off of performance in online change point detection. A smaller block size will have a smaller computational cost and a smaller detection delay but will result in higher type II error. A larger block size will have better type II error but will take longer to compute. Unfortunately, no matter what block size is chosen, the computation time for the Scan-B algorithm is the longest relative to the other kernel change point methods discussed below. 

From here, theoretical bounds are developed for the time to false alarm rate and expected detection delay. They run several experiments on synthetic data including change in mean, change in variance, change from Gaussian to Gaussian mixture, and change from Gaussian to Laplace distribution. Experiments are also done on real-data sets including a speech dataset and the Human Activity Sensing Consortium (HASC) dataset where the performance was better than the relative density-ratio (RDR) algorithm described in \cite{liu2013change}.

%https://media.nips.cc/nipsbooks/nipspapers/paper_files/nips28/reviews/1852.html

A modified, "no-prior-knowledge" exponentially-weighted moving average called NEWMA is introduced in \cite{keriven2018newma}. Based on the standard exponentially weighted moving average shown in \ref{ewma}, NEWMA computes two EWMA statistics of different weights. If the difference between the two EWMA statistics exceeds a predefined threshold then a changepoint is declared at that time step. The reason for using two EWMA statistics is to set one to have a larger forgetting factor. Any recent changes in a distribution will weigh heavily on one statistic, resulting in a sudden, large difference between the two statistics. 

Since a standard EWMA is a parametric method, the authors apply a kernel mapping function, $\Psi$, to the data prior to applying the exponential weights. This provides a memoryless, non-parametric, online change point detection method that does not need to constantly store all previously streamed data. Once the statistics are updated at each iteration, the raw data may be discarded. This characteristic makes it especially useful in applications where security is a concern.

While kernel mean embeddings could be used for approximating, $\Psi$, as is the case for standard implementations of MMD, this would require the storage of past examples of data. Because the authors aim to reduce run-time cost and storage cost, they use a \textit{random fourier features} (RFF) approach for estimating $\Psi$ (Note RFF is sometimes referred to as \textit{random kitchen sinks}). Because $\Psi$  is estimated using RFF, this is the only method discussed that does not use the kernel function for an implicit representation of the data in another feature. Instead, the RFF explicitly maps the data to a lower dimension Euclidean product space using a randomized feature map $\mathbf{z}: \mathbb{R}^d \rightarrow \mathbb{R}^m$ such that:
\begin{equation}
k(\mathbf{x}, \mathbf{y})=\langle\phi(\mathbf{x}), \phi(\mathbf{y})\rangle_{\mathcal{H}} \approx \mathbf{z}(\mathbf{x})^{\top} \mathbf{z}(\mathbf{y})
\end{equation}

where $\mathbf{z}(\mathbf{x}):=\mathbf{W}^{\top} \mathbf{x}$. Each element, $w_{ij}$, is sampled from a distribution that is the Fourier transform of a translation invariant kernel \cite{rahimi2008random}.

There are several approaches available for calculating RFF already studied in the literature and the authors use three common ones for comparison. They use the standard RFF implementation from \cite{rahimi2008random}, the FastFood implementation introduced in \cite{le2014fastfood}, and Optical Processing Unit implementation from \cite{saade2016random}. All three are used to create three variants of their NEWMA algorithm.

The NEWMA variants are compared to the MStats (Scan-B) algorithm by running empirical experiments on synthetic and real datasets. The synthetic datasets are run using streaming data that is generated from different Gaussian mixture models. They use an audio dataset for testing on real data. The variants of NEWMA are similar, if not better than MStats (Scan-B) in terms of missed detection percentage. In terms of average detection delay and false alarm trade-off, the NEWMA algorithm and its variants appear to be mildly better as well. The largest advantage of the NEWMA variants over the MStats (Scan-B) method is in the execution time. MStats (Scan-B)'s execution scales linearly with window size, while NEWMA's execution time does not depend on window size.

Finally, in a recent, preprint paper \cite{flynn2019change}, a kernel CUSUM (KCUSUM) algorithm is proposed, where the classic CUSUM algorithm from \ref{cusum} is adapted using the faster MMD$_l$ statistic for online detection. The algorithm functions as follows, every two observations, the MMD$_l$ is calculated using newly observed data points and data points sampled from some reference distribution that is known at the outset. The calculated MMD$_l$ acts as the update term to the cumulative sum statistic. If this kernel cumulative sum statistic exceeds some predefined threshold, then a change point is flagged. Interestingly, unlike the previous methods that set the Gaussian kernel's bandwidth using the median heuristic, the authors chose to fix the bandwidth to one. Furthermore, because it relies on a reference distribution that is outside the data stream for comparison, making KCUSUM more of a semi-supervised algorithm rather than a completely unsupervised one. %This reference distribution can be thought of as the "in-control" distribution of the data-stream that new observations are compared to. 

Besides its speed of computation, an additional benefit of MMD$_l$ is that it is normally distributed under the null distribution unlike the quadratic-time estimate. This facilitates analysis and provides statistical guarantees for worst-case detection delays and the expected time to false alarm. While this non-parametric approach can detect any change in the distribution of a sequence, it does struggle with more complicated distributional changes such as variance changes of a single dimension and changes beyond first and second-order moments. It was also not benchmarked against other kernel methods, which we cover in this thesis for completeness.

Given how useful for selecting the bandwidth hyperparameter, it is noteworthy then to see how the above online change point models tune median heuristic. In the context of an infinite data stream, most methods simply take a small initial sample and use that to set the bandwidth once at the beginning and never change it again. you can simply run and expanding median on all incoming data that infinitely grows. But is this ideal? This would require storing all the observations you ever observe which is obviously intractable. Alternatively, you can set it using some initial training data but again how long is an initial bandwidth valid in a continuous data stream? To our knowledge, these questions have not been addressed anywhere in the kernel change point detection literature, which is why the topic successive median calculation is a focus in section \ref{our_approach}.

%In 2016, James et al. \cite{james2016leveraging} proposed using energy statistics to test the significance of a change point that is robust to anomalies. They point out that their work is the first to accurately detect change points with fast time to detection while not being affected by extreme outliers that are not change points. They compare their E-divisive with medians algorithm to the parametric PELT technique. 

\section{Our Approach}
\label{our_approach}

As stated in the related work, all kernel change point detection methods that use a Gaussian kernel use the median heuristic to determine the bandwidth. While this method has proved to be sufficient for most applications, it has not been thoroughly tested against any alternative bandwidth selection procedures. This is ironic given how much impact the choice of bandwidth can have on performance. We present a natural alternative for calculating the median heuristic in real-time for online change point detection. This method is largely based on the \texttt{Binapprox} algorithm from \cite{tibshirani2008fast} that is shown below. 

%https://tex.stackexchange.com/questions/142313/how-to-see-the-steps-number-in-an-algorithm
\begin{algorithm}[H]
\SetAlgoLined
\KwIn{$x_i,..,x_n, B$}
\KwOut{Approx. median}
Calculate $\bar{x}$ and $s_x$\\
Form $B$ bins across $[\bar{x} - s_x, \bar{x} + s_x]$\\
Map $x_i,..,x_n$ to bins\\
Find the bin $b$ that contains the median\\
Return the midpoint of bin b\\
 \caption{Binapprox algorithm}
\end{algorithm}

Several characteristics make \texttt{Binapprox} particularly appealing for online, non-parametric data streaming.  Firstly,
the runtime of \texttt{Binapprox} does not depend on a data's distribution. Secondly, the algorithm requires $\mathcal{O}(1)$ storage space, and doesn't perturb the input data, so rearranging is never a problem. Thirdly, in the worst-case scenario the algorithm has $\mathcal{O}(n)$ computational complexity. In practice, it consistently runs faster than the classic quickselect algorithm and \texttt{Binmedian}, a more exhaustive  algorithm that is presented in the same paper. Lastly, it can handle newly acquired data very quickly to provide an updated approximation of the median. 

The main drawback of the algorithm is hinted in its name. It is an approximation of the median and will be at most $\sigma / B$ away from the true median. Therefore, if the standard deviation of the data is extremely large,
the approximation could be significantly different from the actual median. To combat this, the author recommends setting $B=1000$, which yields sufficient approximations in most empirical cases.

Naturally, this fits in exactly to what we want to accomplish with online change point detection. We have incoming data that we want to roll into the calculation of the median heuristic but we also want a fast way to update the median while being accurate. The \texttt{Binapprox} algorithm checks all of these boxes and is simple to incorporate into any online change point detection work flow. 

\section{Experiment Setup}
\label{experiments}
\subsection{Initialization}
An important factor for online change point methods is determining how to initialize the algorithm. Because we are running an unsupervised model for classifying change points, we do not spend time training the model or tuning any of the hyperparameters. This is a double-edged sword because on one hand, we can drop in a change point algorithm onto a data stream let it start running without much configuration. On the other hand, if it does not have any prior distribution to compare to then it will need to be adjusted to know what's an appropriate reference or in-control distribution. This is usually referred to as the initialization phase for online algorithms.

There are many ways to get through the initialization step and create an appropriate reference sample. One way is to initially compare the oncoming data stream with some zero valued data. As data comes in, the algorithm can replace the reference data with real observations, creating a reference distribution. While this does assume that the data is initially in-control, it does provide a simple way to create a reference distribution on the fly without prior knowledge. When analysing the results, the practitioner must exclude the initial phase when comparisons were done with the initial zero values because the test statistic calculated from them will be insignificant. 

Another method is to construct the reference distribution with past data that do not contain change points if it is available as done in \cite{li2015m} and \cite{flynn2019change}. This method benefits from not having to spend time initializing the algorithm since it can immediately provide an informed statistical comparison. If the data stream is not very long or costly to acquire then this method is beneficial because the entire signal is retained.  On the other hand, prior work needs to be done to construct this sample and this may not always be feasible in practice. Another possible issue is what if the regime of the in-control distribution shifts to a new normal? Then the old reference distribution is no longer applicable and must be recreated again, which may be costly. 

\subsection{Structuring the Data Stream}
In practice, there is no universal way for evaluating online change point models given the nature of the problem. There are two main reasons for this. First, because change point detection originated in the statistical literature, most algorithms or models have strong theoretical results but have not being applied to synthetic or real datasets. Being able to handle high dimensional data streams is a relatively new benefit that has not been so research up until the mid-90's was largely theoretical in nature. Second, it's not obvious how an infinite data stream should be simulated to evaluate the efficacy of an algorithm. How long does synthetic data stream have to be to objectively evaluate an algorithm?  How can you actually know where change points occurred in a real dataset that has no obvious ground truth? It is not obvious how to answer these questions, nor does there exist a universal answer. Like most things, the answer depends on the context and what a practitioner is aiming to test with their particular algorithm.  

For this thesis, we wish to compare several methods not just one novel method, therefore data consistency and reproducibility will be important for drawing conclusions about performance. Synthetic datasets are a good way to replicate different distributions over and over. Real change points can also be exactly identifiable and compared to with synthetic data. Take for example a data stream where a single random variable is sampled from a normal distribution $\mathcal{N}(0,1)$ that incurs a change point and becomes a random variable from a normal $\mathcal{N}(0,2)$. I can control both of these distributions and know exactly where the change point occurred, but how should I determine how many observations should be observed before switching distributions? How many observations after the change point should I draw from the new distribution before realizing the algorithm will never detect this change point? Again, decisions like this are arbitrary and nuanced for evaluating online change point models because there is no universal method. 

We choose to

\subsection{Evaluation}
\label{eval}
In cases where a change point is detected, the average detection delay (ADD) is used to estimate the average time it takes for a change point to be detected. It is computed by comparing the time difference between a predicted change point and an actual change point. This difference is then normalized the total by the total number of change points:
\begin{equation}
\text{ADD} = \frac{\sum_i^{\#CP} |\hat{t_i} - t^*|}{\#CP}
\end{equation}
The closer the predicted change points are to the actual change points, the smaller the average detection delay. Because it relies on ground truth change points, it can only be used when experimenting on synthetic data where all the information is available.%Therefore, the ADD can range from $(0, \inf)$. 

If labelled change points are available for a real world dataset or a synthetic dataset, then the ground truth change point vector, 
$t^*$, is known. Evaluating performance in this case is the same as evaluating a binary classifier from a supervised learning problem. For example, the metrics from discussed in hypothesis testing, type-I error (false positives) and type-II error (false negatives), are commonly used for evaluating change point detection performance. The false positive rate (FPR) can be calculated by $FPR = FP / N$. Plotting the false positive rate and the true positive rate, gives the receiver operator characteristic curve. The area under this curve (AUC), is calculated and compared to random baseline performance of 0.5 that would be equivalent to classification by tossing a coin.


\subsection{Synthetic Datasets}
A common difficulty in change point detection is evaluating the performance of an algorithm with datasets that aren't overly simplistic and difficult enough to ascertain some real world use.

Unlike fields like image recognition where standardized datasets like MNIST provide a common benchmark, there are no standard datasets that are widely used across the change point detection literature for evaluating new methods. Most papers propose experiments that are relevant for the specific problem they are trying to solve  but lack examples or explanations of when their method would not be applicable.  Furthermore, because change point evolved out of the statistics literature, many papers focus on theoretical results and provide minor experimental results, if any.

Given the empirical focus of this thesis, we attempt to put together the most comprehensive experiments using synthetic data. To the best of our knowledge, no change point detection paper covers as many variations as presented in this thesis. While synthetic datasets are idealistic in their formulation, they provide a good starting point for comparing different methods because the exact location of the change points can be controlled for. This is a luxury that is often not available with real world datasets, making it difficult to ascertain performance on them. Therefore, to compare several kernel change point detection methods, they will be evaluated across several synthetic datasets.

Inspired by recent papers \cite{chang2019kernel} and \cite{flynn2019change} that attempt to bridge the gap between the statistics and machine-learning literature, we put together various challenging changes in distribution that may be encountered in a data stream. They are the following: change in mean, scaling variance, alternating between a Gaussian distribution and a Gaussian mixture, alternating between Gaussian mixtures, and alternating between Gaussian distribution and Laplace distribution. It is truly hard to properly generalize all the possible situations a non-parametric algorithm may be used in, but the synthetic cases presented in this thesis cover a range of applications. The following paragraphs describe how each one is constructed in detail.

For a change in mean, a change point is inserted in the time series at some random time where the mean is shifted either positively or negatively. There are two variants to this scenario. In the first, the mean change is in all dimensions simultaneously. In the second variation, the mean change is in only one dimension making it harder to detect. 

For a change in variance, the distribution alternates between a Gaussian with $\mathcal{N}(0,1)$ to a Gaussian where the variance is scaled by a factor of 2 giving $\mathcal{N}(0,2)$.

For a change between Gaussian mixtures, the data stream starts with a Gaussian mixture model XXXX and 

In the last scenario, the time series alternates from a Gaussian with $\mathcal{N}(0,1)$ to a Laplace distribution with zero mean and unit variance. The idea for this scenario is detecting changes when a data stream is consistent in its mean and variance, but not in its later moments.

For each experiment above, a synthetic time series is created with 500 change points that will be fed into each of the algorithms.  Once the detection statistics are calculated, the evaluation metrics from \ref{eval} are computed at various thresholds. A Monte-Carlo approach is used to estimate false alarm rates and average detection delay at various thresholds. See table \ref{Tab:synthetic} for a summary of each synthetic dataset.

It should be noted this testing setup is nearly identical to the testing procedure used in the NEWMA except we use longer time series with more change points and more variations of possible distribution changes. The other two kernel algorithms, Scan-B and KCUSUM, did not use long time series with several change points but rather a repeated test approach with a single change point. 

\begin{center}
\captionof{table}{Synthetic Datasets Summary}
\begin{tabular}{SSSSSS} \toprule
    {Type of Change} & {No. of Dimensions} & {Length} & {No. of Changepoints}  \\ \midrule
    {Mean (all dimensions)}  & 20 & 5000 & 500  \\
    {Mean (single dimension)}  & 20 & 5000 & 500  \\
    {Variance}  & 20 & 5000 & 500  \\
    {Frequency}  & 1 & 5000  & 500  \\
    {Blobs}  & 10 & 10 000  & 500  \\
    {GMMs}  & 50 & 10 000  & 500  \\ 
    {Normal to Laplace} & 2 & 5000  & 500  \\ \bottomrule
\end{tabular}
 \label{Tab:synthetic}
\end{center}

\subsection{Synthetic Results}

