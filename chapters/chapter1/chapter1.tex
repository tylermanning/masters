\chapter{Introduction}
\section{Motivation}
The use of statistical control charts for detecting real-time changes in variation was pioneered by Walter Shewart in the first half of the twentieth century.  Shewart was interested in reducing the unexpected causes of variation in the manufacturing processes that produced faulty manufacturing equipment \cite{shewhart1931economic}. Shewart's method involved charting the process measurements over time and detecting when a statistical process was no longer exhibiting an expected level of variation. Once this detection occurred, the process was stopped and was not restarted until the cause of the variation was fixed.
Shewart's control charts were one of the first formal frameworks to solve the problem of detecting changes in a distribution of a sequence of random variables. This problem is now known more generally as the \textit{change point detection problem}. Many industries make use of change-point techniques including healthcare monitoring systems, monitoring computer network traffic, and detecting regime shifts financial markets. The following are a few motivating examples.

\subsection{Health Care}
Health care is an important area for quickly detecting signal changes. Some recent studies include applications to heart rate monitoring \cite{yang2006adaptive} \cite{staudacher2005new}, epilepsy signal segmentation \cite{malladi2013online}, and multi-modal MRI lesion detection \cite{bosc2003automatic} to name a few. Quickly detecting changes to a patient's health is absolutely necessary for any system to be of practical use. However, this quick detection must be balanced with high accuracy as false positives or missed detections could have life-threatening consequences. Therefore, balancing type I and type II error is a central theme to online change-point detection.

%\subsection{Computer Network Surveillance}
%Networks/monitoring systems for intrusions and/or changes in climate or natural disasters.

\subsection{Financial Applications}
The application of accurate and timely change point detection is also very popular in the finance sector where shifts in asset prices can suddenly happen. Change point detection is particularly hard in financial applications because of the non-stationary data typically observed in asset price time series. Note, in the financial literature, change-points  are also referred to as structural breaks, but for this thesis we will use the broader term change-points.

An on-line, quick detection technique is proposed in  \cite{pepelyshev2015real}, where a modified Shiryaev - Roberts procedure is used in a case study to detect a change-point on a single stock's daily returns. They compare their non-parametric method with other classic control chart methods using speed of detection and false alarm rate as measures of performance.

Detecting changes in variance is specifically explored in  \cite{lavielle2007adaptive}. The authors propose an off-line change-point algorithm that minimizes a global cost function by using an adaptive regularization function. The algorithm is applied to the absolute returns of the FTSE 100 stock index and the US dollar-Japanese Yen foreign intra-day exchange rate to detect changes in asset price volatility. The change-points  identified in the FTSE 100 coincided with key market events such as the stock market crash that occurred on October 14$^{th}$, 1987 and breaking the 5000 price barrier in August 1997.

See section 1.3.6 of \cite{tartakovsky2014sequential} for more applications to options markets and arbitrage opportunities.

\section{Characteristics of the change point problem}
A number of surveys of the literature already exist \cite{aminikhanghahi2017survey} \cite{niu2016multiple}, therefore we will not cover all existing methods but rather touch upon several, important factors to consider when tackling the change point detection problem. Across the body of literature, these factors determine what methods are available to practitioners. %Furthermore, Bayesian methods that relate to change point detection but will not be covered in this thesis.

The first factor is selecting between \textit{parametric} and \textit{non-parametric} techniques. Deciding between these two broad techniques is dependent on the prior knowledge one wants to encode into the problem. For example, if it is known that data is generated by a distribution from the exponential family of distributions, then we can subset the problem from the space of all possible distributions to a smaller space of distributions. Shewart control charts and CUSUM change-point techniques are both parametric techniques based on the Gaussian-family of distributions \cite{page1954continuous} \cite{chen2011parametric}. In other settings, it is not possible to leverage information about the data and non-parametric techniques must be used instead \cite{brodsky2013nonparametric}.

% . On the other hand, if the data is being generated by processes that have no fundamental or well-understood process then modelling particular probability distributions becomes intractable or risky. Therefore, models that do not model particular probability distributions must be used. For reference, see monograph by Brodsky and Darkhovsky \cite{brodsky2013nonparametric}. % Usually this added flexibility is a trade-off with performance and/or speed with parametric models.

The second factor is deciding whether change-points should be detected \textit{offline} or \textit{online}. Some algorithms are off-line---also referred to  as batch algorithms or retrospective or a posteriori change-point detection---and they are applied in an ex-post fashion after the dataset has been completely acquired \cite{truong2018review}. If change-points must be detected as soon as possible, then waiting for the entire dataset to be acquired is not feasible and methods that operate on data streams as they arrive must be used. Such methods fall into the category of on-line change-point detection. The aforementioned Shewart control chart and CUSUM algorithm are both designed for data that is streamed in a real-time fashion. In the literature, on-line methods of change point detection are also referred to sequential change point detection  \cite{tartakovsky2014sequential}. For this thesis, we will use the terms interchangeably.% For an exhaustive review of sequential change point analysis, see the book by Tartakovsky, Nikiforov, and Basseville  \cite{tartakovsky2014sequential}. %It is possible to convert an on-line algorithm to an off-line algorithm by simply 

%The third consideration that is a consequence of detecting change points in an on-line setting is the presence of outliers. This a particularly important concern for on-line methods that are looking for changes in the underlying distribution of points, not a single anomalous data point

The third factor is determining if there are multiple change points or to assume there is only a single change point to detect. This is an important  factor for off-line change point detection where the decision to detect one or more change-points is chosen at the outset.  Detecting multiple change-points could also be relevant for the on-line case if a situation arises where the window of time series under consideration may contain more than one change point. However, most on-line change-point methods are designed to detect a single change-point at a time.

Finally, the last factor to address is determining exactly what kinds of statistical changes an algorithm should detect. Many methods focus solely on detecting changes in the mean of a distribution \cite{lee2010change}. Some methods are more general and can detect changes in the variance or higher order moments and do not focus on any particular one. Methods like kernel change point detection can typically detect any distributional changes. 

This thesis will concern itself with on-line change point detection, where data is received in a streaming nature. We assume no prior distributional characteristics on the data and operate in a completely non-parametric setting. 

\section{Problem Formulation}
This sections describes how the change-point problem will be formulated in this thesis and, by extension, how all methods will be described using the change-point detection problem notation. Because online change-point detection is closely related to two sample testing, a background statistical hypothesis testing is presented first. 

\subsection{Hypothesis Testing}
The goal is to determine if the null hypothesis is rejected or if it is failed to reject the null hypothesis. A two-sample hypothesis test draws samples from a set $\mathbb{P}$ and second sample $\mathbb{Q}$ 

\subsection{Change-point Detection Problem}
The basic change-point problem is set up as hypothesis test between two segments of a time series. Let $X_i,...,X_n$ be a series of independent random variables of dimensions $d \geq 1$ be sequentially observed . Then, one of the following hypotheses holds:
\begin{equation}
  \begin{cases}
    H_0: X_1, X_2, ...,X_n \sim  F_0 & \text{(no change-point occured)} \\
    H_1: X_1,X_2, X_{t_0-1} \sim F_0, X_{t_0}, X_{t_0+1},  \sim F_1 & \text{(a change-point occured)}. 
  \end{cases}
\end{equation}
Where $i=1,2,...,t_0-1$ and $j=t_0,...,n$  are two distinct segments separated by change-point $t_0$ that is within the time series window. Furthermore, $F_0, F_1$ are cumulative distribution functions (CDFs) with corresponding probability density functions (PDFs), $f_0, f_1$. Because we are operating in a non-parametric setting, the CDFs are assumed to be completely unknown. 

If there is no change in the data then we say the change time is equal to infinity and denote this probability as $P^{\infty}$ and the expectation is $E^{\infty}$.

Many change-point detection algorithms define a statistic that is computed using each set before and after the possible change-point,$t$. If the statistic is above a particular threshold then time $t$ is classified as a change-point, $\hat{\tau}$.

In the on-line scenario, the time series under consideration can be thought of as a sliding window with data constantly coming in and out of the window of interest. The size of the window is an important consideration that is typically chosen based on the problem being solved. Too small a window and the sets of data may not yield a statistically significant result. Too large of a window and the problem leans more towards and off-line model, where high volumes must be stored and several change-points may appear in a given window. If the amount of data is not a limitation then throttling the data may not be necessary.

\subsection{Performance Measures}
Because of the unsupervised nature of detecting change-points, it is difficult to evaluate the performance of change-point detection models with real world data. Many papers detail asymptotic or non-asymptotic theoretical guarantees of their proposed change-point methods.  These theoretical results are typically compared across different change-point methods for benchmarking a new algorithm.

Two main issues arise when detecting change-points in a stream of data. The first is detecting a change-point when there is no actual statistical change in the observed sequence. These are typically called false positives or \textit{false alarms} the in change-point detection literature. The false alarm rate is defined by a metric known as the \textit{time to false alarm} (TTFA) rate. 
\begin{equation}
TTFA = E_{\infty}[T] = E_{\theta}[N]
\end{equation}
Where it is the expected number of observations that must be recorded before a change-point is incorrectly detected. In other words, it is the average amount of time until a change is detected given a sequence of observations with no change. Therefore, a larger value of TTFA is preferable. From a hypothesis testing perspective, this is equivalent to rejecting $H_0$ in \hl{cite equation in problem statement} when it should not be rejected, i.e. type I error.

The second issue is not detecting a change-point when one occurs. This could be caused by detecting a change-point much too late for it to be of any use or simply missing it altogether. For quantifying this error, the worst case detection delay (WCD) metric measures how slow a model will detect a  change-point in a worst case scenario. Conversely to TTFA, lower values of WCD are preferable.
\begin{equation}
WCD = sup esssup E_t[(T-t)^+ | F_{t-1}]
\end{equation}

From a hypothesis testing perspective, this is equivalent to not rejecting $H_0$ in \hl{cite equation in problem statement} when it should be rejected, i.e. type II error.

Balancing the TTFA and WCD of an on-line detection algorithm is crucial to for an algorithm to be of any practical use. In \hl{1971 Lorden procedures to reacting...}, it was shown asymptotically that the CUSUM algorithm provides an optimal trade-off between TTFA and WCD and ,in \hl{moustakides optimal stopping times for detecting.. 1986}, it was proved optimal in the non-asymptotic case as well. Note, TTFA and WCD are also commonly referred to as ARL$_0$ and ARL$_1$ respectively where ARL stands for average run length. For clarity, we use the more explicit terms TTFA and WCD.

When detecting changes of a distribution, a user may want to quantify the size of the change in the mean by
$|\mathbb{E}[X_{\tau}]-\mathbb{E}[X_{\tau+1}]|$ or, similarly, the size of the change in the variance by $|\text{Var}[X_{\tau}]-\text{Var}[X_{\tau+1}]|$.

\subsection{Other performance measures}

If labelled change-points are available for a real world dataset or a synthetic dataset, then the ground truth change-point vector, $\tau^*$, is known. For example the \textit{Hausdorff} metric can be used. It measures the furthest temporal distance between a predicted change-point $\hat{\tau}$ and $\tau^*$. It is defined as:

Other standard classifier metrics can also used for comparing $\hat{\tau}$ and $\tau^*$. This includes the  F1-Score that is based on a classifier's precision and recall:
\begin{equation}
F_1(\hat{\tau}, \tau^*) = 2 * \frac{\text{precision*recall}}{\text{precision + recall}}
\end{equation}

 F1-Score is defined as the harmonic mean of precision and recall. Precision is defined as the ratio of true positives (TP) to the
number of true positives (TP) and false positives (FP) and recall is defined as the ratio the number of true positives to the
number of true positives plus the number of false negatives. F1-Score is best when F1 = 1 (perfect precision and recall) and reaches its worst value at F1 = 0. Depending on the context, any other classifier evaluation tools such as the Receiver Operating Characteristics Curve and the Precision Recall Curve may be used as well.

\section{Classic Algorithms}
Presented below are the fundamental approaches to on-line change-point detection that have been very influential. 

\subsection{Shewart Control Chart}
Shewart control charts were originally designed to detect changes in the mean of a process where the values being observed are assumed to be Gaussian distributed. As the data arrives, the data is batched into samples of size $N$.  The sample mean, $\bar{X}=\frac{1}{N} \sum_{i=1}^N X_i$, is the then calculated and compared it to a known, true mean $\mu^*$. If the absolute difference is greater than a threshold, then a change-point is declared at the current batch. Therefore, the decision rule is defined as,
\begin{equation}
|\bar{X}-\mu^*| > \kappa \frac{\sigma}{\sqrt{N}}
\end{equation}

Where $\kappa$ is a constant that controls how sensitive the algorithm is. Typically, it is set to $\kappa=3$ as this \hl{need explanation}.
The true mean is assumed to known and is defined as $\mu^* = \mathbb{E}[X_i]$. In applications, the true mean can also be replaced by some target specification that a process must adhere to. Similarly, it is assumed the standard deviation, $\sigma$, is known in advance but it can also be estimated.

Tuning the hyper-parameters can drastically change the performance of the algorithm. Choosing a lower value for $\kappa$ makes the control chart detect change-points more often, whereas a higher value results in less detections. The chosen sample size, $N$, is also critical and its effect on the performance of Shewart control charts was studied in \cite{haridy2017effect}.

\subsection{CUSUM}
Similar to the Shewart control chart, the CUSUM algorithm tracks a statistic over time relative to a predetermined threshold. CUSUM is best applied to a process that is already under control. It can be thought of accumulating the information of current and past samples. 

The algorithm can be recursively defined by updating a statistic, $S_i$, after each $X_i$, such that:
\begin{equation}
  \begin{cases}
    S_0 = 0  & \text{(Initialization)} \\
    S_i = \text{max}(0, S_{i-1} + Z_i) & \text{for i=1,2,...}
  \end{cases}
\end{equation}
Where $Z_i=ln(\frac{f_{\theta_1}(X_i)}{f_{\theta_0}(X_i)})$ and the statistic $S_i$ is compared to a threshold $h$ that is predetermined by the user. If $Z_i \geq h$ then a change-point is declared at time $i$ and the algorithm is either completed or restarted. Given that the statistic only flags change-points when greater than a threshold, this algorithm only detects positive changes in the distribution. In \cite{page1954continuous}, it is suggested to use two CUSUM algorithms to detect positive and negative changes in a distributional parameter.

Furthermore, it is assumed the distributions, $f_0$ and $f_1$, are known at the outset. In most applications, this is quite limiting and unrealistic. Therefore, in cases where parameters $\theta_0$ and $\theta_1$, maximum likelihood estimates of the parameters are usually computed.

\subsection{Extensions to CUSUM}
The filtered-derivative extension of the CUSUM introduced in \hl{XXXX} uses the change of the discrete derivative of a signal over time to detect a change-point. 

\hl{In X}, a fast initial response (FIR) CUSUM algorithm is proposed where the starting value of initial cumulative sums adapts over time. Instead of resetting $S_0$ to zero as shown above, it is \hl{add a bit more detail} . This gives the algorithm a head-start in quickly detecting when a process is out of control and is especially useful for processes that don't start in control.

Finally, since CUSUM is typically better at detecting small shifts in signals and the Shewart control chart is faster at detecting larger changes, the two can be combined. The combined Shewart-CUSUM algorithm leverages the strengths of both techniques for better overall performance. See \cite{lucas1982combined}, \cite{yashchin1985analysis}, and \cite{westgard1977combined} for more details. 

\subsection{EWMA}
The exponentially weighted moving average as described in \cite{roberts1959control} (originally called a geometric average), is a type of moving average that applies exponentially weighting. First introduced as a forecasting technique in the econometrics field for smoothing noisy functions, the EWMA can also be used for determining out of control processes as shown in \hl{XXX}. Rather than weight all observations uniformly like the standard CUSUM algorithm or a simple moving average, a decay factor (also called a  forgetting factor), $\lambda$, is used to control how much weight is distributed over the previous observations. As each new observation arrives, the EWMA statistic is updated and compared to a threshold. If the EWMA statistic exceeds the threshold then the process is deemed out of control or 

The EWMA statistic is calculated as follows at each time step, $t$:
$$S_t = \lambda X_t + (1-\lambda)S_{t-1} $$

Where the decay factor,  $\lambda$, is set to a value between $0$ and $1$. As $\lambda$ approaches $1$, the EWMA control chart gives more and more weight to the most recent observations similar to a Shewhart control chart that gives weight to the last observation only. Conversely, as $\lambda$ approaches $0$, the weights are distributed further into the past giving the EWMA a longer memory similar to the CUSUM algorithm. Therefore, a EWMA control chart can be interpreted as a trade-off between a Shewhart control chart and a CUSUM control chart. 

For detecting deviations away from a mean target value, control limits may be calculated in a similar manner to the Shewart control chart. In \hl{XXXX}, control limits for the EWMA are chosen to be  $\pm 3 \cdot \sigma_{\text {Shewhart }} \sqrt{\frac{\lambda}{2-\lambda}}$. The standard deviation, $\sigma_{\text {Shewhart }}$, is calculated similarly to the Shewart control chart problem and its derivation is shown in the appendix of \hl{XX}.

More generally, if a function, $g(x)$ is applied to the observations such that we monitor $\tau= \mathbb{E}[g(X_t)]$ over time. Then the EWMA statistic will be compared to the in-control $\tau^*$ at each iteration and is assumed to be known beforehand. If 

As with the other methods previously mentioned, the standard EWMA is a parametric method as it assumes the time series have some in-control average that is known prior to use. This makes is difficult to apply in situations where the data is non-numeric or where third or fourth moment changes in the distribution occur. It is however very fast due to its recursive structure and does not hold a lot of data in memory making it appealing for live data streams that rely on quick processing.

\section{Maximum Mean Discrepancy}
Based on the kernel mean embedding , the maximum mean discrepancy (MMD) is a type of \textit{integral probability metric} that can be used to compare two distributions \hl{[cite what IPM is from]. It is akin to the Kolmogorov distance between two distributions (which is the max norm of the difference between 2 cumulative distributions) or the Wassertstain distance.}
% MMD can be thought of a distance metric

The two-sample kernel test statistic is defined in \cite{gretton2012kernel} and uses the MMD as a distance measure for comparing two probability distributions. For example, suppose $n$ samples $X = \{x_1, x_2, ..., x_n\}$ and and $m$ samples from a different set $Y=\{y_1, y_2, ..., y_m\}$ are observed from a sample space $\mathcal{X}$. They are distributed as $X \sim  \mathbb{P}$ and $Y \sim \mathbb{Q}$ respectively. The goal is to determine if in fact the two distributions are the same. The MMD is defined by a feature map $\phi: \mathcal{X} \rightarrow \mathcal{H}$

$$\text{MMD}(P,Q)=|| \mathbb{E}_{X \sim P}[\phi(X)] -  \mathbb{E}_{Y \sim Q}[\phi(Y)]||_\mathcal{H}$$

Where MMD can be understood as the distance in $\mathcal{H}$ between the kernel embeddings.
Where the null hypothesis is that both samples stem from the same distribution,  (i.e. $\mathbb{P} = \mathbb{Q}$) and the alternative hypothesis is that they are not drawn from the same distribution such that $\mathbb{P} \neq \mathbb{Q}$.

In [get citation], the unbiased, estimate of the squared MMD is shown to be:

\begin{align*}
\widehat{\mathbf{M M D}}_{u}^{2}(\mathcal{F}, X, Y)=\frac{1}{m(m-1)} \sum_{i=1}^m \sum_{ j=1}^{m} k\left(x_{i}, x_{j}\right)-\frac{2}{m n} \sum_{i=1}^m \sum_{ j=1}^{n} k\left(x_{i}, y_{j}\right)+\frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j=1}^{n} k\left(y_{i}, y_{j}\right)
\end{align*}
Which by law of large numbers converges to the theoretical values at a rate of XX. On its own, the MMD is not a metric but it can be one if the chosen kernel,$k$, is a \textit{characteristic kernel}. That is, the kernel mean embedding is injective in the mapping into the reproducing kernel Hilbert space. This property is critical as it ensures MMD=0 if and only if $\mathbb{P}=\mathbb{Q}$. As [cite KME review] points out, there is no loss if information in this case.

For instance given the radial basis function kernel, $k(x, y)= e^{-\frac{1}{2\sigma^2}||x-y||^2}$.

\hl{REWORD: We call the function that achieves the supremum, the witness function because it is the function that witnesses the difference in the two distributions. This means that we can interpret the witness function as showing where the estimated densities of
p and q are most different.}

The witness function 
\begin{equation}
f(x)=\mathbb{E}_{x^{\prime} \sim p}\left[k\left(x, x^{\prime}\right)\right]-\mathbb{E}_{x^{\prime} \sim q}\left[k\left(x, x^{\prime}\right)\right]
\end{equation}

which can also be estimated from finite samples of data by:
\begin{equation}
\hat{f}(x)=\frac{1}{m} \sum_{i=1}^{m} k\left(x, x_{i}\right)-\frac{1}{n} \sum_{i=1}^{n} k\left(x, y_{i}\right)
\end{equation}
Thus, as [need citation] points at, the witness function tracks where the densities of $X$ and $Y$ are most different. 

\section{Our Contributions}
The contributions of this work are several fold. First, given the strong emphasis of theoretical results in the change-point detection community, we test classic on-line change-point algorithms with modern, competitive algorithms on synthetic data and real-world data. Classification error as well as time to detection are compared as on-line methods are tasked with balancing this trade-off and comparisons between algorithms that leave out one of the these two key metrics out is incomplete. The focus of recent algorithms is on kernel methods such as KCUSUM, NEWMA and Mstats-CPD. 