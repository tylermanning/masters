\chapter{Introduction}
\section{Motivation}
Almost 100 years ago, Walter Shewart recognized the need for monitoring manufacturing processes in real-time in a systematic way. The use of statistical control charts allowed for real-time detection of changes in variation that may have indicated a degradation in quality in the production process \cite{shewhart1931economic} . Shewart's method was one of the first formal frameworks to solve the problem of detecting changes in a distribution of a sequence of random variables. This problem would come to be known more generally as the \textit{change point detection problem} and would come to apply across various industries. The following are a few motivating examples.

\subsection{Health Care}
Health care is an important area for quickly detecting signal changes in heart rate monitoring \cite{yang2006adaptive} \cite{staudacher2005new}, epilepsy signal segmentation \cite{malladi2013online}, and multi-modal MRI lesion detection \cite{bosc2003automatic} to name a few. Quickly detecting changes to a patient's health is absolutely necessary for any system to be of practical use. However, this quick detection must be balanced with high accuracy as false positives or missed detections could have life-threatening consequences.

%\subsection{Computer Network Surveillance}
%Networks/monitoring systems for intrusions and/or changes in climate or natural disasters.

\subsection{Financial Applications}
The application of accurate and timely change point detection is also very appealing to the finance sector where shifts in asset prices can happen suddenly. Change point detection is further complicated in financial applications because of the non-stationarities typically observed in asset pricing along with long memory processes. Note, in the financial literature, change-points  are also referred to as structural breaks, but for this thesis will use the broader term change-points.

An on-line, quick detection technique is proposed in  \cite{pepelyshev2015real}, where a modified Shiryaev - Roberts procedure is used in a case study to detect a change-point on a single stock's daily returns. They compare their non-parametric method with other classic control chart methods using speed of detection and false alarm rate as measures of performance.

Detecting changes in variance is specifically explored in  \cite{lavielle2007adaptive}. The authors propose an off-line change-point algorithm that minimizes a global cost function by using an adaptive regularization function. The algorithm is applied to the absolute returns of the FTSE 100 stock index and the US dollar-Japanese Yen foreign intra-day exchange rate to detect changes in asset price volatility. The change-points  identified in the FTSE 100 coincided with key market events such as the stock market crash that occurred on October 14$^{th}$, 1987 and breaking the 5000 price barrier in August 1997.

See section 1.3.6 of \cite{tartakovsky2014sequential} for more applications to options markets and arbitrage opportunities.

\section{Characteristics of the change point problem}
A number of surveys of the literature already exist \cite{aminikhanghahi2017survey} \cite{niu2016multiple}, therefore we will not cover all existing methods but rather touch upon several, important factors to consider when tackling the change point detection problem. Across the body of literature, these factors determine what methods are available to practitioners. %Furthermore, Bayesian methods that relate to change point detection but will not be covered in this thesis.

The first factor is selecting between \textit{parametric} and \textit{non-parametric} techniques. Deciding between these two broad techniques is dependent on the prior knowledge one wants to encode into the problem. For example, if it is known that data is generated by a distribution from the exponential family of distributions, then we can subset the problem from the space of all possible distributions to a smaller space of distributions.For example, Shewart control charts and CUSUM change-point techniques are all parametric techniques based on Gaussian-family of distributions \cite{page1954continuous} \cite{chen2011parametric}. In certain settings, it is not possible to leverage information about the data and non-parametric techniques must be used instead \cite{brodsky2013nonparametric}.

% . On the other hand, if the data is being generated by processes that have no fundamental or well-understood process then modelling particular probability distributions becomes intractable or risky. Therefore, models that do not model particular probability distributions must be used. For reference, see monograph by Brodsky and Darkhovsky \cite{brodsky2013nonparametric}. % Usually this added flexibility is a trade-off with performance and/or speed with parametric models.

The second factor is to deciding whether change-points should be detected \textit{offline} or \textit{online}. Some algorithms are off-line---also referred to  as batch algorithms or retrospective or a posteriori change-point detection---and they are applied in an ex-post fashion after the dataset has been completely acquired \cite{truong2018review}. The aforementioned Shewart control chart and CUSUM algorithm were both designed for data that is streamed in a real-time fashion. In the literature, on-line methods of change point detection are also referred to sequential change point detection  \cite{tartakovsky2014sequential}. For this thesis, we will use the terms interchangeably.% For an exhaustive review of sequential change point analysis, see the book by Tartakovsky, Nikiforov, and Basseville  \cite{tartakovsky2014sequential}. %It is possible to convert an on-line algorithm to an off-line algorithm by simply 

%The third consideration that is a consequence of detecting change points in an on-line setting is the presence of outliers. This a particularly important concern for on-line methods that are looking for changes in the underlying distribution of points, not a single anomalous data point

The third factor is determining if there are multiple change points or to assume there is only a single change point to detect. This is an important  factor for off-line change point detection where the decision to detect 1 or more change-points is chosen at the outset.  Detecting multiple change-points could also be relevant for the on-line case if a situation arises where the window of time series under consideration may contain more than one change point. However, most on-line change-point methods are designed to detect a single change-point at a time.

Finally, the last factor to address is determining exactly what kinds of statistical changes the algorithm should detect. Many methods focus solely on detecting changes in the mean of a distribution \cite{lee2010change}. Some methods are more general and can detect changes in the variance or higher order moments and do not focus on any particular one. Methods like kernel change point detection can typically detect any distributional changes. 

This thesis will concern itself with on-line change point detection, where data is received in a streaming nature. We assume no prior distributional characteristics on the data and operate in a completely non-parametric setting. 

\section{Problem Formulation}
The basic change-point problem is set up as hypothesis test between two segments of a time series. Since we are concerned with the on-line setting, we will always consider a time series of fixed size, $n$.  Let $X_i,...,X_n$ be a series of independent random variables observed. Then, one of the following hypotheses holds:
\begin{equation}
  \begin{cases}
    H_0: X_1, X_2, ...,X_n \sim  F_0 & \text{(no change-point occured)} \\
    H_1: X_i \sim F_1, X_j \sim F_2 & \text{(a change-point occured)}. 
  \end{cases}
\end{equation}
Where $i=1,2,...,t-1$ and $j=t,...,n$  are two distinct segments separated by change-point $t$ that is within the time series window. Furthermore, $F_0, F_1,F2$ are cumulative distribution functions (CDFs) with corresponding probability density functions (PDFs), $f_0, f_1, f_2$. Because we are operating in a non-parametric setting, the CDFs are assumed to be completely unknown. 

Many change-point detection algorithms define a statistic that is computed using each set before and after the possible change-point,$t$. If the statistic is above a particular threshold then time $t$ is classified as a change-point, $\hat{\tau}$.



In the on-line scenario, the time series under consideration can be thought of as a sliding window with data constantly coming in and out of the window of interest. The size of the window is an important consideration that is typically chosen based on the problem being solved. Too small a window and the sets of data may not yield a statistically significant result. Too large of a window and the problem leans more towards and off-line model, where high volumes must be stored and several change-points may appear in a given window.

\subsection{Performance Measures}
Because of the unsupervised nature of detecting change-points, it is difficult to evaluate the performance of change-point detection models with real world data. Many papers detail asymptotic and non-asymptotic theoretical guarantees of their proposed change-point methods.  These theoretical optimalities are typically compared across different change-point methods for benchmarking a new algorithm. 

Average run length is often used to compare A and B by Y. It is a function of $\theta$ and $N$ and is defined as:
\begin{equation}
ARL = E_{\theta}[N]
\end{equation}
Where it is the expected number of observations that must be recorded before a change-point is incorrectly detected. Therefore, a larger value of ARL is preferable. 

Similarly, worst case detection delay is a metric that gives an idea of how slow a model will detect a  change-point in a worst case scenario.

If labelled change-points are available for a real world dataset or a synthetic dataset is created, then the ground truth change-point vector, $\tau^*$, is known. For example the \textit{Hausdorff} metric can be used. It measures the furthest temporal distance between a predicted change-point $\hat{\tau}$ and $\tau^*$. It is defined as:

Other standard classifier metrics can also used for comparing $\hat{\tau}$ and $\tau^*$. This includes the  F1-Score that is based on a classifier's precision and recall:
\begin{equation}
F_1(\hat{\tau}, \tau^*) = 2 * \frac{\text{precision*recall}}{\text{precision + recall}}
\end{equation}

 F1-Score is defined as the harmonic mean of precision and recall. Precision is defined as the ratio of true positives (TP) to the
number of true positives (TP) and false positives (FP) and recall is defined as the ratio the number of true positives to the
number of true positives plus the number of false negatives. F1-Score is best when F1 = 1 (perfect precision and recall) and reaches its worst value at F1 = 0. Depending on the context, any other classifier evaluation tools such as the Receiver Operating Characteristics Curve and the Precision Recall Curve may be used as well.

\section{Related Work}

In 2005, Desobry et al. \cite{desobry2005online} developed an on-line kernel change point detection model based on single class support vector machines ($\nu$-SVMs). The authors train a single class support vector on a past set, $\mathbf{x}_{t,1}={x_{t-m_1},...,x_{t-1}}$ of size $m_1$ and train another single class support vector on a future set $\mathbf{x}_{t,2}={x_t,...,x_{t+m_2-1}}$ of size $m_2$. A ratio is then computed between the two sets that acts as the dissimilarity measure in Hilbert space. If the points are sufficiently dissimilar over some predetermined threshold,$\eta$, then a change point is assigned to the time spitting the two sets of data. Desobry argues that a dissimilarity measure between kernel projection of points in a Hilbert space should estimate the \textit{density supports} rather than estimate the probability distributions of each set of points. 

In 2007, Harchoui and Cappe \cite{harchaoui2007retrospective} approached the off-line change point problem with a fixed number of change points by using kernel change point detection. This was further extended to an unknown number of change points in 2012 by Arlot et al. \cite{arlot2012kernel}. Finally, Garreau and Arlot extended this in line of research kernel change points in the off-line setting of detecting change points. Fundamentally, their method is the kernel version of the following least squares optimization problem:

\begin{equation}
J(\tau, \mathbf{y}) = \frac{1}{n} \sum_{k=1}^K(\tau) \sum (Y_i - \hat(Y_k)^2 + \beta \text{pen}(\tau)
\end{equation}


The benefits of this off-line kernel change point detection is that it operates on any kind of data for which a kernel that properly reproduces a Hilbert space can be applied. For example, it can be applied to image data, histogram data, as well as data in $\mathbb{R}^P$. Garreau shows their KCP procedure outputs an off-line segmentation near optimal with high probability. Lastly, the authors recommend choosing the kernel based on best possible signal to noise ratio that the distribution gives based on $ \Delta^2 / M^2$ . Therefore, some prior knowledge of the data is necessary for choosing the kernel. Finally, a comparison is made to the kernel two sample hypothesis test that is proposed in  \cite{li2015m} that uses  the kernel maximum mean discrepancy (MMD) B-test statistic for two-sample testing for rejecting null hypothesis of first equation.

%His method is theoretically sound and tested but has not been implemented on very many real datasets nor compared with other benchmark methods.
%In an application setting, the user would use some training set to calibrate the kernel and the penalty parameters, to then be tested with appropriate accuracy measures on some out of sample data set.

%https://media.nips.cc/nipsbooks/nipspapers/paper_files/nips28/reviews/1852.html

 %The B-test statistic is a recently developed alternative to the MMD that is more efficient; it involves taking an average of the MMD over a partitioning of the data into N blocks.

%In 2016, James et al. \cite{james2016leveraging} proposed using energy statistics to test the significance of a change point that is robust to anomalies. They point out that their work is the first to accurately detect change points with fast time to detection while not being affected by extreme outliers that are not change points. They compare their E-divisive with medians algorithm to the parametric PELT technique. 



 

\section{Our Contributions}
Lay out the contributions of this work.