\chapter{Introduction}
\section{Motivation}
The use of statistical control charts for detecting real-time changes in variation was pioneered by Walter Shewart in the first half of the twentieth century.  Shewart was interested in reducing the unexpected causes of variation in the manufacturing processes that produced faulty manufacturing equipment \cite{shewhart1931economic}. Shewart's method involved charting the process measurements over time and detecting when a statistical process was no longer exhibiting an expected level of variation. Once this detection occurred, the process was stopped and was not restarted until the cause of the variation was fixed.
Shewart's control charts were one of the first formal frameworks to solve the problem of detecting changes in a distribution of a sequence of random variables. This problem is now known more generally as the \textit{change point detection problem}. Many industries make use of change-point techniques including healthcare monitoring systems, monitoring computer network traffic, and detecting regime shifts financial markets. The following are a few motivating examples.

\subsection{Health Care}
Health care is an important area for quickly detecting signal changes. Some recent studies include applications to heart rate monitoring \cite{yang2006adaptive} \cite{staudacher2005new}, epilepsy signal segmentation \cite{malladi2013online}, and multi-modal MRI lesion detection \cite{bosc2003automatic} to name a few. Quickly detecting changes to a patient's health is absolutely necessary for any system to be of practical use. However, this quick detection must be balanced with high accuracy as false positives or missed detections could have life-threatening consequences. Therefore, balancing type I and type II error is a central theme to online change-point detection.

%\subsection{Computer Network Surveillance}
%Networks/monitoring systems for intrusions and/or changes in climate or natural disasters.

\subsection{Financial Applications}
The application of accurate and timely change point detection is also very popular in the finance sector where shifts in asset prices can suddenly happen. Change point detection is particularly hard in financial applications because of the non-stationary data typically observed in asset price time series. Note, in the financial literature, change-points  are also referred to as structural breaks, but for this thesis we will use the broader term change-points.

An on-line, quick detection technique is proposed in  \cite{pepelyshev2015real}, where a modified Shiryaev - Roberts procedure is used in a case study to detect a change-point on a single stock's daily returns. They compare their non-parametric method with other classic control chart methods using speed of detection and false alarm rate as measures of performance.

Detecting changes in variance is specifically explored in  \cite{lavielle2007adaptive}. The authors propose an off-line change-point algorithm that minimizes a global cost function by using an adaptive regularization function. The algorithm is applied to the absolute returns of the FTSE 100 stock index and the US dollar-Japanese Yen foreign intra-day exchange rate to detect changes in asset price volatility. The change-points  identified in the FTSE 100 coincided with key market events such as the stock market crash that occurred on October 14$^{th}$, 1987 and breaking the 5000 price barrier in August 1997.

See section 1.3.6 of \cite{tartakovsky2014sequential} for more applications to options markets and arbitrage opportunities.

\section{Characteristics of the change point problem}
A number of surveys of the literature already exist \cite{aminikhanghahi2017survey} \cite{niu2016multiple}, therefore we will not cover all existing methods but rather touch upon several, important factors to consider when tackling the change point detection problem. Across the body of literature, these factors determine what methods are available to practitioners. %Furthermore, Bayesian methods that relate to change point detection but will not be covered in this thesis.

The first factor is selecting between \textit{parametric} and \textit{non-parametric} techniques. Deciding between these two broad techniques is dependent on the prior knowledge one wants to encode into the problem. For example, if it is known that data is generated by a distribution from the exponential family of distributions, then we can subset the problem from the space of all possible distributions to a smaller space of distributions. Shewart control charts and CUSUM change-point techniques are both parametric techniques based on the Gaussian-family of distributions \cite{page1954continuous} \cite{chen2011parametric}. In other settings, it is not possible to leverage information about the data and non-parametric techniques must be used instead \cite{brodsky2013nonparametric}.

% . On the other hand, if the data is being generated by processes that have no fundamental or well-understood process then modelling particular probability distributions becomes intractable or risky. Therefore, models that do not model particular probability distributions must be used. For reference, see monograph by Brodsky and Darkhovsky \cite{brodsky2013nonparametric}. % Usually this added flexibility is a trade-off with performance and/or speed with parametric models.

The second factor is deciding whether change-points should be detected \textit{offline} or \textit{online}. Some algorithms are off-line---also referred to  as batch algorithms or retrospective or a posteriori change-point detection---and they are applied in an ex-post fashion after the dataset has been completely acquired \cite{truong2018review}. If change-points must be detected as soon as possible, then waiting for the entire dataset to be acquired is not feasible and methods that operate on data streams must be used. Such methods fall into the category of on-line change-point detection. The aforementioned Shewart control chart and CUSUM algorithm are both designed for data that is streamed in a real-time fashion. In the literature, on-line methods of change point detection are also referred to sequential change point detection  \cite{tartakovsky2014sequential}. For this thesis, we will use the terms interchangeably.% For an exhaustive review of sequential change point analysis, see the book by Tartakovsky, Nikiforov, and Basseville  \cite{tartakovsky2014sequential}. %It is possible to convert an on-line algorithm to an off-line algorithm by simply 

%The third consideration that is a consequence of detecting change points in an on-line setting is the presence of outliers. This a particularly important concern for on-line methods that are looking for changes in the underlying distribution of points, not a single anomalous data point

The third factor is determining if there are multiple change points or to assume there is only a single change point to detect. This is an important  factor for off-line change point detection where the decision to detect one or more change-points is often chosen at the outset.  Detecting multiple change-points could also be relevant for the on-line case if a situation arises where the window of time series under consideration may contain more than one change point. However, most on-line change-point methods are designed to detect a single change-point at a time.

Finally, the last factor to address is determining exactly what kinds of statistical changes an algorithm should detect. Many methods focus solely on detecting changes in the mean of a distribution \cite{lee2010change}. Some methods are more general and can detect changes in the variance or higher order moments and do not focus on any particular one. Methods like kernel change point detection can typically detect any distributional changes, making them attractive in situations where very little is known about the data.

This thesis will concern itself with on-line change point detection, where data is received in a streaming nature. We assume no prior distributional characteristics on the data and operate in a completely non-parametric setting. 

\section{Background}
This sections describes how the change-point problem will be formulated in this thesis and, by extension, how all methods will be described using the change-point detection problem notation. Because online change-point detection is closely related to two sample testing, a background on statistical hypothesis testing is presented first. 

\subsection{Hypothesis Testing}

Two-sample hypothesis testing concerns itself with the following problem: Suppose data is independently, sampled from two groups of different populations. Sample one is denoted as  $X=\{X_1, X_2, ...,X_n\} \sim P$ and sample two is denoted as $Y=\{Y_1, Y_2, ...,Y_n\} \sim Q$. The fundamental question of a two-sample test is determining whether samples are $X$ and $Y$ are significantly different on a statistical level. Therefore, a two-sample hypothesis can be setup where the null hypothesis, $H_0$, is that the two distributions do not differ based on some statistical test. If the null hypothesis is rejected, then the alternative hypothesis, $H_1$, is accepted and we conclude the two samples differ in their distributions according to the specific two-sample test used.

\begin{equation}
  \begin{cases}
    H_0: P = Q & \text{(both samples come from the same distribution)} \\
    H_1: P \neq Q & \text{(samples do not come from the same distribution)}. 
  \end{cases}
\end{equation}

In order to summarize the difference between the two samples, a test statistic, $\hat{t} \in \mathbb{R}$, is computed based on the type of test used. This test statistic is compared to a significance level, $\alpha \in [0,1]$ that is chosen  at the outset. Common choices for $\alpha$ are $0.1, 0.05$ and $0.01$. In other words the p-value$=\mathbb{P}(T>\hat{t}|H_0$.

Given the two possible outcomes of a two-sample sample test, it is clear the hypothesis test can fail in two ways. The first is rejecting the null hypothesis when it is correct. This is known as a false-positive or a type-I error and is upper-bounded by the chosen significance level, $ \alpha$.  Therefore the probability of falsely rejecting the null hypothesis is $ 1 - \alpha = \mathbb{P}(\text{reject } H_0 | H_0 \text{ is true})$. The second possible source of error, is  a type-II error or false negative and the probability of committing a type-II error is $\beta$. The quantity $1-\beta$ is referred to as the \textit{power} of a test. It is equivalent to \(P\left(\text { reject } H_{0} | H_{0} \text { is false }\right)\). Maximizing test power is an important part of designing new algorithms and this metric is typically used as a metric to compare different methods. Often there is a trade-off  between type-I and type-II error and the practitioner must decide how to balance the two given their domain-specific knowledge of the problem. In some cases, it may be desirable to sacrifice one for the other, such as in medical field where a false positive diagnosis is a more desirable outcome than missing a diagnosis (type-II error) and never giving treatment to a patient. 

Many two sample tests exist for tackling the different types of differences two samples may have. For example, the Student $t$-test is a two sample test for determining if two samples of univariate data come from a population with the same mean. A generalization of the Student $t$-test for the multivariate case is the Hotelling $T^2$ test that tests whether multivariate means of two samples are significantly different. Both of these are parametric tests as they assume the samples are normally distributed. 

% Add citation for Hotelling, H. (1931). The Generalization of Student’s Ratio. Ann. Math. Statist., 2(3):360–378.

Non-parametric tests also exist such as the Kolmogorov-Smirnov test (K-S test) for determining whether two samples come from the same distribution. This is done by computing the \textit{supremum} of the difference of the empirical cumulative distribution functions from each sample. That is, $\text{KS statistic} =\text{sup} |P(x) - Q(x)|$. Depending on the number of samples and the significance level chosen, the K-S statistic may reject or fail to reject the null hypothesis. The K-S test does not specify what distribution the samples come from, only if they differ according to the K-S statistic. More recently, the kernel two-sample test was introduced in \cite{gretton2012kernel} as a very flexible, non-parametric test. It is not limited to one dimensional data, and can be applied to non-numeric data. It is based on the \textit{maximum mean discrepancy} (MMD) criterion and is capable of detecting any kind of change in statistical moment. It is a focus in this thesis and is discussed in further detail in section \ref{mmd}.

\subsection{Maximum Mean Discrepancy}
\label{mmd}
The maximum mean discrepancy (MMD) is a type of \textit{integral probability metric} that can be used to compare two probability distributions. It is based on the kernel mean embedding developed in \cite{smola2007hilbert}. The kernel mean embedding can be thought of applying the \textit{kernel trick} to probability measures and mapping them into a higher dimensional feature space. Rather than applying the kernel trick to individual data points and mapping them to an implicit feature space, the kernel mapping, $\phi$, (also called feature map) will be applied to a probability distribution in order to represent in a reproducing kernel Hilbert space (RKHS), that is $\phi: \mathcal{X} \rightarrow \mathcal{H}$.

Suppose $n$ samples $X = \{x_1, x_2, ..., x_n\}$ and and $m$ samples from a different set $Y=\{y_1, y_2, ..., y_m\}$ are observed from a sample space $\mathcal{X}$. They are distributed as $X \sim  P$ and $Y \sim Q$ respectively. The kernel mean embeddings are represented by $\mu_P = \mathbb{E}_{X \sim P}[\phi(X)] $ and $\mu_Q =\mathbb{E}_{Y \sim Q}[\phi(Y)]$ for samples $X$ and $Y$ respectively. It is not required but typically a \textit{characteristic} kernel function is chosen as it guarantees the mapping into the Hilbert space to be injective. As \cite{muandet2017kernel} points out, there is no loss if information in this case. See \cite{fukumizu2008kernel} for this proof. This implies that $||\mu_P - \mu_Q ||=0$ if and only if $P=Q$. This leads naturally to the definition of the MMD as:

\begin{equation}
\text{MMD}(P,Q)=|| \mathbb{E}_{X \sim P}[\phi(X)] -  \mathbb{E}_{Y \sim Q}[\phi(Y)]||_\mathcal{H}
\end{equation}

Where the MMD distance can be understood as the distance in $\mathcal{H}$ between the mean embeddings of the features. As long as a kernel mean embedding can be defined on the given data structure, the MMD can be used. This is why it can be applied to non-numeric data such as strings, graphs, and other data structures \cite{hofmann2008kernel}. One advantage of using the MMD approach for comparing distributions over other classic techniques like Kullback–Leibler (K-L) divergence is the density of the distributions do not have to be estimated as an interim step. 

The two-sample kernel test statistic is defined in \cite{gretton2012kernel} and uses the MMD as a distance measure for comparing two probability distributions. Where the null hypothesis is that both samples stem from the same distribution,  (i.e. $P=Q$) and the alternative hypothesis is that they are not drawn from the same distribution such that $P \neq Q$.

In \cite{gretton2012kernel}, the unbiased, estimate of the squared MMD is shown to be:

\begin{equation}
\begin{split}
\widehat{\mathbf{M M D}}_{u}^{2}(\mathcal{F}, X, Y)=\frac{1}{m(m-1)} \sum_{i=1}^m \sum_{ j=1}^{m} k\left(x_{i}, x_{j}\right)-\frac{2}{m n} \sum_{i=1}^m \sum_{ j=1}^{n} k\left(x_{i}, y_{j}\right)+ \\
\frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j=1}^{n} k\left(y_{i}, y_{j}\right)
\end{split}
\end{equation}


%Which by law of large numbers converges to the theoretical values at a rate of XX. 
%On its own, the MMD is not a metric but it can be one if the chosen kernel,$k$, is a \textit{characteristic kernel}. 

Common kernel functions used in practice are the radial basis function kernel, $k(x, y)= e^{-\frac{1}{2\sigma^2}||x-y||^2}$ and laplace kernel, $k(x, y)= e^{-\frac{1}{\sigma^2}||x-y||}$, where $\sigma > 0$. In \cite{gretton2005kernel}, the authors recommend selecting the bandwidth kernel, $\sigma$, based on the \textit{median heuristic}, which results in $\sigma^2=\text{median}\{||x_i-x_j||:i,j = 1,...,n\}$. This heuristic is a good starting point, however, it has been shown to be sub-optimal in high-dimensional and small-sample cases as shown in \cite{muandet2014kernel} and \cite{ramdas2015decreasing}. Choosing the proper kernel is often done on a per case basis by maximizing test power, thereby reducing the chance of Type-II error. Overall kernel selection is a difficult problem that is still actively researched.

\hl{REWORD: We call the function that achieves the supremum, the witness function because it is the function that witnesses the difference in the two distributions. This means that we can interpret the witness function as showing where the estimated densities of
p and q are most different.}

The witness function 
\begin{equation}
f(x)=\mathbb{E}_{x^{\prime} \sim p}\left[k\left(x, x^{\prime}\right)\right]-\mathbb{E}_{x^{\prime} \sim q}\left[k\left(x, x^{\prime}\right)\right]
\end{equation}

which can also be estimated from finite samples of data by:
\begin{equation}
\hat{f}(x)=\frac{1}{m} \sum_{i=1}^{m} k\left(x, x_{i}\right)-\frac{1}{n} \sum_{i=1}^{n} k\left(x, y_{i}\right)
\end{equation}
Thus, as [need citation] points at, the witness function tracks where the densities of $X$ and $Y$ are most different. 

Originally in \cite{gretton2012kernel}, it was thought the MMD does not suffer from the curse of dimensionaity when used to compare distributions in higher dimensions. However, it was shown in \hl{https://arxiv.org/pdf/1406.2083.pdf} that indeed the MMD does struggle in higher dimensions like many other metrics do.

\subsection{MMD as a two-sample test}
As mentioned in \hl{hypothesis testing section}, every two-sample test follows a similar procedure, therefore using the MMD as a measure of distance, the steps to perform a two-sample test  are as follows:

1.Distance MMD2k(P,Q):

Choose a kernel $k$
For characteristic $k$, k(P,Q)=0 iff P=Q
2.Estimate the distribution distance from data: MMD(X,Y)
3.Choose a rejection threshold calpha:

Under H0, mMMD converges to distribution depending on data, k

Kernel selection is important because it decides the kind of witness functions that can be learned. For most applications it is simply set to the RBF kernel but ideally it should be selected based on maximizing test power.

\section{Problem Formulation}

\subsection{Change-point Detection Problem}
The basic change-point problem is set up as hypothesis test between two segments of a time series. Let $X_i,...,X_n$ be a series of independent random variables of dimension $d \geq 1$ be sequentially observed . Then, one of the following hypotheses holds:
\begin{equation}
  \begin{cases}
    H_0: X_1, X_2, ...,X_n \sim  P & \text{(no change-point occured)} \\
    H_1: X_1,X_2, X_{t_0-1} \sim P, X_{t_0}, X_{t_0+1},  \sim Q & \text{(a change-point occured)}. 
  \end{cases}
\end{equation}
Where $i=1,2,...,t_0-1$ and $j=t_0,...,n$  are two distinct segments separated by change-point $t_0$ that is within the time series window.  Because we are operating in a non-parametric setting, the distributions $P, Q$ are assumed to be completely unknown. 

If there is no change in the data then we say the change time is equal to infinity and denote this probability as $P^{\infty}$ and the expectation is $E^{\infty}$.

Many change-point detection algorithms define a statistic that is computed using each set before and after the possible change-point,$t$. If the statistic is above a particular threshold then time $t$ is classified as a change-point, $\hat{\tau}$.

In the on-line scenario, the time series under consideration can be thought of as a sliding window with data constantly coming in and out of the window of interest. The size of the window is an important consideration that is typically chosen based on the problem being solved. Too small a window and the sets of data may not yield a statistically significant result. Too large of a window and the problem leans more towards and off-line model, where high volumes must be stored and several change-points may appear in a given window. If the amount of data is not a limitation then throttling the data may not be necessary.

\subsection{Performance Measures}
Because of the unsupervised nature of detecting change-points, it is difficult to evaluate the performance of change-point detection models with real world data. Many papers detail asymptotic or non-asymptotic theoretical guarantees of their proposed change-point methods.  These theoretical results are typically compared across different change-point methods for benchmarking a new algorithm.

Two main issues arise when detecting change-points in a stream of data. The first is detecting a change-point when there is no actual statistical change in the observed sequence. These are typically called false positives or \textit{false alarms} the in change-point detection literature. The false alarm rate is defined by a metric known as the \textit{time to false alarm} (TTFA) rate. 
\begin{equation}
TTFA = E_{\infty}[T] = E_{\theta}[N]
\end{equation}
Where it is the expected number of observations that must be recorded before a change-point is incorrectly detected. In other words, it is the average amount of time until a change is detected given a sequence of observations with no change. Therefore, a larger value of TTFA is preferable. From a hypothesis testing perspective, this is equivalent to rejecting $H_0$ in \hl{cite equation in problem statement} when it should not be rejected, i.e. type I error.

The second issue is not detecting a change-point when one occurs. This could be caused by detecting a change-point much too late for it to be of any use or simply missing it altogether. For quantifying this error, the worst case detection delay (WCD) metric measures how slow a model will detect a  change-point in a worst case scenario. Conversely to TTFA, lower values of WCD are preferable.
\begin{equation}
WCD = sup esssup E_t[(T-t)^+ | F_{t-1}]
\end{equation}

From a hypothesis testing perspective, this is equivalent to not rejecting $H_0$ in \hl{cite equation in problem statement} when it should be rejected, i.e. type II error.

Balancing the TTFA and WCD of an on-line detection algorithm is crucial to for an algorithm to be of any practical use. In \hl{1971 Lorden procedures to reacting...}, it was shown asymptotically that the CUSUM algorithm provides an optimal trade-off between TTFA and WCD and ,in \hl{moustakides optimal stopping times for detecting.. 1986}, it was proved optimal in the non-asymptotic case as well. Note, TTFA and WCD are also commonly referred to as ARL$_0$ and ARL$_1$ respectively where ARL stands for average run length. For clarity, we use the more explicit terms TTFA and WCD.

When detecting changes of a distribution, a user may want to quantify the size of the change in the mean by
$|\mathbb{E}[X_{\tau}]-\mathbb{E}[X_{\tau+1}]|$ or, similarly, the size of the change in the variance by $|\text{Var}[X_{\tau}]-\text{Var}[X_{\tau+1}]|$.

\subsection{Other performance measures}

If labelled change-points are available for a real world dataset or a synthetic dataset, then the ground truth change-point vector, $\tau^*$, is known. For example the \textit{Hausdorff} metric can be used. It measures the furthest temporal distance between a predicted change-point $\hat{\tau}$ and $\tau^*$. It is defined as:

Other standard classifier metrics can also used for comparing $\hat{\tau}$ and $\tau^*$. This includes the  F1-Score that is based on a classifier's precision and recall:
\begin{equation}
F_1(\hat{\tau}, \tau^*) = 2 * \frac{\text{precision*recall}}{\text{precision + recall}}
\end{equation}

 F1-Score is defined as the harmonic mean of precision and recall. Precision is defined as the ratio of true positives (TP) to the
number of true positives (TP) and false positives (FP) and recall is defined as the ratio the number of true positives to the
number of true positives plus the number of false negatives. F1-Score is best when F1 = 1 (perfect precision and recall) and reaches its worst value at F1 = 0. Depending on the context, any other classifier evaluation tools such as the Receiver Operating Characteristics Curve and the Precision Recall Curve may be used as well.

\section{Classic Algorithms}
Presented below are the fundamental approaches to on-line change-point detection that have been very influential. 

\subsection{Shewart Control Chart}
Shewart control charts were originally designed to detect changes in the mean of a process where the values being observed are assumed to be Gaussian distributed. As the data arrives, the data is batched into samples of size $N$.  The sample mean, $\bar{X}=\frac{1}{N} \sum_{i=1}^N X_i$, is the then calculated and compared it to a known, true mean $\mu^*$. If the absolute difference is greater than a threshold, then a change-point is declared at the current batch. Therefore, the decision rule is defined as,
\begin{equation}
|\bar{X}-\mu^*| > \kappa \frac{\sigma}{\sqrt{N}}
\end{equation}

Where $\kappa$ is a constant that controls how sensitive the algorithm is. Typically, it is set to $\kappa=3$ as this coincides with the observations within $3$ standard deviations of the mean.  Under the assumption that the data is distributed normally, $99.7$ of the observations are distributed in this region, therefore a change-point is declared if it falls outside this region.The true mean is assumed to known and is defined as $\mu^* = \mathbb{E}[X_i]$. In applications, the true mean can also be replaced by some target specification that a process must adhere to. Similarly, it is assumed the standard deviation, $\sigma$, is known in advance but it can also be estimated.

Tuning the hyper-parameters can drastically change the performance of the algorithm. Choosing a lower value for $\kappa$ makes the control chart detect change-points more often, whereas a higher value results in less detections. The chosen sample size, $N$, is also critical and its effect on the performance of Shewart control charts was studied in \cite{haridy2017effect}.

\subsection{CUSUM}
Similar to the Shewart control chart, the CUSUM algorithm tracks a statistic over time relative to a predetermined threshold. CUSUM is best applied to a process that is already under control. It can be thought of accumulating the information of current and past samples. 

The algorithm can be recursively defined by updating a statistic, $S_i$, after each $X_i$, such that:
\begin{equation}
  \begin{cases}
    S_0 = 0  & \text{(Initialization)} \\
    S_i = \text{max}(0, S_{i-1} + Z_i) & \text{for i=1,2,...}
  \end{cases}
\end{equation}
Where $Z_i=ln(\frac{f_{\theta_1}(X_i)}{f_{\theta_0}(X_i)})$ and the statistic $S_i$ is compared to a threshold $h$ that is predetermined by the user. If $Z_i \geq h$ then a change-point is declared at time $i$ and the algorithm is either stopped or restarted. Given that the statistic only flags change-points when greater than a threshold, this algorithm only detects positive changes in the distribution. In \cite{page1954continuous}, it is suggested to combine two CUSUM algorithms to detect positive and negative changes in a distributional parameter.

As a parametric algorithm, it is assumed the distributions, $f_0$ and $f_1$, are known at the outset. In most applications, this is quite limiting and unrealistic. Therefore, in cases where parameters $\theta_0$ and $\theta_1$ are unknown, maximum likelihood estimates of the parameters are usually computed.

\subsection{Extensions to CUSUM}
The filtered-derivative extension of the CUSUM introduced in \\cite{basseville1981edge} uses the change of the discrete derivative of a signal over time to detect a change-point. 

In \cite{lucas1982fast}, a fast initial response (FIR) CUSUM algorithm is proposed where the starting value of initial cumulative sums adapts over time. Instead of resetting $S_0$ to zero as shown above, it is reset to a non-zero value, typically based on the threshold chosen. This gives the algorithm a head-start in quickly detecting when a process is out of control and is especially useful for processes that don't start in control.

Finally, since CUSUM is typically better at detecting small shifts in signals and the Shewart control chart is faster at detecting larger changes, the two can be combined. The combined Shewart-CUSUM algorithm leverages the strengths of both techniques for better overall performance. See \cite{lucas1982combined}, \cite{yashchin1985analysis}, and \cite{westgard1977combined} for more details. 

\subsection{EWMA}
The exponentially weighted moving average as described in \cite{roberts1959control} (originally called a geometric average), is a type of moving average that applies exponentially weighting. First introduced as a forecasting technique in the econometrics field for smoothing noisy functions, the EWMA can also be used for determining out of control processes as shown in \cite{hunter1986exponentially}. Rather than weight all observations uniformly like the standard CUSUM algorithm or a simple moving average, a decay factor (also called a  forgetting factor), $\lambda$, is used to control how much weight is distributed over the previous observations. As each new observation arrives, the EWMA statistic is updated and compared to a threshold. If the EWMA statistic exceeds the threshold then the process is deemed out of control or, in other words, a change-point is detected.

The EWMA statistic is calculated as follows at each time step, $t$:
$$S_t = \lambda X_t + (1-\lambda)S_{t-1} $$

Where the decay factor,  $\lambda$, is set to a value between $0$ and $1$. As $\lambda$ approaches $1$, the EWMA control chart gives more and more weight to the most recent observations similar to a Shewhart control chart that gives weight to the last observation only. Conversely, as $\lambda$ approaches $0$, the weights are distributed further into the past giving the EWMA a longer memory similar to the CUSUM algorithm. Therefore, a EWMA control chart can be interpreted as a trade-off between a Shewhart control chart and a CUSUM control chart. 

For detecting deviations away from a mean target value, control limits may be calculated in a similar manner to the Shewart control chart. In \cite{hunter1986exponentially}, control limits for the EWMA are chosen to be  $\pm 3 \cdot \sigma_{\text {Shewhart }} \sqrt{\frac{\lambda}{2-\lambda}}$. The standard deviation, $\sigma_{\text {Shewhart }}$, is calculated similarly to the Shewart control chart problem. %and its derivation is shown in the appendix of \hl{XX}.

More generally, if a function, $g(x)$ is applied to the observations such that we monitor $\tau= \mathbb{E}[g(X_t)]$ over time. Then the EWMA statistic will be compared to the in-control $\tau^*$ at each iteration and is assumed to be known beforehand.  

As with the other methods previously mentioned, the standard EWMA is a parametric method as it assumes the time series has some in-control average that is known prior to use. This makes is difficult to apply in situations where the data is non-numeric or where third or fourth moment changes in the distribution occur. It is however very fast due to its recursive structure and does not hold a lot of data in memory making it appealing for live data streams that rely on quick processing.


\section{Our Contributions}
The contributions of this work are several fold. First, given the strong emphasis of theoretical results in the change-point detection community, we test classic on-line change-point algorithms with modern, competitive algorithms on synthetic data and real-world data. Classification error as well as time to detection are compared as on-line methods are tasked with balancing this trade-off and comparisons between algorithms that leave out one of the these two key metrics out is incomplete. The focus of recent algorithms is on kernel methods such as KCUSUM, NEWMA and Mstats-CPD. 