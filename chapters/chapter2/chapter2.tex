\chapter{Background}
This chapter describes how the change point problem will be formulated in this thesis and, by extension, how all methods will be described using the change point detection problem notation. Because online change point detection is closely related to two-sample testing, a background on statistical hypothesis testing is presented first. 

\section{Hypothesis Testing}
\label{hyptesting}
%Two-sample hypothesis testing concerns itself with the following problem: 
Let $x$ and $y$ be random variables defined on the topological space $\mathcal{X}$ with probability distributions $P$ and $Q$ respectively. Assume we draw $n$ observations from $P$ and $m$ observations from $Q$ resulting in two samples $X=\{x_1, x_2, ...,x_n\} \sim P$ and $Y=\{y_1, y_2, ...,y_m\} \sim Q$, where each sample is independently and identically distributed with respect to $P$ and $Q$. The main question posed in this thesis is can we determine if $P$ and $Q$ are statistically the same or different distributions. 

To answer this question, we use the \textit{statistical hypothesis testing} framework as it is described in \cite{casella2002statistical}. Generally, a $hypothesis$ is a statement about a population parameter, $\theta$. Examples of a population parameter are the population mean, variance or other higher order moments. In hypothesis testing, we are trying to determine whether one of two complementary hypotheses is true. The first, denoted by $H_0$, is called the \textit{null hypothesis} and it states that $\theta \in \Theta_0$ where $\Theta_0$ is some subset of the parameter space. The second hypothesis, denoted by $H_1$, is called the \textit{alternative hypothesis} and it states that $\theta \in \Theta_0^c$. For instance, if it is believed $P$ and $Q$ are distinguishable by their population means, $\mathbb{E}[P]$ and $\mathbb{E}[Q]$, then the possible hypotheses are:
 
\begin{equation}
\label{ex_hyp}
  \begin{cases}
    H_0: \mathbb{E}[Q] \in \{\mathbb{E}[P]\} & \text{(same mean, i.e. } P=Q) \\
    H_1: \mathbb{E}[Q] \notin \{\mathbb{E}[P] \} & \text{(different mean, i.e. } P \neq Q). 
  \end{cases}
\end{equation}

How do we pick between $H_0$ and $H_1$ in equation \ref{ex_hyp}? Every hypothesis test relies on a corresponding \textit{test statistic} $T$ that is a real-valued random variable. Because $P$ and $Q$ are unknown in our context, an estimate $\hat{T}$ can be calculated using $X$ and $Y$ such that $T: \mathcal{X}^n \times \mathcal{X}^m \rightarrow \mathbb{R}$, which yields:
\begin{equation}
\label{test_func}
\hat{T} = T(X, Y).
\end{equation}

The test statistic $\hat{T}$ is then compared to some \textit{threshold} selected by the user. There are many ways to set such a threshold, but usually if the test statistic exceeds the threshold then $H_0$ is rejected and $H_1$ is accepted. Otherwise, the hypothesis test fails to reject $H_0$. 

A common way to set the threshold is through a significance level, $\alpha \in [0,1]$ that is chosen at the outset. Common choices for $\alpha$ are $0.1, 0.05$ and $0.01$ \cite{moore1993introduction}. The test statistic is compared to $\alpha$ by computing a \textit{p-value} that is estimated by $\hat{p}=P(T \geq \hat{T} | H_0)$. The p-value is the probability of observing $\hat{T}$ under the null hypothesis.  A p-value < $\alpha$ would be improbable under the null hypothesis, therefore, in this situation it is rejected and the alternative hypothesis is accepted.

Given the binary outcome of a two-sample test, it is clear the hypothesis test can fail in the two following ways. The first is rejecting the null hypothesis when it is correct. This is known as a false positive or a type-I error and is upper-bounded by the chosen significance level, $ \alpha$. It is equivalent to following conditional probability: $\mathbb{P}(\text{reject } H_0 | H_0 \text{ is true})$. The second possible source of error is  a false negative or type-II error. The probability of committing a type-II error is denoted as $\beta=\mathbb{P}\left(\text {accept } H_{0} | H_{0} \text { is false}\right)\). The quantity $1-\beta$ is referred to as the \textit{power} of a test. Maximizing test power is an important part of designing new algorithms and is typically used to compare different methods. 

There is often a trade-off between type-I and type-II errors and the practitioner must decide how to balance the two given their domain-specific knowledge of the problem. In some cases, it may be desirable to sacrifice one for the other. For example, in the medical field \cite{lieberman2009type}, a false positive diagnosis (type-I error) may be more desirable than missing a diagnosis (type-II error) which would result in never giving treatment to a patient. 

Note, there is not a lot of information at our disposal for constructing a non-parametric two-sample hypothesis test. Distributions $P$ and $Q$ are unknown, $X$ and $Y$ are the observed data and the other settings are chosen according to the specific hypothesis test.  This means the main decision left up to the practitioner is determining what statistical test to use. In other words, what $T$ should be used for evaluating equation \ref{test_func}? The choice should depend on how $P$ and $Q$ may differ from each other. For example, the Student $t$-test is a two-sample test for determining if samples of univariate data come from a population with the same mean \cite{student1908probable}. A generalization of the Student $t$-test for the multivariate case is the Hotelling $T^2$ test that compares whether the means of two multivariate samples are significantly different \cite{hotelling1992generalization}. Both of these are parametric tests as they assume the samples are normally distributed. This means, in the context of our problem where nothing is known about $P$ and $Q$, they are not suitable tests for comparing two samples.

Alternatively, non-parametric tests make no assumptions about the distributions $P$ and $Q$. For example, the Kolmogorov-Smirnov test (KS test) \cite{massey1951kolmogorov} can determine whether or not two univariate samples come from the same distribution. This is done by computing the \textit{supremum} of the difference of the empirical cumulative distribution functions from each sample. The KS test does not specify what distribution the samples come from, only if they differ according to the KS statistic. More recently in \cite{gretton2012kernel} , the kernel two-sample test is introduced as a flexible, non-parametric test. It is not limited to one dimensional data, and can be applied to non-numeric data. It is based on the \textit{maximum mean discrepancy} (MMD) statistic and is capable of detecting any kind of change in distribution. It is a focus in this thesis and is discussed in more detail in section \ref{mmd}.

\section{Problem Formulation}
Because there is no official standard formulation for the online change point detection problem in the literature, we use the description in \cite{kifer2004detecting} as the basis for the following problem description. 

\subsection{Change Point Detection Problem}
\label{probFormulation}
Consider a data stream $X$ to be a sequence of random variables, $X_1, X_2, X_3,...$ where each $X_t$ for $t=0,1,2,...$ is generated by some probability distribution $P_t$ and each $X_t$ is independent of the one that came before it. A change point occurs at time $t+1$ if $P_t \neq P_{t+1}$. The goal of online change point detection is detecting this change point as soon as possible. 

Consider the case where we don't know when $P_t \neq P_{t+1}$ and we rely only on the observed values $X_t$ to answer this question. We can re-frame the online change point problem to the problem of comparing two sets from $X$. The intuition behind this is simple. If indeed a change point occurs then any observed values after the change point should exhibit a statistical difference with observed values prior to the change point. We simply need to decide what observations should be part of each set so that a significant comparison can be made. Using this approach, a general formulation can now be made.

Let $w$ be the number of most recent observations of $X$. Let this set be denoted by $X_t^w = \{X_{t-w},...,X_{t-1}, X_t\}$ whose length is the window, $w$ where $ w \in \mathbb{Z}^+$. This recent set will be compared to some reference set of $X$ denoted by $X_t^{ref}= \{X_0, X_1, ..., X_{t-w} \}$ whose distributions $P_0 = P_1=...= P_{t-w}$ are sampled from a common distribution $P$. What is this reference set? It is data that is known to be distributed consistently prior to a change point. In the process control literature, it is known as the in-control distribution \cite{bersimis2007multivariate}. We will use these terms interchangeably as they capture the same idea. We do not explicitly state how many observations are stored for $X_t^{ref}$ because this depends on the change point detection method used. 

Let $\theta_t$ be the parameter we expect to change in our data stream at a particular time. The most recent parameter $\theta_t^w$ will be compared to the set reference parameters, $\{\theta_0, \theta_1,...,\theta_{t-w}\}$, to determine if a change point has occurred. In other words, at each time, $t$, the online change point detection problem is performing the following hypothesis test:
\begin{equation}
\label{formula_cpd}
  \begin{cases}
    H_0: \theta_t^w \in \{\theta_0, \theta_1,...,\theta_{t-w}\}  & \text{(no change point occured)} \\
    H_1: \theta_t^w \notin \{\theta_0, \theta_1,...,\theta_{t-w}\}  & \text{(a change point occured)}
  \end{cases}
\end{equation}
If the null hypothesis is true then the streaming data is distributed consistently along $P$ and no change point exists. If the null hypothesis is rejected, the time series may be partitioned by a change point, $t^*$, that signifies all data from $t \geq t^*$ are distributed differently than data from $t<t^*$. Because we are operating in a non-parametric setting, the parameters used for comparison are estimated using the streamed data and aren't assumed to follow any particular family of distributions. 

Many change point detection algorithms define a statistic that is computed using each set before and after the possible change point \cite{aminikhanghahi2017survey}. If the statistic is above a threshold, $h \in \mathbb{R}$, then time $t$ is classified as a change point. The estimated change point time is denoted as $\hat{t}$ and can be compared to underlying true change point, $t^*$, if it is known.

The size of the window, $w$, is an important consideration that is typically chosen based on the problem being solved. A small  window will detect change points more frequently resulting in fast change point detection but at the cost of more incorrect detections. A large window will have the opposite problem. The change point method will have less incorrect detections but will be slower to detect real change points.

\subsection{Performance Measures}
\label{perf}
%Because of the unsupervised nature of detecting change points, it is difficult to evaluate the performance of change point detection models with real world data. Many papers detail asymptotic or non-asymptotic theoretical bounds of their proposed change point methods.  These theoretical results are typically compared across different change point methods for benchmarking a new algorithm.

An important crux of change point detection is how to evaluate an algorithm. Real life streaming data does not always announce when its distribution has changed even when clearly it has. In some cases, it may be possible to trace a change in variation back to a deterministic machine that is no longer operating as expected, but in other cases this may be impossible. For this reason most past research has focused on theoretical results rather than empirical ones. In the cases where empirical data is used, it is usually generated synthetically. This allows the practitioner to control exactly where the distribution changes and compare a proposed method to ground truth change points. Therefore, the following measures of performance will be presented in the context of synthetic data when all change points are known.

Suppose we continuously generate synthetic data according to some distribution and some point in time, $t_1^*$, we generate the data from a different distribution. If we repeat this process $K^*$ times then we can let the set of true change points be denoted by $\mathcal{T}^*=\{t_1^*, t_2^*,...,t_{K^*}^*\}$ whose size is $|\mathcal{T}^*| = K^*$. If this particular dataset is fed into an online change point detection algorithm in a streaming fashion then the algorithm can generate an estimated set of change points. Let this set of estimated change points be denoted by $\hat{\mathcal{T}}=\{\hat{t}_1, \hat{t}_2,...,\hat{t}_{\hat{K}}\}$ whose size is $|\hat{\mathcal{T}}| = \hat{K}$. Note, $\hat{K}$ does not necessarily equal $K^*$.

Armed with both sets of change points, we can now assess performance of an algorithm. A change point is considered detected if it occurs within a user-defined margin of error $M>0$. That is, if $\hat{t}$ is detected within $M$ samples of $t^*$ then it is an estimated change point. Therefore, the set of true positives $\operatorname{TP}$ can be written as,
\begin{equation}
\operatorname{TP}(\mathcal{T}^*, \hat{\mathcal{T}}) = \{t^* \in \mathcal{T}^* | \exists \hat{t} \in  \hat{\mathcal{T}} \text{ s.t. } 0 < \hat{t} - t^* \leq M\}.
\end{equation}
Notice by this definition, an estimated change point occurring just prior to a real one is a missed detection in online detection. This is in stark contrast to offline change point detection that is more concerned with the segmentation of a time series, where being as close as possible to an actual change point is sufficient. This is unrealistic in the online scenario.

If an estimated change point falls outside the margin, then it is deemed a false positive. These are typically called \textit{false alarms} or false detections in the change point detection literature \cite{lee2010change}. The set of false alarms can be written as:
\begin{equation}
\operatorname{FA}(\mathcal{T}^*, \hat{\mathcal{T}}) = \{\hat{t} \in  \hat{\mathcal{T}} | \nexists t^* \in \mathcal{T}^* \text{ s.t. } 0 < \hat{t} - t^* \leq M\}.
\end{equation}

Missed change points are referred to as missed detections and they occur when no estimated change point is flagged following an actual change point. This is equivalent to a false negative in machine learning classification. Therefore the set of missed detections are:
\begin{equation}
\operatorname{MD}(\mathcal{T}^*) = \{t^* \in \mathcal{T}^*  \text{ s.t. } t^* \notin \operatorname{TP} \}.
\end{equation}

In figure \ref{fig:changepoints}, and example demonstrating the relationship between true positives, false alarms and missed detections is shown.

Using the set of true positives, we can also use \textit{precision} and \textit{recall} to evaluate the performance. Precision is the proportion of the predicted change points that are true change points. Recall is the proportion of the true change points that are well predicted. This gives:

\noindent
\begin{tabularx}{\linewidth}{@{}XX@{}}
\begin{equation}
\operatorname{PREC}(\mathcal{T}^{*}, \hat{\mathcal{T}})=\frac{|\operatorname{TP}(\mathcal{T}^{*}, \hat{\mathcal{T}})|} {\hat{K}}
\end{equation}
&
\begin{equation}
\operatorname{REC}(\mathcal{T}^{*}, \hat{\mathcal{T}})=\frac{|\operatorname{TP}(\mathcal{T}^{*}, \hat{\mathcal{T}})|} {K^*}.
\end{equation}
\end{tabularx}
Precision and recall are well-defined between $(0,1)$ if the margin $M$ is smaller than the spacing between two true change points. This is an important consideration that will be useful for experiments conducted in section \ref{experiments}.

\begin{center} 
\captionof{figure}[Change Point Classification Example ]{An example of actual change points $t_1^*, t_2^*, t_3^*$ and estimated change points ${\hat{t}_1, \hat{t}_2, \hat{t}_3}$. The first two estimated change points in green are true positives as they are detected within the margin $M$.  The third estimated change point in red is not within the margin making it a false alarm. Finally the last actual change point does not have any estimated change points after it making it a missed detection.} 
\ctikzfig{changepoints}
\label{fig:changepoints} 
%\medskip
%\tiny
\end{center}

While all the above metrics are useful, they are not sufficient on their own. Suppose we have two online change point methods, one is capable of consistently detecting change points one time-step after an actual change point and the second method is capable of consistently detecting change points 20 time-steps after an actual change point. Assuming the margin of error in this case is $M>20$, is there anyway to discern between the two methods using the above evaluation metrics?  The answer is no. This is a problem because clearly one method is better at capturing changes in a time series quickly. Simply counting the number of false alarms or missed detections does not capture the goal of the online change point detection problem, which is to quickly detect change points over time, while minimizing false alarms. 
 
%These are typically called \textit{false alarms} or false detections in the change point detection literature \cite{lee2010change}. 

%The second issue is not detecting a change point when one occurs, called a \textit{missed detection}. This could be caused by detecting a change point much too late for it to be of any use or simply missing it altogether. 
%n both cases, a running total of false alarms and missed detections is monitored for performance. 
%However, simply counting the number of false alarms or missed detections does not capture the goal of the online change point detection problem, which is to quickly detect change points over time, while minimizing false alarms. 

In the online change point detection literature, there are two common ways to incorporate the notion of delay when flagging change points. The first is a metric known as the \textit{average time to false alarm} (TTFA) and it is defined as 
the expected amount of time that must pass before observing a false alarm is raised. It is defined as:
\begin{equation}
\label{ttfa}
\text{TTFA} = \mathbb{E}[\text{ }  \hat{t} \text{ } |\text{ }  t^* \text{ does not exist} ].
\end{equation}
Clearly, a larger value of TTFA is preferable. In the literature this measure is also referred to average run length (ARL) or average run length under the null (ARL$_0$) \cite{johnson2017detecting}. 

Alternatively, the second method of evaluating the delay of estimated change points is by measuring the average amount of time that passes before a true change point is detected. This is known as the \textit{expected detection delay} (EDD) and is defined as, 
\begin{equation}
\label{edd}
\text{EDD} = \mathbb{E}[\text{ }  \hat{t} \text{ } |\text{ }  t^* \text{ does exists} ].
\end{equation}

Because we will use EDD to assess performance in later chapters, it can be estimated by:
\begin{equation}
\label{est_edd}
\widehat{\text{EDD}} = \sum_{i=1}^{|\operatorname{TP}|} \sum_{j=1}^{\hat{K}} \frac{\hat{t}_j - t_i^*}{|\operatorname{TP}|} \quad \quad \forall t^* \in \operatorname{TP}, \forall \hat{t} \in \hat{\mathcal{T}} \text{ s.t. } |\hat{t}_j - t_i^*| \leq M
\end{equation} 

Both TTFA and EDD are two sides of the same coin. Reducing the EDD also reduces the TTFA, which is good for the former and bad for the latter. The opposite is also true. Increasing TTFA, also increases the EDD. Optimizing one or the other may depend on the domain-specific application, where the practitioner may want to sacrifice one for the other. 
%\begin{equation}
% \text{Missed Detections = 100*\frac{\text{\# of missed changes}}{total change points}}
% \end{equation}

%For quantifying this error, the worst case detection delay (WCD) metric measures how slow a model will detect a  change point in a worst case scenario. Conversely to TTFA, lower values of WCD are preferable.
%\begin{equation}
%WCD = sup esssup E_t[(T-t)^+ | F_{t-1}]
%\end{equation}

%From a hypothesis testing perspective, this is equivalent to not rejecting $H_0$ in \hl{cite equation in problem statement} when it should be rejected, i.e. type II error.

%Balancing the TTFA and WCD of an online detection algorithm is crucial to for an algorithm to be of any practical use. In \hl{1971 Lorden procedures to reacting...}, it was shown asymptotically that the CUSUM algorithm provides an optimal trade-off between TTFA and WCD and ,in \hl{moustakides optimal stopping times for detecting.. 1986}, it was proved optimal in the non-asymptotic case as well. Note, TTFA and WCD are also commonly referred to as ARL$_0$ and ARL$_1$ respectively where ARL stands for average run length. For clarity, we use the more explicit terms TTFA and WCD.

%When detecting changes of a distribution, a user may want to quantify the size of the change in the mean by
%$|\mathbb{E}[X_{\tau}]-\mathbb{E}[X_{\tau+1}]|$ or, similarly, the size of the change in the variance by $|\text{Var}[X_{\tau}]-\text{Var}[X_{\tau+1}]|$.


\iffalse
Other standard classifier metrics can also used for comparing $\hat{t}$ and $t^*$. This includes the  F1-Score that is based on a classifier's precision and recall:
\begin{equation}
F_1(\hat{t}, t^*) = 2 * \frac{\text{precision*recall}}{\text{precision + recall}}
\end{equation}

F1-Score is defined as the harmonic mean of precision and recall. Precision is defined as the ratio of true positives (TP) to the
number of true positives (TP) and false positives (FP) and recall is defined as the ratio the number of true positives to the
number of true positives plus the number of false negatives. F1-Score is best when F1 = 1 (perfect precision and recall) and reaches its worst value at F1 = 0. Depending on the context, any other classifier evaluation tools such as the Receiver Operating Characteristics Curve and the Precision Recall Curve may be used as well.
\fi

\section{Classic Algorithms}
\label{classic_algo}
Presented below are the fundamental approaches to online change point detection that have been very influential on modern approaches. Many modern algorithms are variants of the classic algorithms discussed below. In the following sections let $X_t$ be defined as it is in \ref{probFormulation}.

\subsection{Shewart Control Chart}
Shewart control charts were originally designed to detect changes in the mean of a process where the values being observed are assumed to be Gaussian distributed \cite{shewhart1931economic}. As the data arrives, the data is batched into samples of size $N$. The sample mean, $\bar{X}=\frac{1}{N} \sum_{t=1}^N X_t$, is then calculated and compared it to a known, true mean $\mu^*$, which is assumed to be known in advance.  Similarly, it is assumed the standard deviation, $\sigma$, is known in advance but it can also be estimated. If the absolute difference is greater than a threshold, then a change point is declared at the current batch. Therefore, the decision rule is defined as,
\begin{equation}
|\bar{X}-\mu^*| > \kappa \frac{\sigma}{\sqrt{N}},
\end{equation}

where $\kappa \in \mathbb{R}$ is a constant that controls how sensitive the algorithm is. Typically, it is set to $\kappa=3$ as this coincides with the observations within $3$ standard deviations of the mean. Under the assumption that the data is distributed normally,  approximately $99.7\%$ of the observations are distributed in this region, therefore a change point is declared if it falls outside this region. The true mean is assumed to known and is defined as $\mu^* = \mathbb{E}[X_t]$. In applications, the true mean can also be replaced by some target specification that a process must adhere to.

Tuning the hyper-parameters can drastically change the performance of the algorithm. The parameter $\kappa$ is used to control the trade-off between false alarms and missed detections. Choosing a lower $\kappa$ makes the control chart detect change points more often and, consequently, increases the false alarms. Whereas a higher $\kappa$ results in less false detections but also more missed detections. The chosen sample size, $N$, is also critical and its effect on the performance of Shewart control charts is studied in \cite{haridy2017effect}.

\subsection{CUSUM}
\label{cusum}
Similar to the Shewart control chart, the CUSUM algorithm tracks a statistic over time relative to a predetermined threshold \cite{page1954continuous}. CUSUM is best applied to a process that is already under control. It can be thought of accumulating the information of current and past samples. 

The algorithm is defined by a statistic, $S_t \in \mathbb{R}$, that is recursively updated after each sample, $X_t$, is observed, such that:
\begin{equation}
  \begin{cases}
    S_0 = 0  & \text{(Initialization)} \\
    S_t = \text{max}(0, S_{t-1} + Z_t) & \text{for t=1,2,...,}
  \end{cases}
\end{equation}
where $Z_t=ln(\frac{f_{1}(X_i)}{f_{0}(X_i)})$ and the statistic $S_t$ is compared to a threshold $h \in \mathbb{R}$ that is predetermined by the user. Functions $f_0$ and $f_1$ are probability density functions for the pre-change distribution and post-change distribution respectively.  If $S_t \geq h$ then a change point is declared at time $t$ and the algorithm is either stopped or restarted. Given that the statistic only flags change points when greater than a threshold, this algorithm only detects positive changes in the distribution. In \cite{page1954continuous}, it is suggested to combine two CUSUM algorithms to detect positive and negative changes in a distributional parameter.

As a parametric algorithm, it is assumed $f_0$ and $f_1$ are known at the outset. In most applications, this is quite limiting and unrealistic. Therefore, in cases where they are unknown, \textit{maximum likelihood estimates} of each distribution's parameters are usually computed. See \cite{kay1993fundamentals} for details on maximum likelihood estimation. 

Several extensions to the CUSUM algorithm have been proposed such as the filtered-derivative extension introduced in \cite{basseville1981edge}, which uses the change of the discrete derivative of a signal over time to detect a change point. In \cite{lucas1982fast}, a fast initial response (FIR) CUSUM algorithm is proposed where the starting value of initial cumulative sums adapts over time. Instead of resetting $S_0$ to zero as shown above, it is reset to a non-zero value, typically based on the threshold chosen. This gives the algorithm a head-start in quickly detecting when a process is out of control and is especially useful for processes that don't start in control.

Finally, since CUSUM is typically better at detecting small shifts in signals and the Shewart control chart is faster at detecting larger changes, the two can be combined \cite{lucas1982combined}. The combined Shewart-CUSUM algorithm leverages the strengths of both techniques for better overall performance. See  \cite{yashchin1985analysis} and \cite{westgard1977combined} for more details. 

\subsection{EWMA}
\label{ewma}
First described in \cite{roberts1959control} as a \textit{geometric average}, the exponentially weighted moving average (EWMA) is a type of moving average that applies exponential weighting to time series samples. Initially used as a forecasting technique in the econometrics field for smoothing noisy functions, the EWMA can also be used for determining out of control processes as shown in \cite{hunter1986exponentially}. Rather than weighing all observations uniformly like the standard CUSUM algorithm or a simple moving average, a decay factor (also called a  forgetting factor), $\lambda \in [0,1]$, is used to control how much weight is distributed over the previous observations. As each new observation arrives, the EWMA statistic, $E_t \in \mathbb{R}$, is recursively updated and compared to a threshold. If the EWMA statistic exceeds the threshold then the process is deemed out of control or, in other words, a change point is detected.

The EWMA statistic is calculated as follows at each time step, $t$:
$$E_t = \lambda X_t + (1-\lambda)E_{t-1}. $$

As $\lambda \to 1$, the EWMA gives more and more weight to the most recent observations similar to a Shewhart control chart, which gives weight to the last observation only. Conversely, as $\lambda \to 0$, the weights are distributed further into the past giving the EWMA a longer memory similar to the CUSUM algorithm. Therefore, a EWMA control chart can be interpreted as a trade-off between a Shewhart control chart and a CUSUM control chart. 

For detecting deviations away from a mean target value, control limits may be calculated in a similar manner to the Shewart control chart. In \cite{hunter1986exponentially}, control limits for the EWMA are chosen to be  $\pm 3 \sigma \sqrt{\frac{\lambda}{2-\lambda}}$.  Like the Shewart control chart, it is assumed the standard deviation, $\sigma$, is known in advance but it can be estimated if it is not known. %and its derivation is shown in the appendix of \hl{XX}.

%More generally, if a function, $g(x)$ is applied to the observations such that we monitor $\tau= \mathbb{E}[g(X_t)]$ over time. Then the EWMA statistic will be compared to the in-control $\tau^*$ at each iteration and is assumed to be known beforehand.  

As with the other methods previously mentioned, the standard EWMA is a parametric method as it assumes the time series has some in-control average that is known prior to use. This makes is difficult to apply in situations where the data is coming from unknown distributions. It is however very fast due to its recursive structure and does not hold a lot of data in memory making it appealing for live data streams that need fast data processing.