\chapter{Background}
This chapter describes how the change point problem will be formulated in this thesis and, by extension, how all methods will be described using the change point detection problem notation. Because online change point detection is closely related to two-sample testing, a background on statistical hypothesis testing is presented first. 

\section{Hypothesis Testing}

Two-sample hypothesis testing concerns itself with the following problem: Suppose data is independently sampled from two groups of different populations. The first sample has $n$ observations where each observation is denoted as $X_i \in \mathbb{R}$ such that $1 \leq i \leq n$. The second sample has $m$ observations where each observation is denoted as $Y_i \in \mathbb{R}$ such that $1 \leq i \leq m$.  Assume $X$ has a probability distribution $P$ and $Y$ has a probability distribution $Q$. Sample one is denoted as  $X=\{X_1, X_2, ...,X_n\} \sim P$ and sample two is denoted as $Y=\{Y_1, Y_2, ...,Y_m\} \sim Q$. 

The fundamental question of a two-sample test is determining whether samples are $X$ and $Y$ are significantly different on a statistical level. Therefore, a two-sample hypothesis test can be setup where the \textit{null hypothesis}, denoted by $H_0$, is that the two distributions do not differ based on some statistical test. If the null hypothesis is rejected, then the \textit{alternative hypothesis}, denoted by $H_1$, is accepted and we conclude the two samples differ in their distributions according to the specific two-sample test used. The type of two-sample test used indicates how the distributions differ.

\begin{equation}
  \begin{cases}
    H_0: P = Q & \text{(both samples come from the same distribution)} \\
    H_1: P \neq Q & \text{(samples do not come from the same distribution)}. 
  \end{cases}
\end{equation}

The general procedure of a two-sample test is the following: first, a test statistic, $\hat{t} \in \mathbb{R}$, is computed based on the type of test used and the observed data. This test statistic is compared to a significance level, $\alpha \in [0,1]$ that is chosen  at the outset. Common choices for $\alpha$ are $0.1, 0.05$ and $0.01$ \cite{moore1993introduction}. Finally, a \textit{p-value} is calculated using the test statistic. The p-value is the probability of observing $\hat{t}$ under the null hypothesis, such that the p-value$=\mathbb{P}(T>\hat{t}|H_0)$. A p-value < $\alpha$ would be improbable under the null hypothesis, therefore, in this situation it is rejected and the alternative hypothesis is accepted.

Given the two possible outcomes of a two-sample test, it is clear the hypothesis test can fail in the two following ways. The first is rejecting the null hypothesis when it is correct. This is known as a false-positive or a type-I error and is upper-bounded by the chosen significance level, $ \alpha$. It is equivalent to following conditional probability: $\mathbb{P}(\text{reject } H_0 | H_0 \text{ is true})$. The second possible source of error is  a false-negative or type-II error . The probability of committing a type-II error is denoted as $\beta=\mathbb{P}\left(\text {accept } H_{0} | H_{0} \text { is false}\right)\). The quantity $1-\beta$ is referred to as the \textit{power} of a test. Maximizing test power is an important part of designing new algorithms and is typically used to compare different methods. 

Often there is a trade-off between type-I and type-II errors and the practitioner must decide how to balance the two given their domain-specific knowledge of the problem. In some cases, it may be desirable to sacrifice one for the other. For example, in the medical field, a false positive diagnosis (type-I error) may be more desirable  than missing a diagnosis (type-II error) which would result in never giving treatment to a patient. 

Many two-sample tests exist for evaluating the different types of differences two samples may have. For example, the Student $t$-test is a two-sample test for determining if samples of univariate data come from a population with the same mean \cite{student1908probable}. A generalization of the Student $t$-test for the multivariate case is the Hotelling $T^2$ test that compares whether the means of two multivariate samples are significantly different \cite{hotelling1992generalization}. Both of these are parametric tests as they assume the samples are normally distributed. 

Alternatively, non-parametric tests make no assumptions about the distributions $P$ and $Q$. For example, the Kolmogorov-Smirnov test (KS test) \cite{massey1951kolmogorov} can determine whether or not two univariate samples come from the same distribution. This is done by computing the \textit{supremum} of the difference of the empirical cumulative distribution functions from each sample. The KS test does not specify what distribution the samples come from, only if they differ according to the KS statistic. More recently, the kernel two-sample test introduced in \cite{gretton2012kernel} as another flexible, non-parametric test. It is not limited to one dimensional data, and can be applied to non-numeric data. It is based on the \textit{maximum mean discrepancy} (MMD) statistic and is capable of detecting any kind of change in distribution. It is a focus in this thesis and is discussed in further detail in section \ref{mmd}.

\section{Problem Formulation}

\subsection{Change Point Detection Problem}
\label{probFormulation}
The basic change point problem is set up as hypothesis test between two segments of a time series. Let $X_t, X_{t+1}...,X_n$ be a series of independent random variables of dimension $d \geq 1$ that are sequentially observed over time, $t=0,1,2...$  At every time increment, $t$, it is possible that previous observations no longer follow the same distribution as new observation(s) that are arriving. Therefore, one of the two hypotheses holds at each time increment:
\begin{equation}
  \begin{cases}
    H_0: X_{t^*-1}, X_{t^*}, ...,X_{t+n} \sim  P & \text{(no change point occured)} \\
    H_1: X_{t^*-3},X_{t^*-2}, X_{t^*-1} \sim P, X_{t^*}, X_{t^*+1},...,  \sim Q & \text{(a change point occured)}
  \end{cases}
\end{equation}
We do not explicitly state how many observations are stored for the pre-change distribution because this depends on the method used for conducting the two-sample test. If the null hypothesis is true then the streaming data is distributed consistently along $P$ and no change point exists. If the null hypothesis is rejected, the time series may be partitioned by a change point, $t^*$, that signifies all data from $t \geq t^*$ are distributed differently than data from $t<t^*$. Because we are operating in a non-parametric setting, the distributions $P$ and $Q$ are assumed to be completely unknown. 

%Where $i=1,2,..., \tau-1$ and $j=\tau,...,n$  are two distinct segments separated by the change point $\tau$ that is within the time series window.  

%If there is no change in the data then we say the change time is equal to infinity and denote this probability as $P^{\infty}$ and the expectation is $E^{\infty}$.

Many change point detection algorithms define a statistic that is computed using each set before and after the possible change point. If the statistic is above a threshold, $h \in \mathbb{R}$, then time $t$ is classified as a change point. The estimated change point time is denoted as $\hat{t}$ and can be compared to underlying true change point, $t^*$, if it is provided.

In the online scenario, the time series under consideration can be thought of as a sliding window with data constantly coming in and out of the window of interest. The size of the window is an important consideration that is typically chosen based on the problem being solved. Too small a window and the sets of data may not yield a statistically significant result. Too large of a window and the problem leans more towards and off-line model, where high volumes must be stored and several change points may appear in a given window. 

\subsection{Performance Measures}
%Because of the unsupervised nature of detecting change points, it is difficult to evaluate the performance of change point detection models with real world data. Many papers detail asymptotic or non-asymptotic theoretical bounds of their proposed change point methods.  These theoretical results are typically compared across different change point methods for benchmarking a new algorithm.

Two main issues arise when detecting change points in a stream of data. The first is detecting a change point when there is no actual statistical change in the observed sequence. These are typically called \textit{false alarms} or false detections in the change point detection literature. The false alarm rate is defined by a metric known as the \textit{average time to false alarm} (TTFA).   It is defined as:
\begin{equation}
TTFA = \mathbb{E}[\text{ }  \hat{t} \text{ } |\text{ }  t^* \text{ does not exist} ],
\end{equation}
where it is the expected number of observations that are observed before a change point is incorrectly detected. In other words, it is the average amount of time until a change is detected given a sequence of observations with no change. Therefore, a larger value of TTFA is preferable. From a hypothesis testing perspective, this is equivalent to rejecting $H_0$ in  when it should not be rejected, i.e. type I error. In the literature it is also referred to average run length (ARL) or average run length under the null (ARL$_0$). 

The second issue is not detecting a change point when one occurs. This could be caused by detecting a change point much too late for it to be of any use or simply missing it altogether. It is measured by the missed number of detections and can be evaluated as a percentage by:
%\begin{equation}
% \text{Missed Detections = 100*\frac{\text{\# of missed changes}}{total change points}}
% \end{equation}

%For quantifying this error, the worst case detection delay (WCD) metric measures how slow a model will detect a  change point in a worst case scenario. Conversely to TTFA, lower values of WCD are preferable.
%\begin{equation}
%WCD = sup esssup E_t[(T-t)^+ | F_{t-1}]
%\end{equation}

%From a hypothesis testing perspective, this is equivalent to not rejecting $H_0$ in \hl{cite equation in problem statement} when it should be rejected, i.e. type II error.

%Balancing the TTFA and WCD of an online detection algorithm is crucial to for an algorithm to be of any practical use. In \hl{1971 Lorden procedures to reacting...}, it was shown asymptotically that the CUSUM algorithm provides an optimal trade-off between TTFA and WCD and ,in \hl{moustakides optimal stopping times for detecting.. 1986}, it was proved optimal in the non-asymptotic case as well. Note, TTFA and WCD are also commonly referred to as ARL$_0$ and ARL$_1$ respectively where ARL stands for average run length. For clarity, we use the more explicit terms TTFA and WCD.

%When detecting changes of a distribution, a user may want to quantify the size of the change in the mean by
%$|\mathbb{E}[X_{\tau}]-\mathbb{E}[X_{\tau+1}]|$ or, similarly, the size of the change in the variance by $|\text{Var}[X_{\tau}]-\text{Var}[X_{\tau+1}]|$.

In cases where a change point is detected, the average detection delay (ADD) is used to estimate the average time it takes for a change point to be detected. It is computed by comparing the time difference between a predicted change point and an actual change point. This difference is then normalized the total by the total number of change points:
\begin{equation}
\text{ADD} = \frac{\sum_i^{\#CP} |\hat{t_i} - t^*|}{\#CP}
\end{equation}
The closer the predicted change points are to the actual change points, the smaller the average detection delay. Because it relies on ground truth change points, it can only be used when experimenting on synthetic data where all the information is available.%Therefore, the ADD can range from $(0, \inf)$. 

If labelled change points are available for a real world dataset or a synthetic dataset, then the ground truth change point vector, 
$t^*$, is known. Evaluating performance in this case is the same as evaluating a binary classifier from a supervised learning problem. For example, the metrics from discussed in hypothesis testing, type-I error (false positives) and type-II error (false negatives), are commonly used for evaluating change point detection performance. The false positive rate (FPR) can be calculated by $FPR = FP / N$. Plotting the false positive rate and the true positive rate, gives the receiver operator characteristic curve. The area under this curve (AUC), is calculated and compared to random baseline performance of 0.5 that would be equivalent to classification by tossing a coin.

%For example the \textit{Hausdorff} metric can be used. It measures the furthest temporal distance between a predicted change point $\hat{\tau}$ and $\tau^*$. It is defined as:

Other standard classifier metrics can also used for comparing $\hat{t}$ and $t^*$. This includes the  F1-Score that is based on a classifier's precision and recall:
\begin{equation}
F_1(\hat{t}, t^*) = 2 * \frac{\text{precision*recall}}{\text{precision + recall}}
\end{equation}

 F1-Score is defined as the harmonic mean of precision and recall. Precision is defined as the ratio of true positives (TP) to the
number of true positives (TP) and false positives (FP) and recall is defined as the ratio the number of true positives to the
number of true positives plus the number of false negatives. F1-Score is best when F1 = 1 (perfect precision and recall) and reaches its worst value at F1 = 0. Depending on the context, any other classifier evaluation tools such as the Receiver Operating Characteristics Curve and the Precision Recall Curve may be used as well.

\section{Classic Algorithms}
Presented below are the fundamental approaches to online change point detection that have been very influential on modern approaches. Many modern algorithms are variants of the classic algorithms discussed below. In the following sections let $X_t$ be defined as it is in \ref{probFormulation}.

\subsection{Shewart Control Chart}
Shewart control charts were originally designed to detect changes in the mean of a process where the values being observed are assumed to be Gaussian distributed. As the data arrives, the data is batched into samples of size $N$. The sample mean, $\bar{X}=\frac{1}{N} \sum_{t=1}^N X_t$, is the then calculated and compared it to a known, true mean $\mu^*$.  Similarly, it is assumed the standard deviation, $\sigma$, is known in advance but it can also be estimated. If the absolute difference is greater than a threshold, then a change point is declared at the current batch. Therefore, the decision rule is defined as,
\begin{equation}
|\bar{X}-\mu^*| > \kappa \frac{\sigma}{\sqrt{N}},
\end{equation}

where $\kappa \in \mathbb{R}$ is a constant that controls how sensitive the algorithm is. Typically, it is set to $\kappa=3$ as this coincides with the observations within $3$ standard deviations of the mean. Under the assumption that the data is distributed normally, $99.7$ of the observations are distributed in this region, therefore a change point is declared if it falls outside this region. The true mean is assumed to known and is defined as $\mu^* = \mathbb{E}[X_t]$. In applications, the true mean can also be replaced by some target specification that a process must adhere to.

Tuning the hyper-parameters can drastically change the performance of the algorithm. The parameter $\kappa$ is used to control the trade-off between false detections and missed detections. Choosing a lower $\kappa$ makes the control chart detect change points more often and, consequently, increases the false detections. Whereas a higher $\kappa$ results in less false detections but also more missed detections. The chosen sample size, $N$, is also critical and its effect on the performance of Shewart control charts is studied in \cite{haridy2017effect}.

\subsection{CUSUM}
Similar to the Shewart control chart, the CUSUM algorithm tracks a statistic over time relative to a predetermined threshold. CUSUM is best applied to a process that is already under control. It can be thought of accumulating the information of current and past samples. 

The algorithm is defined by a statistic, $S_t \in \mathbb{R}$, that is recursively updated after each sample, $X_t$, is observed, such that:
\begin{equation}
  \begin{cases}
    S_0 = 0  & \text{(Initialization)} \\
    S_t = \text{max}(0, S_{t-1} + Z_t) & \text{for t=1,2,...,}
  \end{cases}
\end{equation}
where $Z_t=ln(\frac{f_{1}(X_i)}{f_{0}(X_i)})$ and the statistic $S_t$ is compared to a threshold $h \in \mathbb{R}$ that is predetermined by the user. Functions $f_0$ and $f_1$ are probability density functions for the pre-change distribution and post-change distribution.  If $S_t \geq h$ then a change point is declared at time $t$ and the algorithm is either stopped or restarted. Given that the statistic only flags change points when greater than a threshold, this algorithm only detects positive changes in the distribution. In \cite{page1954continuous}, it is suggested to combine two CUSUM algorithms to detect positive and negative changes in a distributional parameter.

As a parametric algorithm, it is assumed $f_0$ and $f_1$ are known at the outset. In most applications, this is quite limiting and unrealistic. Therefore, in cases where they are unknown, maximum likelihood estimates of each distribution's parameters are usually computed.

Several extensions to the CUSUM algorithm have been proposed such as the filtered-derivative extension introduced in \cite{basseville1981edge}, which uses the change of the discrete derivative of a signal over time to detect a change point. In \cite{lucas1982fast}, a fast initial response (FIR) CUSUM algorithm is proposed where the starting value of initial cumulative sums adapts over time. Instead of resetting $S_0$ to zero as shown above, it is reset to a non-zero value, typically based on the threshold chosen. This gives the algorithm a head-start in quickly detecting when a process is out of control and is especially useful for processes that don't start in control.

Finally, since CUSUM is typically better at detecting small shifts in signals and the Shewart control chart is faster at detecting larger changes, the two can be combined. The combined Shewart-CUSUM algorithm leverages the strengths of both techniques for better overall performance. See \cite{lucas1982combined}, \cite{yashchin1985analysis}, and \cite{westgard1977combined} for more details. 

\subsection{EWMA}
First described in \cite{roberts1959control} as a "geometric average", the exponentially weighted moving average (EWMA) is a type of moving average that applies exponential weighting to time series samples. Initially used as a forecasting technique in the econometrics field for smoothing noisy functions, the EWMA can also be used for determining out of control processes as shown in \cite{hunter1986exponentially}. Rather than weighing all observations uniformly like the standard CUSUM algorithm or a simple moving average, a decay factor (also called a  forgetting factor), $\lambda \in [0,1]$, is used to control how much weight is distributed over the previous observations. As each new observation arrives, the EWMA statistic, $E_t \in \mathbb{R}$, is recursively updated and compared to a threshold. If the EWMA statistic exceeds the threshold then the process is deemed out of control or, in other words, a change point is detected.

The EWMA statistic is calculated as follows at each time step, $t$:
$$E_t = \lambda X_t + (1-\lambda)E_{t-1}. $$

As $\lambda \to 1$, the EWMA control chart gives more and more weight to the most recent observations similar to a Shewhart control chart, which gives weight to the last observation only. Conversely, as $\lambda \to 0$, the weights are distributed further into the past giving the EWMA a longer memory similar to the CUSUM algorithm. Therefore, a EWMA control chart can be interpreted as a trade-off between a Shewhart control chart and a CUSUM control chart. 

For detecting deviations away from a mean target value, control limits may be calculated in a similar manner to the Shewart control chart. In \cite{hunter1986exponentially}, control limits for the EWMA are chosen to be  $\pm 3 \sigma \sqrt{\frac{\lambda}{2-\lambda}}$.  Like the Shewart control chart, it is assumed the standard deviation, $\sigma$, is known in advance but it can be estimated. %and its derivation is shown in the appendix of \hl{XX}.

%More generally, if a function, $g(x)$ is applied to the observations such that we monitor $\tau= \mathbb{E}[g(X_t)]$ over time. Then the EWMA statistic will be compared to the in-control $\tau^*$ at each iteration and is assumed to be known beforehand.  

As with the other methods previously mentioned, the standard EWMA is a parametric method as it assumes the time series has some in-control average that is known prior to use. This makes is difficult to apply in situations where the data is non-numeric or where third or fourth moment changes in the distribution occur. It is however very fast due to its recursive structure and does not hold a lot of data in memory making it appealing for live data streams that need fast data processing.